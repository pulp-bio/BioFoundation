/*
 * Copyright (c) 2026 Thorir Mar Ingolfsson, ETH Zurich
 * SPDX-License-Identifier: Apache-2.0
 */

/**
 * Integer Softmax Kernel (i-Softmax)
 *
 * LUT-based integer softmax for bit-exact reproducibility between
 * Python and C implementations.
 *
 * This file is included directly into network_kernels.c - it is NOT
 * compiled separately. This allows sharing static variables and inline
 * functions while keeping the code modular.
 */

// LUT parameters (must match Python atomic_ops/mhsa.py i_softmax_int16)
#define I_SOFTMAX_INPUT_MIN (-8.0f)
#define I_SOFTMAX_INPUT_STEP (8.0f / 1024.0f)  // 0.0078125
#define I_SOFTMAX_NUM_ENTRIES 1024
#define I_SOFTMAX_OUTPUT_SCALE 32767

// Pre-computed LUT: exp(x) * 32767 for x in [-8.0, 0.0] with 1024 entries
// Generated by: python tools/generate_softmax_lut.py --num-entries 1024 --output-scale 32767 --input-min -8.0 --input-max 0.0
const int16_t i_softmax_lut[I_SOFTMAX_NUM_ENTRIES] = {
    // exp(-8.0) * 32767 = 11, exp(-7.992) * 32767 = 11, ... exp(0) * 32767 = 32767
       11,    11,    11,    11,    11,    11,    12,    12,    12,    12,    12,    12,    12,    12,    12,    12,
       12,    13,    13,    13,    13,    13,    13,    13,    13,    13,    13,    14,    14,    14,    14,    14,
       14,    14,    14,    14,    15,    15,    15,    15,    15,    15,    15,    15,    16,    16,    16,    16,
       16,    16,    16,    16,    17,    17,    17,    17,    17,    17,    17,    17,    18,    18,    18,    18,
       18,    18,    18,    19,    19,    19,    19,    19,    19,    19,    20,    20,    20,    20,    20,    20,
       21,    21,    21,    21,    21,    21,    22,    22,    22,    22,    22,    22,    23,    23,    23,    23,
       23,    23,    24,    24,    24,    24,    24,    25,    25,    25,    25,    25,    26,    26,    26,    26,
       26,    27,    27,    27,    27,    27,    28,    28,    28,    28,    29,    29,    29,    29,    29,    30,
       30,    30,    30,    31,    31,    31,    31,    32,    32,    32,    32,    33,    33,    33,    33,    34,
       34,    34,    34,    35,    35,    35,    36,    36,    36,    36,    37,    37,    37,    38,    38,    38,
       38,    39,    39,    39,    40,    40,    40,    41,    41,    41,    42,    42,    42,    43,    43,    43,
       44,    44,    44,    45,    45,    45,    46,    46,    46,    47,    47,    47,    48,    48,    49,    49,
       49,    50,    50,    51,    51,    51,    52,    52,    53,    53,    53,    54,    54,    55,    55,    55,
       56,    56,    57,    57,    58,    58,    59,    59,    60,    60,    60,    61,    61,    62,    62,    63,
       63,    64,    64,    65,    65,    66,    66,    67,    67,    68,    69,    69,    70,    70,    71,    71,
       72,    72,    73,    74,    74,    75,    75,    76,    76,    77,    78,    78,    79,    79,    80,    81,
       81,    82,    83,    83,    84,    85,    85,    86,    87,    87,    88,    89,    89,    90,    91,    92,
       92,    93,    94,    94,    95,    96,    97,    97,    98,    99,   100,   101,   101,   102,   103,   104,
      105,   105,   106,   107,   108,   109,   110,   110,   111,   112,   113,   114,   115,   116,   117,   118,
      118,   119,   120,   121,   122,   123,   124,   125,   126,   127,   128,   129,   130,   131,   132,   133,
      134,   135,   136,   137,   139,   140,   141,   142,   143,   144,   145,   146,   147,   149,   150,   151,
      152,   153,   155,   156,   157,   158,   159,   161,   162,   163,   165,   166,   167,   168,   170,   171,
      172,   174,   175,   177,   178,   179,   181,   182,   184,   185,   186,   188,   189,   191,   192,   194,
      195,   197,   198,   200,   202,   203,   205,   206,   208,   210,   211,   213,   215,   216,   218,   220,
      221,   223,   225,   227,   228,   230,   232,   234,   236,   238,   239,   241,   243,   245,   247,   249,
      251,   253,   255,   257,   259,   261,   263,   265,   267,   269,   271,   273,   276,   278,   280,   282,
      284,   287,   289,   291,   293,   296,   298,   300,   303,   305,   308,   310,   312,   315,   317,   320,
      322,   325,   327,   330,   333,   335,   338,   340,   343,   346,   349,   351,   354,   357,   360,   362,
      365,   368,   371,   374,   377,   380,   383,   386,   389,   392,   395,   398,   401,   404,   408,   411,
      414,   417,   420,   424,   427,   430,   434,   437,   441,   444,   448,   451,   455,   458,   462,   465,
      469,   473,   477,   480,   484,   488,   492,   496,   499,   503,   507,   511,   515,   519,   523,   527,
      532,   536,   540,   544,   549,   553,   557,   562,   566,   570,   575,   579,   584,   589,   593,   598,
      602,   607,   612,   617,   622,   627,   631,   636,   641,   646,   652,   657,   662,   667,   672,   677,
      683,   688,   694,   699,   705,   710,   716,   721,   727,   733,   738,   744,   750,   756,   762,   768,
      774,   780,   786,   792,   798,   805,   811,   817,   824,   830,   837,   843,   850,   857,   863,   870,
      877,   884,   891,   898,   905,   912,   919,   926,   934,   941,   948,   956,   963,   971,   978,   986,
      994,  1002,  1010,  1017,  1025,  1033,  1042,  1050,  1058,  1066,  1075,  1083,  1092,  1100,  1109,  1118,
     1126,  1135,  1144,  1153,  1162,  1171,  1180,  1190,  1199,  1208,  1218,  1227,  1237,  1247,  1257,  1266,
     1276,  1286,  1297,  1307,  1317,  1327,  1338,  1348,  1359,  1370,  1380,  1391,  1402,  1413,  1424,  1435,
     1447,  1458,  1469,  1481,  1493,  1504,  1516,  1528,  1540,  1552,  1564,  1577,  1589,  1601,  1614,  1627,
     1639,  1652,  1665,  1678,  1691,  1705,  1718,  1732,  1745,  1759,  1773,  1787,  1801,  1815,  1829,  1843,
     1858,  1872,  1887,  1902,  1917,  1932,  1947,  1962,  1978,  1993,  2009,  2025,  2041,  2057,  2073,  2089,
     2106,  2122,  2139,  2155,  2172,  2189,  2207,  2224,  2241,  2259,  2277,  2295,  2313,  2331,  2349,  2368,
     2386,  2405,  2424,  2443,  2462,  2481,  2501,  2520,  2540,  2560,  2580,  2600,  2621,  2641,  2662,  2683,
     2704,  2725,  2747,  2768,  2790,  2812,  2834,  2856,  2879,  2901,  2924,  2947,  2970,  2994,  3017,  3041,
     3065,  3089,  3113,  3137,  3162,  3187,  3212,  3237,  3262,  3288,  3314,  3340,  3366,  3393,  3419,  3446,
     3473,  3500,  3528,  3556,  3583,  3612,  3640,  3669,  3697,  3726,  3756,  3785,  3815,  3845,  3875,  3905,
     3936,  3967,  3998,  4029,  4061,  4093,  4125,  4157,  4190,  4223,  4256,  4290,  4323,  4357,  4391,  4426,
     4461,  4496,  4531,  4567,  4602,  4638,  4675,  4712,  4749,  4786,  4823,  4861,  4899,  4938,  4977,  5016,
     5055,  5095,  5135,  5175,  5216,  5257,  5298,  5340,  5382,  5424,  5466,  5509,  5553,  5596,  5640,  5684,
     5729,  5774,  5819,  5865,  5911,  5957,  6004,  6051,  6099,  6147,  6195,  6244,  6293,  6342,  6392,  6442,
     6493,  6544,  6595,  6647,  6699,  6751,  6804,  6858,  6912,  6966,  7021,  7076,  7131,  7187,  7244,  7301,
     7358,  7416,  7474,  7533,  7592,  7651,  7711,  7772,  7833,  7894,  7956,  8019,  8082,  8145,  8209,  8274,
     8339,  8404,  8470,  8537,  8604,  8671,  8739,  8808,  8877,  8947,  9017,  9088,  9159,  9231,  9303,  9376,
     9450,  9524,  9599,  9674,  9750,  9827,  9904,  9982, 10060, 10139, 10219, 10299, 10380, 10461, 10543, 10626,
    10710, 10794, 10878, 10964, 11050, 11137, 11224, 11312, 11401, 11491, 11581, 11672, 11763, 11856, 11949, 12043,
    12137, 12232, 12328, 12425, 12523, 12621, 12720, 12820, 12921, 13022, 13124, 13227, 13331, 13436, 13541, 13648,
    13755, 13863, 13972, 14081, 14192, 14303, 14416, 14529, 14643, 14758, 14874, 14990, 15108, 15227, 15346, 15467,
    15588, 15710, 15834, 15958, 16083, 16210, 16337, 16465, 16594, 16725, 16856, 16988, 17122, 17256, 17392, 17528,
    17666, 17805, 17944, 18085, 18227, 18370, 18514, 18660, 18806, 18954, 19103, 19253, 19404, 19556, 19710, 19864,
    20020, 20178, 20336, 20496, 20657, 20819, 20982, 21147, 21313, 21480, 21649, 21819, 21990, 22163, 22337, 22512,
    22689, 22867, 23047, 23227, 23410, 23594, 23779, 23966, 24154, 24343, 24534, 24727, 24921, 25117, 25314, 25513,
    25713, 25915, 26118, 26323, 26530, 26738, 26948, 27160, 27373, 27588, 27805, 28023, 28243, 28465, 28688, 28913,
    29140, 29369, 29600, 29832, 30066, 30302, 30540, 30780, 31022, 31265, 31511, 31758, 32007, 32259, 32512, 32767
};

/**
 * Apply i-Softmax to a single row of attention scores.
 * Uses integer-only arithmetic for bit-exact reproducibility.
 *
 * Algorithm:
 * 1. Find max value for numerical stability
 * 2. Normalize: norm_score = score - max
 * 3. Quantize to LUT index: idx = round((norm_score - INPUT_MIN) / INPUT_STEP)
 * 4. LUT lookup: exp_val = lut[idx]
 * 5. Sum all exp values (INT32)
 * 6. Normalize with integer division: attn_int16 = (exp_val * 32767) / sum
 * 7. Convert to FP32: attn_fp32 = attn_int16 / 32767.0
 */
void i_softmax_row(
    const float *scores_row,
    float *attn_row,
    int seq_len,
    const int16_t *softmax_lut
) {
    // 1. Find max for numerical stability
    float max_val = scores_row[0];
    for (int j = 1; j < seq_len; j++) {
        if (scores_row[j] > max_val) max_val = scores_row[j];
    }

    // STACK-SAFE IMPLEMENTATION: Avoid VLA which can cause stack overflow on GAP9 worker cores
    // For seq_len=256, VLA would allocate 1024 bytes per worker core - too much for 8KB stack
    //
    // Solution: Store intermediate INT32 exp_vals in attn_row (reinterpret cast)
    // This is safe because:
    // - attn_row is the same size as scores_row (seq_len floats = seq_len * 4 bytes)
    // - INT32 and float are both 4 bytes
    // - We write final FP32 results back to attn_row in the same loop

    // Pass 1: Compute LUT indices, lookup exp values, and store in attn_row temporarily
    // Also accumulate sum
    int32_t *exp_vals = (int32_t *)attn_row;  // Reuse output buffer for intermediate values
    int32_t exp_sum = 0;

    for (int j = 0; j < seq_len; j++) {
        float norm_score = scores_row[j] - max_val;

        // Quantize to LUT index with rounding (match Python np.round)
        int idx = (int)lrintf((norm_score - I_SOFTMAX_INPUT_MIN) / I_SOFTMAX_INPUT_STEP);

        // Clip to valid range
        if (idx < 0) idx = 0;
        if (idx >= I_SOFTMAX_NUM_ENTRIES) idx = I_SOFTMAX_NUM_ENTRIES - 1;

        // LUT lookup (INT16 -> INT32 for accumulation)
        exp_vals[j] = (int32_t)softmax_lut[idx];
        exp_sum += exp_vals[j];
    }

    // Pass 2: Normalize and convert to FP32
    // Read exp_vals from end to start to avoid overwriting values we still need
    // (FP32 and INT32 share the same memory)
    for (int j = seq_len - 1; j >= 0; j--) {
        int32_t exp_val = exp_vals[j];
        int32_t attn_int16 = (exp_val * I_SOFTMAX_OUTPUT_SCALE) / exp_sum;
        // Convert to FP32 and store
        attn_row[j] = (float)attn_int16 / (float)I_SOFTMAX_OUTPUT_SCALE;
    }
}
