/*
 * Copyright (c) 2026 Thorir Mar Ingolfsson, ETH Zurich
 * SPDX-License-Identifier: Apache-2.0
 */

/**
 * Generated by ARES Code Generator
 * Network: ${network_name}
% if board_mode:
 * Mode: BOARD (minimal output, no golden checks, clean timing)
% else:
 * Mode: DEBUG (verbose output, golden validation enabled)
% endif
 */

/* ---
 * Includes and Compile-time Defines
 * --- */

#include <stdio.h>
#include <stddef.h>
#include <math.h>
#include <stdlib.h>
#include <string.h>
#include "pmsis.h"
#include "mem.h"
#include "network.h"
#include "network_kernels.h"
#include "tile_buffer_manager.h"
#include "l3_prefetch.h"
#include "network_dma_pipeline.h"
#include "network_l3_prefetch.h"
#include "network_buffer_mgmt.h"
#include "layer_descriptors.h"
#include "network_executor.h"

<%!
def resolve_l3_buffer_name(buffer_name, buffers):
    for buf in buffers:
        if buf.get('c_name') == buffer_name and buf.get('use_l3_fallback', False):
            return buffer_name
    return None
%>

#define NUM_CLASSES ${num_classes}

pi_cl_dma_copy_t __attribute__((section(".data"))) g_load_dma_descs[MAX_DMA_DESCRIPTORS];
pi_cl_dma_copy_t __attribute__((section(".data"))) g_store_dma_descs[MAX_DMA_DESCRIPTORS];

/* ---
 * Layer Descriptors (Static Array)
 * Defines the network_layers[] array for the data-driven executor
 * --- */

${layer_array_code}

<%
    l3_fallback_buffers = [b for b in activation_buffers if b.get('use_l3_fallback', False)]
    l3_fallback_pools = [p for p in shared_activation_pool if p.get('use_l3_fallback', False)]
    ops_present = {spec.get('op') for spec in layer_specs if spec.get('op')}
%>

<%include file="partials/network_args_struct.mako" args="l3_fallback_buffers=l3_fallback_buffers, l3_fallback_pools=l3_fallback_pools"/>

<%include file="partials/network_helpers.mako"/>

/* ---
 * Worker Functions (forked to cluster cores for parallel execution)
 * --- */

% if 'conv2d' in ops_present or 'patch_embed' in ops_present:
#include "ops/op_conv2d.c"
% endif
% if 'linear_int8' in ops_present or 'linear_fp32' in ops_present:
#include "ops/op_linear.c"
% endif
% if 'maxpool' in ops_present or 'avgpool' in ops_present or 'global_avgpool' in ops_present:
#include "ops/op_pool.c"
% endif

typedef struct { int8_t *data; size_t size; } relu_args_t;
static void relu_worker(void *arg) { relu_args_t *a = (relu_args_t *)arg; relu_int8_inplace(a->data, a->size); }

typedef struct { int8_t *data; size_t size; float scale_in; float scale_out; } requantize_args_t;
static void requantize_worker(void *arg) { requantize_args_t *a = (requantize_args_t *)arg; requantize_int8_inplace(a->data, a->size, a->scale_in, a->scale_out); }

typedef struct { int8_t *data; size_t size; } relu_l1_args_t;
static void relu_l1_worker(void *arg) { relu_l1_args_t *a = (relu_l1_args_t *)arg; relu_int8_inplace_l1(a->data, a->size); }

void requantize_l1_worker(void *arg) {
    requantize_l1_args_t *a = (requantize_l1_args_t *)arg;
    requantize_int8_inplace_l1(a->data, a->size, a->scale_in, a->scale_out);
}

// Standard Kernels (L2) - non-square pooling support
typedef struct { const int8_t *input; int8_t *output; uint16_t in_h, in_w, in_ch, out_h, out_w, kernel_h, kernel_w, stride_h, stride_w, pad_h, pad_w; } maxpool_args_t;
static void maxpool_worker(void *arg) { maxpool_args_t *a = (maxpool_args_t *)arg; network_maxpool_int8(a->input, a->output, a->in_h, a->in_w, a->in_ch, a->out_h, a->out_w, a->kernel_h, a->kernel_w, a->stride_h, a->stride_w, a->pad_h, a->pad_w); }

% if 'avgpool' in ops_present or 'global_avgpool' in ops_present or 'adaptive_avgpool' in ops_present:
typedef struct { const int8_t *input; int8_t *output; uint16_t in_h, in_w, in_ch, out_h, out_w, kernel_h, kernel_w, stride_h, stride_w; float scale_in, scale_out; } avgpool_args_t;
static void avgpool_worker(void *arg) { avgpool_args_t *a = (avgpool_args_t *)arg; network_avgpool_int8(a->input, a->output, a->in_h, a->in_w, a->in_ch, a->out_h, a->out_w, a->kernel_h, a->kernel_w, a->stride_h, a->stride_w, a->scale_in, a->scale_out); }

typedef struct { const int8_t *input; int8_t *output; uint16_t batch, ch, h, w; float scale_in, scale_out; } global_avgpool_args_t;
static void global_avgpool_worker(void *arg) { global_avgpool_args_t *a = (global_avgpool_args_t *)arg; network_global_avgpool_int8(a->input, a->output, a->batch, a->ch, a->h, a->w, a->scale_in, a->scale_out); }

typedef struct {
    const int8_t *input;
    int8_t *output;
    uint16_t batch;
    uint16_t channels;
    uint16_t input_len;
    uint16_t output_size;
    uint16_t input_stride_ch;
    uint16_t input_stride_len;
    uint32_t input_batch_stride;
} adaptive_avgpool1d_args_t;
static void adaptive_avgpool1d_worker(void *arg) {
    adaptive_avgpool1d_args_t *a = (adaptive_avgpool1d_args_t *)arg;
    network_adaptive_avgpool1d_int8(
        a->input,
        a->output,
        a->batch,
        a->channels,
        a->input_len,
        a->output_size,
        a->input_stride_ch,
        a->input_stride_len,
        a->input_batch_stride
    );
}
% endif  ## End avgpool workers

% if 'patch_embed' in ops_present or 'mhsa' in ops_present or 'transpose' in ops_present:
typedef struct { const int8_t *input; int8_t *output; uint16_t dim1; uint16_t dim2; } transpose2d_tile_args_t;
static void transpose2d_tile_worker(void *arg) {
    transpose2d_tile_args_t *a = (transpose2d_tile_args_t *)arg;
    const int core_id = pi_core_id();
    const int chunk = (a->dim1 + NUM_CORES - 1) / NUM_CORES;
    const int start = core_id * chunk;
    const int end = (start + chunk > a->dim1) ? a->dim1 : (start + chunk);

    for (int d1 = start; d1 < end; d1++) {
        const int8_t *in_row = a->input + d1 * a->dim2;
        for (int d2 = 0; d2 < a->dim2; d2++) {
            a->output[d2 * a->dim1 + d1] = in_row[d2];
        }
    }
    pi_cl_team_barrier();
}

typedef struct {
    const int8_t *conv_out;
    int8_t *output;
    uint16_t batch;
    uint16_t seq_len;
    uint16_t d_model;
    uint16_t embed_dim;
    uint16_t grid_h;
    uint16_t grid_w;
} patch_embed_permute_args_t;

static void patch_embed_permute_worker(void *arg) {
    patch_embed_permute_args_t *a = (patch_embed_permute_args_t *)arg;
    const int core_id = pi_core_id();
    const int total_tokens = (int)a->batch * (int)a->seq_len;
    const int chunk = (total_tokens + NUM_CORES - 1) / NUM_CORES;
    const int start = core_id * chunk;
    const int end = (start + chunk > total_tokens) ? total_tokens : (start + chunk);

    for (int idx = start; idx < end; idx++) {
        const int b = idx / a->seq_len;
        const int t = idx - b * a->seq_len;
        int8_t *dst = a->output + (b * a->seq_len + t) * a->d_model;
        for (int e = 0; e < a->embed_dim; e++) {
            int8_t *dst_e = dst + e * a->grid_h;
            const int base = ((b * a->embed_dim + e) * a->grid_h) * a->grid_w + t;
            const int8_t *src = a->conv_out + base;
            for (int h = 0; h < a->grid_h; h++) {
                dst_e[h] = src[h * a->grid_w];
            }
        }
    }
    pi_cl_team_barrier();
}

typedef struct {
    int8_t *buffer;
    const int8_t *pos;
    uint16_t batch;
    uint16_t seq_len;
    uint16_t d_model;
    int same_scale;
    int shift;
    int32_t mul_in;
    int32_t mul_pos;
} pos_embed_args_t;

static void pos_embed_worker(void *arg) {
    pos_embed_args_t *a = (pos_embed_args_t *)arg;
    const int core_id = pi_core_id();
    const int total_tokens = (int)a->batch * (int)a->seq_len;
    const int chunk = (total_tokens + NUM_CORES - 1) / NUM_CORES;
    const int start = core_id * chunk;
    const int end = (start + chunk > total_tokens) ? total_tokens : (start + chunk);

    const int64_t round = (a->shift > 0) ? (1LL << (a->shift - 1)) : 0;

    for (int idx = start; idx < end; idx++) {
        const int b = idx / a->seq_len;
        const int t = idx - b * a->seq_len;
        int8_t *dst = a->buffer + (b * a->seq_len + t) * a->d_model;
        const int8_t *pos_row = a->pos + t * a->d_model;

        if (a->same_scale) {
            for (int d = 0; d < a->d_model; d++) {
                int32_t sum = (int32_t)dst[d] + (int32_t)pos_row[d];
                dst[d] = (int8_t)(sum < -128 ? -128 : (sum > 127 ? 127 : sum));
            }
        } else {
            for (int d = 0; d < a->d_model; d++) {
                int64_t acc = (int64_t)dst[d] * (int64_t)a->mul_in + (int64_t)pos_row[d] * (int64_t)a->mul_pos;
                int64_t rounded = acc + (acc >= 0 ? round : -round);
                int32_t val = (int32_t)(rounded >> a->shift);
                dst[d] = (int8_t)(val < -128 ? -128 : (val > 127 ? 127 : val));
            }
        }
    }
    pi_cl_team_barrier();
}
% endif  ## End transformer/patch workers

% if 'concat' in ops_present:
typedef struct {
    int8_t *buffer;
    const uint16_t *channels_per_input;
    const float *input_scales;
    uint16_t num_inputs;
    uint16_t total_channels;
    uint32_t spatial_size;
    float scale_out;
} concat_requant_args_t;
static void concat_requant_worker(void *arg) {
    concat_requant_args_t *a = (concat_requant_args_t *)arg;
    const int core_id = pi_core_id();
    const int chunk = (a->total_channels + NUM_CORES - 1) / NUM_CORES;
    const int start_ch = core_id * chunk;
    const int end_ch = (start_ch + chunk > a->total_channels) ? a->total_channels : (start_ch + chunk);

    for (int ch = start_ch; ch < end_ch; ch++) {
        int input_idx = 0;
        int ch_offset = 0;
        while (input_idx < a->num_inputs && ch >= ch_offset + a->channels_per_input[input_idx]) {
            ch_offset += a->channels_per_input[input_idx];
            input_idx++;
        }
        if (input_idx >= a->num_inputs) continue;

        const float scale_in = a->input_scales[input_idx];
        if (fabsf(scale_in - a->scale_out) < 1e-5f) continue;

        const float scale = scale_in / a->scale_out;
        int8_t *dst = a->buffer + (uint32_t)ch * a->spatial_size;
        for (uint32_t i = 0; i < a->spatial_size; i++) {
            int32_t res = qround((float)dst[i] * scale);
            if (res > 127) res = 127;
            if (res < -128) res = -128;
            dst[i] = (int8_t)res;
        }
    }
    pi_cl_team_barrier();
}
% endif  ## End concat workers

% if 'add' in ops_present:
typedef struct { const int8_t *input_a, *input_b; int8_t *output; uint32_t size; float scale_a, scale_b, scale_out; } add_args_t;
static void add_worker(void *arg) { add_args_t *a = (add_args_t *)arg; network_add_int8(a->input_a, a->input_b, a->output, a->size, a->scale_a, a->scale_b, a->scale_out); }
% endif

% if 'concat' in ops_present:
typedef struct { const int8_t **inputs; const float *input_scales; int8_t *output; uint16_t num_inputs; const uint16_t *channels_per_input; uint16_t height; uint16_t width; float scale_output; } concat_args_t;
static void concat_worker(void *arg) { concat_args_t *a = (concat_args_t *)arg; network_concat_int8(a->inputs, a->input_scales, a->output, a->num_inputs, 1, a->channels_per_input, a->height, a->width, a->scale_output); }
% endif

% if 'ssm' in ops_present or 'mamba_block' in ops_present or 'conv1d_depthwise' in ops_present or 'silu' in ops_present:
// ---
// Mamba/SSM Workers
// ---

// Conv1D Depthwise worker
typedef struct {
    const int8_t *input;
    const int8_t *weights;
    const int32_t *bias;
    int8_t *output;
    int channels;
    int length;
    int kernel_size;
    int causal;
    float scale_input;
    float scale_weight;
    float scale_output;
} conv1d_depthwise_args_t;
static void conv1d_depthwise_worker(void *arg) {
    conv1d_depthwise_args_t *a = (conv1d_depthwise_args_t *)arg;
    network_conv1d_depthwise_int8(a->input, a->weights, a->bias, a->output,
                                   a->channels, a->length, a->kernel_size, a->causal,
                                   a->scale_input, a->scale_weight, a->scale_output);
}

// SiLU LUT worker (MAMBA)
typedef struct {
    int8_t *buffer;
    const int8_t *lut;
    int num_elements;
} silu_lut_args_t;
static void silu_lut_worker(void *arg) {
    silu_lut_args_t *a = (silu_lut_args_t *)arg;
    network_silu_int8_lut_inplace(a->buffer, a->lut, a->num_elements);
}

// FUSED Conv1d + SiLU + Transpose worker (MAMBA F1 optimization)
typedef struct {
    const int8_t *input;
    const int8_t *weights;
    const int32_t *bias;
    const int8_t *silu_lut;
    int8_t *output;
    int batch;
    int channels;
    int length;
    int kernel_size;
    int causal;
    float scale_input;
    float scale_weight;
    float scale_output;
} conv1d_silu_transpose_args_t;
static void conv1d_silu_transpose_worker(void *arg) {
    conv1d_silu_transpose_args_t *a = (conv1d_silu_transpose_args_t *)arg;
    network_conv1d_silu_transpose_fused(
        a->input, a->weights, a->bias, a->silu_lut, a->output,
        a->batch, a->channels, a->length, a->kernel_size, a->causal,
        a->scale_input, a->scale_weight, a->scale_output);
}

// FUSED Conv1d + SiLU + Transpose worker (strided input)
typedef struct {
    const int8_t *input;
    int input_stride;
    const int8_t *weights;
    const int32_t *bias;
    const int8_t *silu_lut;
    int8_t *output;
    int batch;
    int channels;
    int length;
    int kernel_size;
    int causal;
    float scale_input;
    float scale_weight;
    float scale_output;
} conv1d_silu_transpose_strided_args_t;
static void conv1d_silu_transpose_strided_worker(void *arg) {
    conv1d_silu_transpose_strided_args_t *a = (conv1d_silu_transpose_strided_args_t *)arg;
    network_conv1d_silu_transpose_fused_strided(
        a->input, a->input_stride, a->weights, a->bias, a->silu_lut, a->output,
        a->batch, a->channels, a->length, a->kernel_size, a->causal,
        a->scale_input, a->scale_weight, a->scale_output);
}

// z layout codes (shared by SSM + optional gating worker):
// - XZ_INTERLEAVED: z lives inside xz_proj rows [B, L, 2*d_inner] at +d_inner
// - CHANNEL_MAJOR:  z is laid out as [B, d_inner, L]
// - TIME_MAJOR:     z is laid out as [B, L, d_inner]
#ifndef MAMBA_SSM_Z_LAYOUT_XZ_INTERLEAVED
#define MAMBA_SSM_Z_LAYOUT_XZ_INTERLEAVED 0
#define MAMBA_SSM_Z_LAYOUT_CHANNEL_MAJOR  1
#define MAMBA_SSM_Z_LAYOUT_TIME_MAJOR     2
#endif

// Parallel Gating worker: ssm_out = ssm_out * SiLU(z)
typedef struct {
    int8_t *ssm_out;
    const int8_t *z_int8;
    const int16_t *silu_gate_lut_q13;
    int batch;
    int seq_len;
    int d_inner;
    int z_layout;  // MAMBA_SSM_Z_LAYOUT_*
} gating_args_t;
static void gating_worker(void *arg) {
    gating_args_t *a = (gating_args_t *)arg;
    int core_id = pi_core_id();
    int num_elements = a->batch * a->seq_len * a->d_inner;
    int chunk = (num_elements + NUM_CORES - 1) / NUM_CORES;
    int start = core_id * chunk;
    int end = (start + chunk < num_elements) ? (start + chunk) : num_elements;

    for (int idx = start; idx < end; idx++) {
        // Decompose flat index
        int c = idx % a->d_inner;
        int bl = idx / a->d_inner;
        int l = bl % a->seq_len;
        int b = bl / a->seq_len;

        // z value from in_proj output (layout depends on z_layout)
        int8_t z_val;
        if (a->z_layout == MAMBA_SSM_Z_LAYOUT_CHANNEL_MAJOR) {
            z_val = a->z_int8[(b * a->d_inner + c) * a->seq_len + l];
        } else if (a->z_layout == MAMBA_SSM_Z_LAYOUT_TIME_MAJOR) {
            z_val = a->z_int8[(b * a->seq_len + l) * a->d_inner + c];
        } else {
            z_val = a->z_int8[(b * a->seq_len + l) * 2 * a->d_inner + a->d_inner + c];
        }

        // Q13 SiLU lookup
        int16_t silu_q13 = a->silu_gate_lut_q13[(int)(z_val) + 128];

        // Fixed-point Q13 multiply
        int32_t prod = (int32_t)a->ssm_out[idx] * (int32_t)silu_q13;
        int32_t rounded = (prod + (1 << 12)) >> 13;

        // Clip to INT8
        a->ssm_out[idx] = (int8_t)(rounded < -128 ? -128 : (rounded > 127 ? 127 : rounded));
    }
    pi_cl_team_barrier();
}

// ---
// BLOCK-PARALLEL SSM (Optimized: Loop Inversion Strategy)
// ---
// Key insight: Channels are INDEPENDENT. The sequential dependency h[t] = f(h[t-1])
// only exists WITHIN a single channel. We were synchronizing across channels unnecessarily!
//
// OLD (Synchronized Time Loop): 32 barriers per block
//   for t in 0..L-1:
//     parallel: x_proj(t) -> barrier
//     parallel: dt_proj(t) -> barrier
//     parallel: state_update(t) -> barrier
//
// NEW (Block-Parallel): Only 2 barriers per block
//   Stage 1: Batch x_proj for ALL timesteps -> barrier
//   Stage 1.5: Extract B/C (core 0)
//   Stage 2: Batch dt_proj (delta reuse) -> barrier
//   Stage 3: Channel scan (sequential over time, NO barriers)
// ---

typedef struct {
    const int8_t *x_int8;           // Input [batch, seq_len, d_inner]
    int8_t *output_int8;            // Output [batch, seq_len, d_inner] (gated)
    const int8_t *z_int8;           // z buffer (layout depends on z_layout)
    const int8_t *x_proj_weight;    // [dt_rank + 2*d_state, d_inner]
    // Optional: L1 tile buffer for x_proj weights (big-model path)
    int8_t *x_proj_weight_tile_l1;
    int x_proj_tile_out;
    // Optional: L1 staging buffer for PH1 x[t] (per-core, big-model path)
    int8_t *x_int8_tile_l1;
    int x_int8_tile_stride;
    // Optional: L1 delta staging for PH2 (reuse x_proj tile buffer, big-model path)
    int enable_delta_l1_staging;
    // Optional: PH3 x[t] time-tiling into L1 (reuse leftover x_proj tile buffer, big-model path)
    int enable_ph3_x_l1_time_tiling;
    // Optional: PH3 input fallback when full-seq x/z staging doesn't fit â€” stage per-channel tiles into L1.
    int ph3_need_x_l1_ch_tiling;
    int ph3_need_z_l1_ch_tiling;
    const int8_t *dt_proj_weight;   // [d_inner, dt_rank]
    const int32_t *dt_proj_bias_q16_16;
    // Optional: L1 tile buffers for dt_proj weights/bias (big-model path)
    int8_t *dt_proj_weight_tile_l1;
    int32_t *dt_proj_bias_tile_l1;
    int dt_proj_tile_ch;
    const int16_t *A_q15;           // [d_state, d_inner]
    const int16_t *D_q15;           // [d_inner]
    const int16_t *softplus_lut;
    const int16_t *exp_lut;
    const int16_t *silu_gate_lut_q13;
    int16_t *h_state_q15;           // Hidden state [batch, d_inner, d_state]
	// Block-parallel buffers (allocated in L1/L2 scratch)
	int32_t *proj_all;              // [seq_len, dt_rank + 2*d_state] - all x_proj outputs
	int16_t *dt_all;                // [d_inner, seq_len] - all dt values (channel-major)
    int dt_all_in_l1;               // 1 if dt_all points into L1/TCDM (avoid redundant PH3 staging)
	int16_t *B_all;                 // [seq_len, d_state] - B for all timesteps
	int16_t *C_all;                 // [seq_len, d_state] - C for all timesteps
	int batch, seq_len, d_inner, d_state, dt_rank;
	int z_layout;                   // MAMBA_SSM_Z_LAYOUT_*
    int32_t dt_scale_q, bc_scale_factor, output_scale_q;
    int dt_scale_shift;
} ssm_gated_args_t;

#ifdef ENABLE_MAMBA_SSM_EVENT_PROFILING
// Default to the 2 most useful stall signals; override via `-DMAMBA_SSM_EVENT_MASK=...` if needed.
#ifndef MAMBA_SSM_EVENT_MASK
#define MAMBA_SSM_EVENT_MASK ((1U << PI_PERF_LD_STALL) | (1U << PI_PERF_TCDM_CONT))
#endif
// Convenience masks (pass e.g. `-DMAMBA_SSM_EVENT_MASK=MAMBA_SSM_EVENT_MASK_STALLS_EXT`).
#ifndef MAMBA_SSM_EVENT_MASK_STALLS
#define MAMBA_SSM_EVENT_MASK_STALLS ((1U << PI_PERF_LD_STALL) | (1U << PI_PERF_TCDM_CONT))
#endif
#ifndef MAMBA_SSM_EVENT_MASK_STALLS_EXT
#define MAMBA_SSM_EVENT_MASK_STALLS_EXT (MAMBA_SSM_EVENT_MASK_STALLS | (1U << PI_PERF_LD_EXT_CYC) | (1U << PI_PERF_ST_EXT_CYC))
#endif
#endif

// Upper bound for `dt_rank` when using the fast delta-local reuse path in PH2.
// FEMBA-tiny uses `dt_rank=96` (d_inner/16), so keep this at least that large.
#ifndef MAMBA_SSM_MAX_DT_RANK
#define MAMBA_SSM_MAX_DT_RANK 96
#endif

// Big-model fallback: stage dt_proj weights into L1 by channel tiles to avoid re-reading large
// dt_proj weights from L2/EXT for every timestep.
#ifndef MAMBA_SSM_DT_PROJ_TILE_CH
#define MAMBA_SSM_DT_PROJ_TILE_CH 32
#endif
#ifndef MAMBA_SSM_DT_PROJ_TILE_ALIGN
#define MAMBA_SSM_DT_PROJ_TILE_ALIGN 16
#endif

// Big-model fallback: stage x_proj weights into L1 by output-feature tiles to avoid re-reading large
// x_proj weights from L2/EXT for every timestep.
#ifndef MAMBA_SSM_X_PROJ_TILE_OUT
#define MAMBA_SSM_X_PROJ_TILE_OUT 128
#endif
#ifndef MAMBA_SSM_X_PROJ_TILE_ALIGN
#define MAMBA_SSM_X_PROJ_TILE_ALIGN 16
#endif

// Optional: stage delta[t, r] = (proj_all[t, r] >> 8) into L1 to avoid repeatedly
// re-reading proj_all from L2/EXT for every dt_proj channel tile in PH2.
#ifndef MAMBA_SSM_DELTA_L1_ALIGN
#define MAMBA_SSM_DELTA_L1_ALIGN 4
#endif

// L1 staging layout for SSM inputs (x/z). Align and add a small pad between the buffers to help reduce
// worst-case bank conflicts when all cores stream x and z concurrently in PH3.
#ifndef MAMBA_SSM_L1_IO_ALIGN
#define MAMBA_SSM_L1_IO_ALIGN 16
#endif
#ifndef MAMBA_SSM_L1_IO_PAD_BYTES
#define MAMBA_SSM_L1_IO_PAD_BYTES 64
#endif

// Optional: PH3 input time-tiling. When PH3 input channel-tiling cannot stage the full
// [seq_len] for a channel tile (usually due to very long seq_len under tight L1),
// stage x/z in smaller time windows to keep the scan hot loop on TCDM.
#ifndef MAMBA_SSM_PH3_IO_TIME_TILE
#define MAMBA_SSM_PH3_IO_TIME_TILE 64
#endif
// Only consider time-tiling when full-seq channel-tiling produces a very small channel tile.
// This avoids regressing the common case where full-seq staging already uses a reasonably large tile.
#ifndef MAMBA_SSM_PH3_IO_TIME_TILING_TRIGGER_CH
#define MAMBA_SSM_PH3_IO_TIME_TILING_TRIGGER_CH 16
#endif

#if defined(ENABLE_MAMBA_SSM_PH3_IO_L1_TIME_TILING) && !defined(DISABLE_MAMBA_SSM_PH3_IO_L1_TIME_TILING)
// Keep this out-of-line to avoid inflating the stack frame of `ssm_gated_worker` (the
// worker is already register-heavy).
__attribute__((noinline)) static int ssm_ph3_io_time_tiling(
    ssm_gated_args_t *a,
    int core_id,
    int b,
    int need_x,
    int need_z,
    int stage_x,
    int stage_z,
    int tile_ch,
    uintptr_t io_base,
    uintptr_t l1_end,
    size_t avail_bytes,
    size_t pad,
    uintptr_t align_mask,
    const int16_t *B_all_base,
    const int16_t *C_all_base
) {
    const int OUTPUT_SHIFT = 24;

    // Try a PH3 time-tiling fallback when full-seq channel-tiling can't stage the required planes.
    // This keeps the scan compute on TCDM without per-timestep barriers (only per time-tile).
    if (a->seq_len <= MAMBA_SSM_PH3_IO_TIME_TILE) return 0;

    int time_tile_len = MAMBA_SSM_PH3_IO_TIME_TILE;
    if (time_tile_len > a->seq_len) time_tile_len = a->seq_len;

    // Conservative sizing: stage only the required x/z planes for a [time_tile_len] window.
    // Optionally keep h_state for the channel tile in L1 to avoid re-loading it per time tile.
    size_t per_ch = 0;
    if (need_x) per_ch += (size_t)time_tile_len;
    if (need_z) per_ch += (size_t)time_tile_len;
#if defined(ENABLE_MAMBA_SSM_PH3_HSTATE_L1_CH_TILING) && !defined(DISABLE_MAMBA_SSM_PH3_HSTATE_L1_CH_TILING)
    per_ch += (size_t)a->d_state * sizeof(int16_t);
#endif

    // Pad is only needed when staging both x and z (separate buffers).
    const size_t overhead = (need_x && need_z) ? pad : 0;

    int tile_ch_tt = 0;
    if (per_ch > 0 && avail_bytes > overhead) {
        tile_ch_tt = (int)((avail_bytes - overhead) / per_ch);
        tile_ch_tt &= ~3;
    }
    if (tile_ch_tt > a->d_inner) tile_ch_tt = a->d_inner & ~3;
    if (tile_ch_tt < 4) return 0;

    // Decide whether time-tiling is worth it:
    // - If full-seq channel-tiling can't stage the required planes -> use time-tiling.
    // - If full-seq channel-tiling can stage both (or the only needed plane) but tile_ch is tiny,
    //   time-tiling can increase tile size and reduce barrier/loop overhead.
    const int want_both = (need_x && need_z);
    const int have_both = (stage_x && stage_z);
    const int have_any = (stage_x || stage_z);
    int want_time_tiling = 0;
    if (want_both) {
        if (!have_both) want_time_tiling = 1;
        else if (tile_ch <= MAMBA_SSM_PH3_IO_TIME_TILING_TRIGGER_CH && tile_ch_tt > tile_ch) want_time_tiling = 1;
    } else {
        if (!have_any) want_time_tiling = 1;
        else if (tile_ch <= MAMBA_SSM_PH3_IO_TIME_TILING_TRIGGER_CH && tile_ch_tt > tile_ch) want_time_tiling = 1;
    }
    if (!want_time_tiling) return 0;

    // Allocate a temporary L1 layout:
    //   [optional h_state_tile] [x_tile] [pad/align] [z_tile]
    uintptr_t cur_tt = io_base;
    int16_t *h_state_tile_l1 = NULL;
    int8_t *x_tile_l1 = NULL;
    int8_t *z_tile_l1 = NULL;

    const size_t hstate_bytes_per_ch = (size_t)a->d_state * sizeof(int16_t);
    const size_t xz_bytes_per_ch = (size_t)time_tile_len;

    int use_hstate_tiling = 0;
#if defined(ENABLE_MAMBA_SSM_PH3_HSTATE_L1_CH_TILING) && !defined(DISABLE_MAMBA_SSM_PH3_HSTATE_L1_CH_TILING)
    use_hstate_tiling = 1;
#endif

    if (use_hstate_tiling) {
        // Align for int16/int32 access.
        cur_tt = (cur_tt + 3) & ~(uintptr_t)3;
        h_state_tile_l1 = (int16_t *)cur_tt;
        cur_tt += (uintptr_t)(hstate_bytes_per_ch * (size_t)tile_ch_tt);
        // Align next buffer for TCDM bank friendliness.
        cur_tt = (cur_tt + align_mask) & ~align_mask;
    }

    if (need_x) {
        x_tile_l1 = (int8_t *)cur_tt;
        cur_tt += (uintptr_t)(xz_bytes_per_ch * (size_t)tile_ch_tt);
        if (need_z) {
            cur_tt += (uintptr_t)MAMBA_SSM_L1_IO_PAD_BYTES;
            cur_tt = (cur_tt + align_mask) & ~align_mask;
        }
    }

    if (need_z) {
        z_tile_l1 = (int8_t *)cur_tt;
        cur_tt += (uintptr_t)(xz_bytes_per_ch * (size_t)tile_ch_tt);
    }

    if (cur_tt > l1_end) return 0;

    const int z_tile_is_channel_major = need_z && (a->z_layout == MAMBA_SSM_Z_LAYOUT_CHANNEL_MAJOR);

    for (int tile0 = 0; tile0 < a->d_inner; tile0 += tile_ch_tt) {
        int tile_len = a->d_inner - tile0;
        if (tile_len > tile_ch_tt) tile_len = tile_ch_tt;

        // Stage h_state for this channel tile once (optional).
        if (use_hstate_tiling) {
            if (core_id == 0) {
                pi_cl_dma_copy_t hs_dma;
                hs_dma.dir = PI_CL_DMA_DIR_EXT2LOC;
                hs_dma.merge = 0;
                hs_dma.size = (uint32_t)((size_t)tile_len * hstate_bytes_per_ch);
                const int16_t *src = a->h_state_q15 + (size_t)b * (size_t)a->d_inner * (size_t)a->d_state + (size_t)tile0 * (size_t)a->d_state;
                hs_dma.ext = (uint32_t)src;
                hs_dma.loc = (uint32_t)h_state_tile_l1;
                pi_cl_dma_memcpy(&hs_dma);
                pi_cl_dma_wait(&hs_dma);
            }
            pi_cl_team_barrier();
        }

        // Distribute channels in this tile across cores.
        const int tile_channels_per_core = (tile_len + NUM_CORES - 1) / NUM_CORES;
        int m_tile_start = tile0 + core_id * tile_channels_per_core;
        int m_tile_end = m_tile_start + tile_channels_per_core;
        if (m_tile_end > tile0 + tile_len) m_tile_end = tile0 + tile_len;
        if (m_tile_start > tile0 + tile_len) m_tile_start = tile0 + tile_len;

        for (int t0 = 0; t0 < a->seq_len; t0 += time_tile_len) {
            int t_len = a->seq_len - t0;
            if (t_len > time_tile_len) t_len = time_tile_len;

            if (core_id == 0) {
                if (need_x) {
                    // x is time-major [B, L, d_inner]; stage a [t_len, tile_len] slice.
                    pi_cl_dma_copy_2d_t x_dma;
                    x_dma.dir = PI_CL_DMA_DIR_EXT2LOC;
                    x_dma.merge = 0;
                    x_dma.length = (uint32_t)tile_len;
                    x_dma.stride = (uint32_t)a->d_inner;
                    x_dma.size = (uint32_t)((size_t)t_len * (size_t)tile_len);

                    const int8_t *src = a->x_int8 + ((size_t)b * (size_t)a->seq_len + (size_t)t0) * (size_t)a->d_inner + (size_t)tile0;
                    x_dma.ext = (uint32_t)src;
                    x_dma.loc = (uint32_t)x_tile_l1;
                    pi_cl_dma_memcpy_2d(&x_dma);
                    pi_cl_dma_wait(&x_dma);
                }

                if (need_z) {
                    if (z_tile_is_channel_major) {
                        // z is [B, d_inner, L]; stage a [tile_len, t_len] slice (channel-major).
                        pi_cl_dma_copy_2d_t z_dma;
                        z_dma.dir = PI_CL_DMA_DIR_EXT2LOC;
                        z_dma.merge = 0;
                        z_dma.length = (uint32_t)t_len;
                        z_dma.stride = (uint32_t)a->seq_len;
                        z_dma.size = (uint32_t)((size_t)tile_len * (size_t)t_len);

                        const int8_t *src = a->z_int8 + ((size_t)b * (size_t)a->d_inner + (size_t)tile0) * (size_t)a->seq_len + (size_t)t0;
                        z_dma.ext = (uint32_t)src;
                        z_dma.loc = (uint32_t)z_tile_l1;
                        pi_cl_dma_memcpy_2d(&z_dma);
                        pi_cl_dma_wait(&z_dma);
                    } else {
                        // z is time-major ([B, L, d_inner] or interleaved in xz_proj); stage a [t_len, tile_len] slice.
                        pi_cl_dma_copy_2d_t z_dma;
                        z_dma.dir = PI_CL_DMA_DIR_EXT2LOC;
                        z_dma.merge = 0;
                        z_dma.length = (uint32_t)tile_len;
                        z_dma.stride = (uint32_t)((a->z_layout == MAMBA_SSM_Z_LAYOUT_XZ_INTERLEAVED) ? (2 * a->d_inner) : a->d_inner);
                        z_dma.size = (uint32_t)((size_t)t_len * (size_t)tile_len);

                        const int8_t *src;
                        if (a->z_layout == MAMBA_SSM_Z_LAYOUT_XZ_INTERLEAVED) {
                            src = a->z_int8 + ((size_t)b * (size_t)a->seq_len + (size_t)t0) * (size_t)(2 * a->d_inner) + (size_t)a->d_inner + (size_t)tile0;
                        } else {
                            src = a->z_int8 + ((size_t)b * (size_t)a->seq_len + (size_t)t0) * (size_t)a->d_inner + (size_t)tile0;
                        }
                        z_dma.ext = (uint32_t)src;
                        z_dma.loc = (uint32_t)z_tile_l1;
                        pi_cl_dma_memcpy_2d(&z_dma);
                        pi_cl_dma_wait(&z_dma);
                    }
                }
            }
            pi_cl_team_barrier();  // time tile ready

            for (int m = m_tile_start; m < m_tile_end; m++) {
                // Load hidden state into local registers
                int16_t h_local[16];
                int h_base = b * a->d_inner * a->d_state + m * a->d_state;
                const int state_off = (m - tile0) * a->d_state;
                const int16_t *h_src = use_hstate_tiling ? (h_state_tile_l1 + state_off) : (a->h_state_q15 + h_base);
                for (int d = 0; d < a->d_state && d < 16; d++) {
                    h_local[d] = h_src[d];
                }

                // Channel-specific constants
                int16_t D_val = a->D_q15[m];
                int16_t A_local[16];
                for (int d = 0; d < a->d_state && d < 16; d++) {
                    A_local[d] = a->A_q15[d * a->d_inner + m];
                }
                const int16_t *dt_m = &a->dt_all[m * a->seq_len];

                // x (staged) is time-major [t_len, tile_len] in L1.
                const int8_t *x_base = need_x ? (x_tile_l1 + (m - tile0)) : (a->x_int8 + ((size_t)b * (size_t)a->seq_len + (size_t)t0) * (size_t)a->d_inner + (size_t)m);
                int x_stride = need_x ? tile_len : a->d_inner;

                // z (staged) can be channel-major or time-major in L1 depending on z_layout.
                const int8_t *z_base;
                int z_stride;
                if (need_z) {
                    if (z_tile_is_channel_major) {
                        z_base = z_tile_l1 + (size_t)(m - tile0) * (size_t)t_len;
                        z_stride = 1;
                    } else {
                        z_base = z_tile_l1 + (m - tile0);
                        z_stride = tile_len;
                    }
                } else {
                    if (a->z_layout == MAMBA_SSM_Z_LAYOUT_CHANNEL_MAJOR) {
                        z_base = a->z_int8 + ((size_t)b * (size_t)a->d_inner + (size_t)m) * (size_t)a->seq_len + (size_t)t0;
                        z_stride = 1;
                    } else if (a->z_layout == MAMBA_SSM_Z_LAYOUT_TIME_MAJOR) {
                        z_base = a->z_int8 + ((size_t)b * (size_t)a->seq_len + (size_t)t0) * (size_t)a->d_inner + (size_t)m;
                        z_stride = a->d_inner;
                    } else {
                        z_base = a->z_int8 + ((size_t)b * (size_t)a->seq_len + (size_t)t0) * (size_t)(2 * a->d_inner) + (size_t)a->d_inner + (size_t)m;
                        z_stride = 2 * a->d_inner;
                    }
                }

                // Output pointer for this time tile
                int8_t *out_base = a->output_int8 + ((size_t)b * (size_t)a->seq_len + (size_t)t0) * (size_t)a->d_inner + (size_t)m;

                // Sequential time loop - NO per-timestep barriers.
                #define CHANNEL_STATE_UPDATE_TT(d) do {                     int16_t A_val = A_local[d];                     int32_t dt_A_q23 = (int32_t)dt_val * (int32_t)A_val;                     int32_t dt_A_q15 = dt_A_q23 >> 8;                     int32_t exp_idx = (dt_A_q15 * 10) >> 15;                     exp_idx = (exp_idx > 127) ? 127 : ((exp_idx < -128) ? -128 : exp_idx);                     int16_t dA_q15 = a->exp_lut[exp_idx + 128];                     int32_t dB_q23 = (int32_t)dt_val * (int32_t)B_t[d];                     int16_t dB_q15 = (int16_t)(dB_q23 >> 8);                     int32_t h_decay = ((int32_t)dA_q15 * (int32_t)h_local[d]) >> 15;                     int32_t h_input = ((int32_t)dB_q15 * (int32_t)x_i8);                     int32_t h_new = h_decay + (h_input >> 7);                     h_new = (h_new > 32767) ? 32767 : ((h_new < -32768) ? -32768 : h_new);                     h_local[d] = (int16_t)h_new;                     y_acc += ((int32_t)C_t[d] * h_new) >> 15;                 } while(0)

                if (a->d_state == 4) {
                    const int16_t *B_seg = &B_all_base[t0 * 4];
                    const int16_t *C_seg = &C_all_base[t0 * 4];
                    const int16_t *dt_seg = dt_m + t0;
                    for (int tt = 0; tt < t_len; tt++) {
                        const int16_t *B_t = &B_seg[tt * 4];
                        const int16_t *C_t = &C_seg[tt * 4];
                        int8_t x_i8 = x_base[tt * x_stride];
                        int8_t z_val = z_base[tt * z_stride];
                        int16_t dt_val = dt_seg[tt];

                        int32_t y_acc = 0;
                        CHANNEL_STATE_UPDATE_TT(0);
                        CHANNEL_STATE_UPDATE_TT(1);
                        CHANNEL_STATE_UPDATE_TT(2);
                        CHANNEL_STATE_UPDATE_TT(3);

                        y_acc += ((int32_t)D_val * (int32_t)x_i8);
                        int64_t y_scaled = (int64_t)y_acc * (int64_t)a->output_scale_q;
                        int32_t ssm_out = (int32_t)((y_scaled + (1LL << (OUTPUT_SHIFT - 1))) >> OUTPUT_SHIFT);
                        ssm_out = (ssm_out > 127) ? 127 : ((ssm_out < -128) ? -128 : ssm_out);

                        int16_t silu_q13 = a->silu_gate_lut_q13[(int)z_val + 128];
                        int32_t gated = ((int32_t)ssm_out * (int32_t)silu_q13 + (1 << 12)) >> 13;
                        gated = (gated > 127) ? 127 : ((gated < -128) ? -128 : gated);

                        out_base[tt * a->d_inner] = (int8_t)gated;
                    }
                } else if (a->d_state == 8) {
                    const int16_t *B_seg = &B_all_base[t0 * 8];
                    const int16_t *C_seg = &C_all_base[t0 * 8];
                    const int16_t *dt_seg = dt_m + t0;
                    for (int tt = 0; tt < t_len; tt++) {
                        const int16_t *B_t = &B_seg[tt * 8];
                        const int16_t *C_t = &C_seg[tt * 8];
                        int8_t x_i8 = x_base[tt * x_stride];
                        int8_t z_val = z_base[tt * z_stride];
                        int16_t dt_val = dt_seg[tt];

                        int32_t y_acc = 0;
                        CHANNEL_STATE_UPDATE_TT(0); CHANNEL_STATE_UPDATE_TT(1);
                        CHANNEL_STATE_UPDATE_TT(2); CHANNEL_STATE_UPDATE_TT(3);
                        CHANNEL_STATE_UPDATE_TT(4); CHANNEL_STATE_UPDATE_TT(5);
                        CHANNEL_STATE_UPDATE_TT(6); CHANNEL_STATE_UPDATE_TT(7);

                        y_acc += ((int32_t)D_val * (int32_t)x_i8);
                        int64_t y_scaled = (int64_t)y_acc * (int64_t)a->output_scale_q;
                        int32_t ssm_out = (int32_t)((y_scaled + (1LL << (OUTPUT_SHIFT - 1))) >> OUTPUT_SHIFT);
                        ssm_out = (ssm_out > 127) ? 127 : ((ssm_out < -128) ? -128 : ssm_out);

                        int16_t silu_q13 = a->silu_gate_lut_q13[(int)z_val + 128];
                        int32_t gated = ((int32_t)ssm_out * (int32_t)silu_q13 + (1 << 12)) >> 13;
                        gated = (gated > 127) ? 127 : ((gated < -128) ? -128 : gated);

                        out_base[tt * a->d_inner] = (int8_t)gated;
                    }
                } else if (a->d_state == 16) {
                    const int16_t *B_seg = &B_all_base[t0 * 16];
                    const int16_t *C_seg = &C_all_base[t0 * 16];
                    const int16_t *dt_seg = dt_m + t0;
                    for (int tt = 0; tt < t_len; tt++) {
                        const int16_t *B_t = &B_seg[tt * 16];
                        const int16_t *C_t = &C_seg[tt * 16];
                        int8_t x_i8 = x_base[tt * x_stride];
                        int8_t z_val = z_base[tt * z_stride];
                        int16_t dt_val = dt_seg[tt];

                        int32_t y_acc = 0;
                        CHANNEL_STATE_UPDATE_TT(0);  CHANNEL_STATE_UPDATE_TT(1);
                        CHANNEL_STATE_UPDATE_TT(2);  CHANNEL_STATE_UPDATE_TT(3);
                        CHANNEL_STATE_UPDATE_TT(4);  CHANNEL_STATE_UPDATE_TT(5);
                        CHANNEL_STATE_UPDATE_TT(6);  CHANNEL_STATE_UPDATE_TT(7);
                        CHANNEL_STATE_UPDATE_TT(8);  CHANNEL_STATE_UPDATE_TT(9);
                        CHANNEL_STATE_UPDATE_TT(10); CHANNEL_STATE_UPDATE_TT(11);
                        CHANNEL_STATE_UPDATE_TT(12); CHANNEL_STATE_UPDATE_TT(13);
                        CHANNEL_STATE_UPDATE_TT(14); CHANNEL_STATE_UPDATE_TT(15);

                        y_acc += ((int32_t)D_val * (int32_t)x_i8);
                        int64_t y_scaled = (int64_t)y_acc * (int64_t)a->output_scale_q;
                        int32_t ssm_out = (int32_t)((y_scaled + (1LL << (OUTPUT_SHIFT - 1))) >> OUTPUT_SHIFT);
                        ssm_out = (ssm_out > 127) ? 127 : ((ssm_out < -128) ? -128 : ssm_out);

                        int16_t silu_q13 = a->silu_gate_lut_q13[(int)z_val + 128];
                        int32_t gated = ((int32_t)ssm_out * (int32_t)silu_q13 + (1 << 12)) >> 13;
                        gated = (gated > 127) ? 127 : ((gated < -128) ? -128 : gated);

                        out_base[tt * a->d_inner] = (int8_t)gated;
                    }
                } else {
                    const int16_t *B_seg = &B_all_base[t0 * a->d_state];
                    const int16_t *C_seg = &C_all_base[t0 * a->d_state];
                    const int16_t *dt_seg = dt_m + t0;
                    for (int tt = 0; tt < t_len; tt++) {
                        const int16_t *B_t = &B_seg[tt * a->d_state];
                        const int16_t *C_t = &C_seg[tt * a->d_state];
                        int8_t x_i8 = x_base[tt * x_stride];
                        int8_t z_val = z_base[tt * z_stride];
                        int16_t dt_val = dt_seg[tt];

                        int32_t y_acc = 0;
                        for (int d = 0; d < a->d_state; d++) {
                            int16_t A_val = A_local[d];
                            int32_t dt_A_q23 = (int32_t)dt_val * (int32_t)A_val;
                            int32_t dt_A_q15 = dt_A_q23 >> 8;
                            int32_t exp_idx = (dt_A_q15 * 10) >> 15;
                            exp_idx = (exp_idx > 127) ? 127 : ((exp_idx < -128) ? -128 : exp_idx);
                            int16_t dA_q15 = a->exp_lut[exp_idx + 128];
                            int32_t dB_q23 = (int32_t)dt_val * (int32_t)B_t[d];
                            int16_t dB_q15 = (int16_t)(dB_q23 >> 8);
                            int32_t h_decay = ((int32_t)dA_q15 * (int32_t)h_local[d]) >> 15;
                            int32_t h_input = ((int32_t)dB_q15 * (int32_t)x_i8);
                            int32_t h_new = h_decay + (h_input >> 7);
                            h_new = (h_new > 32767) ? 32767 : ((h_new < -32768) ? -32768 : h_new);
                            h_local[d] = (int16_t)h_new;
                            y_acc += ((int32_t)C_t[d] * h_new) >> 15;
                        }

                        y_acc += ((int32_t)D_val * (int32_t)x_i8);
                        int64_t y_scaled = (int64_t)y_acc * (int64_t)a->output_scale_q;
                        int32_t ssm_out = (int32_t)((y_scaled + (1LL << (OUTPUT_SHIFT - 1))) >> OUTPUT_SHIFT);
                        ssm_out = (ssm_out > 127) ? 127 : ((ssm_out < -128) ? -128 : ssm_out);

                        int16_t silu_q13 = a->silu_gate_lut_q13[(int)z_val + 128];
                        int32_t gated = ((int32_t)ssm_out * (int32_t)silu_q13 + (1 << 12)) >> 13;
                        gated = (gated > 127) ? 127 : ((gated < -128) ? -128 : gated);

                        out_base[tt * a->d_inner] = (int8_t)gated;
                    }
                }

                #undef CHANNEL_STATE_UPDATE_TT

                // Write back hidden state (to L1 tile or L2).
                int16_t *h_dst = use_hstate_tiling ? (h_state_tile_l1 + state_off) : (a->h_state_q15 + h_base);
                for (int d = 0; d < a->d_state && d < 16; d++) {
                    h_dst[d] = h_local[d];
                }
            }

            // Ensure all cores are done before the next DMA overwrites the time tile.
            pi_cl_team_barrier();
        }

        // Flush h_state tile back to L2 once per channel tile (optional).
        if (use_hstate_tiling) {
            pi_cl_team_barrier();
            if (core_id == 0) {
                pi_cl_dma_copy_t hs_dma;
                hs_dma.dir = PI_CL_DMA_DIR_LOC2EXT;
                hs_dma.merge = 0;
                hs_dma.size = (uint32_t)((size_t)tile_len * hstate_bytes_per_ch);
                int16_t *dst = a->h_state_q15 + (size_t)b * (size_t)a->d_inner * (size_t)a->d_state + (size_t)tile0 * (size_t)a->d_state;
                hs_dma.ext = (uint32_t)dst;
                hs_dma.loc = (uint32_t)h_state_tile_l1;
                pi_cl_dma_memcpy(&hs_dma);
                pi_cl_dma_wait(&hs_dma);
            }
            pi_cl_team_barrier();
        }
    }

    // Keep cores in sync before returning to caller.
    pi_cl_team_barrier();
    return 1;
}
#endif

static void ssm_gated_worker(void *arg) {
    ssm_gated_args_t *a = (ssm_gated_args_t *)arg;
    int core_id = pi_core_id();

    const int BC_SHIFT = 16;
    const int OUTPUT_SHIFT = 24;
    const int simd_count = a->d_inner >> 2;
    const int proj_size = a->dt_rank + 2 * a->d_state;  // e.g., 4 + 8 = 12

    // Distribute work across cores for each stage
    // Stage 1 (x_proj): parallelize by timestep; compute 4 outputs at a time
    // Stage 2 (dt_proj): parallelize by channel; reuse delta per timestep
    // Stage 3 (scan): 64 channels / 8 cores = 8 channels each

    // Stage 1 (x_proj): parallelize by timestep so each core can reuse x[t] across multiple output features.
    const int timesteps_per_core = (a->seq_len + NUM_CORES - 1) / NUM_CORES;
    const int t_start = core_id * timesteps_per_core;
    const int t_end = (t_start + timesteps_per_core > a->seq_len) ? a->seq_len : (t_start + timesteps_per_core);

    const int channels_per_core = (a->d_inner + NUM_CORES - 1) / NUM_CORES;  // 8
    const int ch_start = core_id * channels_per_core;
    const int ch_end = (ch_start + channels_per_core > a->d_inner) ? a->d_inner : (ch_start + channels_per_core);

#ifdef ENABLE_MAMBA_SSM_EVENT_PROFILING
    // Configure per-core event counters (do NOT include PI_PERF_CYCLES here; it's a shared timer).
    pi_perf_conf(MAMBA_SSM_EVENT_MASK);
    pi_perf_start();
#endif

    // Process each batch
    for (int b = 0; b < a->batch; b++) {
#ifdef ENABLE_MAMBA_SSM_EVENT_PROFILING
        uint32_t phase_cycles_start = 0;
        uint32_t phase_ld_stall_start = 0;
        uint32_t phase_tcdm_cont_start = 0;
        uint32_t phase_ld_ext_cyc_start = 0;
        if (core_id == 0 && b == 0) {
            phase_cycles_start = pi_perf_read(PI_PERF_CYCLES);
            phase_ld_stall_start = pi_perf_read(PI_PERF_LD_STALL);
            phase_tcdm_cont_start = pi_perf_read(PI_PERF_TCDM_CONT);
            phase_ld_ext_cyc_start = pi_perf_read(PI_PERF_LD_EXT_CYC);
        }
#endif
        // ---
        // STAGE 1: Batched x_proj for ALL timesteps (PARALLEL over timesteps)
        // Input: x[L, d_inner], Weight: [proj_size, d_inner]
        // Output: proj_all[L, proj_size] = [16, 12]
        // Implementation: compute 4 output features at a time to reuse x[t] loads.
        // ---
        if (a->x_proj_weight_tile_l1 && a->x_proj_tile_out > 0) {
            const int tile_out = a->x_proj_tile_out;
            for (int tile0 = 0; tile0 < proj_size; tile0 += tile_out) {
                const int tile1 = (tile0 + tile_out < proj_size) ? (tile0 + tile_out) : proj_size;
                const int tile_n = tile1 - tile0;

                if (core_id == 0) {
                    pi_cl_dma_copy_t dma_w;
                    dma_w.dir = PI_CL_DMA_DIR_EXT2LOC;
                    dma_w.size = (uint32_t)((size_t)tile_n * (size_t)a->d_inner);
                    dma_w.ext = (uint32_t)&a->x_proj_weight[tile0 * a->d_inner];
                    dma_w.loc = (uint32_t)a->x_proj_weight_tile_l1;
                    dma_w.merge = 0;
                    pi_cl_dma_memcpy(&dma_w);
                    pi_cl_dma_wait(&dma_w);
                }
                pi_cl_team_barrier();  // weights tile ready

                for (int t = t_start; t < t_end; t++) {
                    const int8_t *x_ptr_ext = &a->x_int8[(b * a->seq_len + t) * a->d_inner];
                    const int8_t *x_ptr = x_ptr_ext;
                    if (a->x_int8_tile_l1 && a->x_int8_tile_stride > 0) {
                        int8_t *x_l1 = a->x_int8_tile_l1 + core_id * a->x_int8_tile_stride;
                        pi_cl_dma_copy_t dma_x;
                        dma_x.dir = PI_CL_DMA_DIR_EXT2LOC;
                        dma_x.size = (uint32_t)a->d_inner;
                        dma_x.ext = (uint32_t)x_ptr_ext;
                        dma_x.loc = (uint32_t)x_l1;
                        dma_x.merge = 0;
                        pi_cl_dma_memcpy(&dma_x);
                        pi_cl_dma_wait(&dma_x);
                        x_ptr = x_l1;
                    }
                    const v4s *pA = (const v4s *)x_ptr;

                    int j = tile0;
                    for (; j + 3 < tile1; j += 4) {
                        const int j_rel = j - tile0;
                        const int8_t *w_ptr0 = &a->x_proj_weight_tile_l1[(j_rel + 0) * a->d_inner];
                        const int8_t *w_ptr1 = &a->x_proj_weight_tile_l1[(j_rel + 1) * a->d_inner];
                        const int8_t *w_ptr2 = &a->x_proj_weight_tile_l1[(j_rel + 2) * a->d_inner];
                        const int8_t *w_ptr3 = &a->x_proj_weight_tile_l1[(j_rel + 3) * a->d_inner];

                        const v4s *pB0 = (const v4s *)w_ptr0;
                        const v4s *pB1 = (const v4s *)w_ptr1;
                        const v4s *pB2 = (const v4s *)w_ptr2;
                        const v4s *pB3 = (const v4s *)w_ptr3;

                        int32_t acc0 = 0, acc1 = 0, acc2 = 0, acc3 = 0;
                        for (int i = 0; i < simd_count; i++) {
                            v4s x = pA[i];
                            acc0 = SumDotpSS(x, pB0[i], acc0);
                            acc1 = SumDotpSS(x, pB1[i], acc1);
                            acc2 = SumDotpSS(x, pB2[i], acc2);
                            acc3 = SumDotpSS(x, pB3[i], acc3);
                        }

                        // Remainder (if d_inner not divisible by 4)
                        for (int i = simd_count * 4; i < a->d_inner; i++) {
                            int32_t x = (int32_t)x_ptr[i];
                            acc0 += x * (int32_t)w_ptr0[i];
                            acc1 += x * (int32_t)w_ptr1[i];
                            acc2 += x * (int32_t)w_ptr2[i];
                            acc3 += x * (int32_t)w_ptr3[i];
                        }

                        int out_base = t * proj_size + j;
                        a->proj_all[out_base + 0] = acc0;
                        a->proj_all[out_base + 1] = acc1;
                        a->proj_all[out_base + 2] = acc2;
                        a->proj_all[out_base + 3] = acc3;
                    }

                    // Tail outputs (tile range not multiple of 4)
                    for (; j < tile1; j++) {
                        const int j_rel = j - tile0;
                        const int8_t *w_ptr = &a->x_proj_weight_tile_l1[j_rel * a->d_inner];
                        const v4s *pB = (const v4s *)w_ptr;

                        int32_t acc = 0;
                        for (int i = 0; i < simd_count; i++) {
                            acc = SumDotpSS(pA[i], pB[i], acc);
                        }
                        for (int i = simd_count * 4; i < a->d_inner; i++) {
                            acc += (int32_t)x_ptr[i] * (int32_t)w_ptr[i];
                        }
                        a->proj_all[t * proj_size + j] = acc;
                    }
                }

                // Ensure all cores are done with this tile before the next DMA overwrites the buffer.
                if (tile1 < proj_size) {
                    pi_cl_team_barrier();
                }
            }
        } else
        for (int t = t_start; t < t_end; t++) {
            const int8_t *x_ptr_ext = &a->x_int8[(b * a->seq_len + t) * a->d_inner];
            const int8_t *x_ptr = x_ptr_ext;
            if (a->x_int8_tile_l1 && a->x_int8_tile_stride > 0) {
                int8_t *x_l1 = a->x_int8_tile_l1 + core_id * a->x_int8_tile_stride;
                pi_cl_dma_copy_t dma_x;
                dma_x.dir = PI_CL_DMA_DIR_EXT2LOC;
                dma_x.size = (uint32_t)a->d_inner;
                dma_x.ext = (uint32_t)x_ptr_ext;
                dma_x.loc = (uint32_t)x_l1;
                dma_x.merge = 0;
                pi_cl_dma_memcpy(&dma_x);
                pi_cl_dma_wait(&dma_x);
                x_ptr = x_l1;
            }
            const v4s *pA = (const v4s *)x_ptr;

            int j = 0;
            for (; j + 3 < proj_size; j += 4) {
                const int8_t *w_ptr0 = &a->x_proj_weight[(j + 0) * a->d_inner];
                const int8_t *w_ptr1 = &a->x_proj_weight[(j + 1) * a->d_inner];
                const int8_t *w_ptr2 = &a->x_proj_weight[(j + 2) * a->d_inner];
                const int8_t *w_ptr3 = &a->x_proj_weight[(j + 3) * a->d_inner];

                const v4s *pB0 = (const v4s *)w_ptr0;
                const v4s *pB1 = (const v4s *)w_ptr1;
                const v4s *pB2 = (const v4s *)w_ptr2;
                const v4s *pB3 = (const v4s *)w_ptr3;

                int32_t acc0 = 0, acc1 = 0, acc2 = 0, acc3 = 0;
                for (int i = 0; i < simd_count; i++) {
                    v4s x = pA[i];
                    acc0 = SumDotpSS(x, pB0[i], acc0);
                    acc1 = SumDotpSS(x, pB1[i], acc1);
                    acc2 = SumDotpSS(x, pB2[i], acc2);
                    acc3 = SumDotpSS(x, pB3[i], acc3);
                }

                // Remainder (if d_inner not divisible by 4)
                for (int i = simd_count * 4; i < a->d_inner; i++) {
                    int32_t x = (int32_t)x_ptr[i];
                    acc0 += x * (int32_t)w_ptr0[i];
                    acc1 += x * (int32_t)w_ptr1[i];
                    acc2 += x * (int32_t)w_ptr2[i];
                    acc3 += x * (int32_t)w_ptr3[i];
                }

	                int out_base = t * proj_size + j;
	                a->proj_all[out_base + 0] = acc0;
	                a->proj_all[out_base + 1] = acc1;
	                a->proj_all[out_base + 2] = acc2;
	                a->proj_all[out_base + 3] = acc3;
	            }

            // Tail outputs (proj_size not multiple of 4)
            for (; j < proj_size; j++) {
                const int8_t *w_ptr = &a->x_proj_weight[j * a->d_inner];
                const v4s *pB = (const v4s *)w_ptr;

                int32_t acc = 0;
                for (int i = 0; i < simd_count; i++) {
                    acc = SumDotpSS(pA[i], pB[i], acc);
                }
	                for (int i = simd_count * 4; i < a->d_inner; i++) {
	                    acc += (int32_t)x_ptr[i] * (int32_t)w_ptr[i];
	                }
	                a->proj_all[t * proj_size + j] = acc;
	            }
        }
        pi_cl_team_barrier();  // BARRIER 1: All x_proj outputs ready

#ifdef ENABLE_MAMBA_SSM_EVENT_PROFILING
        if (core_id == 0 && b == 0) {
            uint32_t cycles_now = pi_perf_read(PI_PERF_CYCLES);
            uint32_t ld_stall_now = pi_perf_read(PI_PERF_LD_STALL);
            uint32_t tcdm_cont_now = pi_perf_read(PI_PERF_TCDM_CONT);
            uint32_t ld_ext_cyc_now = pi_perf_read(PI_PERF_LD_EXT_CYC);
            printf("PERF_SSM PH1: cycles=%u ld_stall=%u tcdm_cont=%u ld_ext_cyc=%u\n",
                   cycles_now - phase_cycles_start,
                   ld_stall_now - phase_ld_stall_start,
                   tcdm_cont_now - phase_tcdm_cont_start,
                   ld_ext_cyc_now - phase_ld_ext_cyc_start);
            phase_cycles_start = cycles_now;
            phase_ld_stall_start = ld_stall_now;
            phase_tcdm_cont_start = tcdm_cont_now;
            phase_ld_ext_cyc_start = ld_ext_cyc_now;
        }
#endif

        // ---
        // STAGE 1.5: Extract B, C for all timesteps (Core 0 only - small work)
        // ---
        if (core_id == 0) {
            for (int t = 0; t < a->seq_len; t++) {
                int32_t *proj_t = &a->proj_all[t * proj_size];
                for (int d = 0; d < a->d_state; d++) {
                    // B extraction
                    int64_t b_scaled = ((int64_t)proj_t[a->dt_rank + d] * a->bc_scale_factor) >> BC_SHIFT;
                    b_scaled = (b_scaled > 32767) ? 32767 : ((b_scaled < -32768) ? -32768 : b_scaled);
                    a->B_all[t * a->d_state + d] = (int16_t)b_scaled;

                    // C extraction
                    int64_t c_scaled = ((int64_t)proj_t[a->dt_rank + a->d_state + d] * a->bc_scale_factor) >> BC_SHIFT;
                    c_scaled = (c_scaled > 32767) ? 32767 : ((c_scaled < -32768) ? -32768 : c_scaled);
                    a->C_all[t * a->d_state + d] = (int16_t)c_scaled;
                }
            }
        }

        // ---
        // Optional: Stage delta[t, r] = (proj_all[t, r] >> 8) into L1.
        // This avoids repeatedly re-reading proj_all from L2/EXT for every dt_proj
        // channel tile in PH2 (big-model dt_proj tiling path).
        // ---
        int32_t *delta_all_l1 = NULL;
#if !defined(DISABLE_MAMBA_SSM_DELTA_L1_STAGING)
        if (a->enable_delta_l1_staging && a->x_proj_weight_tile_l1 && a->x_proj_tile_out > 0) {
            const size_t xproj_bytes = (size_t)a->x_proj_tile_out * (size_t)a->d_inner;
            const size_t delta_bytes = (size_t)a->seq_len * (size_t)a->dt_rank * sizeof(int32_t);

            // Safety: don't overwrite B/C if they were already staged into the same buffer.
            const uintptr_t xproj_base = (uintptr_t)a->x_proj_weight_tile_l1;
            const uintptr_t xproj_end = xproj_base + (uintptr_t)xproj_bytes;
            const uintptr_t b_ptr = (uintptr_t)a->B_all;
            const uintptr_t c_ptr = (uintptr_t)a->C_all;
            const int bc_overlaps_xproj =
                ((b_ptr >= xproj_base) && (b_ptr < xproj_end)) ||
                ((c_ptr >= xproj_base) && (c_ptr < xproj_end));

            if (!bc_overlaps_xproj) {
                const uintptr_t align_mask = (uintptr_t)(MAMBA_SSM_DELTA_L1_ALIGN - 1);
                uintptr_t delta_addr = xproj_base;
                delta_addr = (delta_addr + align_mask) & ~align_mask;
                const size_t head_pad = (size_t)(delta_addr - xproj_base);
                if (head_pad + delta_bytes <= xproj_bytes) {
                    delta_all_l1 = (int32_t *)delta_addr;
                }
            }
        }

        if (delta_all_l1) {
            // Parallel fill over timesteps.
            const int t_chunk = (a->seq_len + NUM_CORES - 1) / NUM_CORES;
            const int t0 = core_id * t_chunk;
            int t1 = t0 + t_chunk;
            if (t1 > a->seq_len) t1 = a->seq_len;
            if (t0 < a->seq_len) {
                for (int t = t0; t < t1; t++) {
                    int32_t *dst = delta_all_l1 + t * a->dt_rank;
                    const int32_t *proj_t = &a->proj_all[t * proj_size];
                    for (int r = 0; r < a->dt_rank; r++) {
                        dst[r] = proj_t[r] >> 8;
                    }
                }
            }
            pi_cl_team_barrier();  // delta_all_l1 ready
        }
#endif
        // ---
        // STAGE 2: Batched dt_proj for ALL timesteps (PARALLEL over channels)
        // Reuse delta per timestep (avoid reloading proj_all for every channel).
        // Output: dt_all[d_inner, seq_len] in CHANNEL-MAJOR order for fast scan
        // ---
        if (a->dt_proj_weight_tile_l1 && a->dt_proj_tile_ch > 0 && a->dt_rank <= MAMBA_SSM_MAX_DT_RANK) {
            // Tile dt_proj weights into L1 by channel blocks (dt_rank is small enough to keep delta_local in registers).
            const int tile_ch = a->dt_proj_tile_ch;
            for (int tile0 = 0; tile0 < a->d_inner; tile0 += tile_ch) {
                int tile_len = a->d_inner - tile0;
                if (tile_len > tile_ch) tile_len = tile_ch;

                if (core_id == 0) {
                    // Stage dt_proj weights for this channel tile into L1 once, then reuse across all timesteps.
                    pi_cl_dma_copy_t dma_w;
                    dma_w.dir = PI_CL_DMA_DIR_EXT2LOC;
                    dma_w.size = (uint32_t)(tile_len * a->dt_rank);
                    dma_w.ext = (uint32_t)&a->dt_proj_weight[tile0 * a->dt_rank];
                    dma_w.loc = (uint32_t)a->dt_proj_weight_tile_l1;
                    dma_w.merge = 0;
                    pi_cl_dma_memcpy(&dma_w);
                    pi_cl_dma_wait(&dma_w);

                    if (a->dt_proj_bias_tile_l1) {
                        pi_cl_dma_copy_t dma_b;
                        dma_b.dir = PI_CL_DMA_DIR_EXT2LOC;
                        dma_b.size = (uint32_t)(tile_len * (int)sizeof(int32_t));
                        dma_b.ext = (uint32_t)&a->dt_proj_bias_q16_16[tile0];
                        dma_b.loc = (uint32_t)a->dt_proj_bias_tile_l1;
                        dma_b.merge = 0;
                        pi_cl_dma_memcpy(&dma_b);
                        pi_cl_dma_wait(&dma_b);
                    }
                }
                pi_cl_team_barrier();

                const int tile_channels_per_core = (tile_len + NUM_CORES - 1) / NUM_CORES;
                int m_tile_start = tile0 + core_id * tile_channels_per_core;
                int m_tile_end = m_tile_start + tile_channels_per_core;
                if (m_tile_end > tile0 + tile_len) m_tile_end = tile0 + tile_len;
                if (m_tile_start > tile0 + tile_len) m_tile_start = tile0 + tile_len;

                for (int t = 0; t < a->seq_len; t++) {
                    const int32_t *delta_local = NULL;
                    int32_t delta_local_buf[MAMBA_SSM_MAX_DT_RANK];
                    const int32_t *proj_t = &a->proj_all[t * proj_size];

                    if (delta_all_l1) {
                        delta_local = delta_all_l1 + t * a->dt_rank;
                    } else {
                        if (a->dt_rank == 16) {
                            delta_local_buf[0]  = proj_t[0]  >> 8;
                            delta_local_buf[1]  = proj_t[1]  >> 8;
                            delta_local_buf[2]  = proj_t[2]  >> 8;
                            delta_local_buf[3]  = proj_t[3]  >> 8;
                            delta_local_buf[4]  = proj_t[4]  >> 8;
                            delta_local_buf[5]  = proj_t[5]  >> 8;
                            delta_local_buf[6]  = proj_t[6]  >> 8;
                            delta_local_buf[7]  = proj_t[7]  >> 8;
                            delta_local_buf[8]  = proj_t[8]  >> 8;
                            delta_local_buf[9]  = proj_t[9]  >> 8;
                            delta_local_buf[10] = proj_t[10] >> 8;
                            delta_local_buf[11] = proj_t[11] >> 8;
                            delta_local_buf[12] = proj_t[12] >> 8;
                            delta_local_buf[13] = proj_t[13] >> 8;
                            delta_local_buf[14] = proj_t[14] >> 8;
                            delta_local_buf[15] = proj_t[15] >> 8;
                        } else if (a->dt_rank == 4) {
                            delta_local_buf[0] = proj_t[0] >> 8;
                            delta_local_buf[1] = proj_t[1] >> 8;
                            delta_local_buf[2] = proj_t[2] >> 8;
                            delta_local_buf[3] = proj_t[3] >> 8;
                        } else {
                            for (int r = 0; r < a->dt_rank; r++) {
                                delta_local_buf[r] = proj_t[r] >> 8;
                            }
                        }
                        delta_local = delta_local_buf;
                    }

                    for (int m = m_tile_start; m < m_tile_end; m++) {
                        const int8_t *w_ptr = &a->dt_proj_weight_tile_l1[(m - tile0) * a->dt_rank];
                        int32_t dt_acc = 0;
                        if (a->dt_rank == 16) {
                            dt_acc =
                                delta_local[0]  * (int32_t)w_ptr[0] +
                                delta_local[1]  * (int32_t)w_ptr[1] +
                                delta_local[2]  * (int32_t)w_ptr[2] +
                                delta_local[3]  * (int32_t)w_ptr[3] +
                                delta_local[4]  * (int32_t)w_ptr[4] +
                                delta_local[5]  * (int32_t)w_ptr[5] +
                                delta_local[6]  * (int32_t)w_ptr[6] +
                                delta_local[7]  * (int32_t)w_ptr[7] +
                                delta_local[8]  * (int32_t)w_ptr[8] +
                                delta_local[9]  * (int32_t)w_ptr[9] +
                                delta_local[10] * (int32_t)w_ptr[10] +
                                delta_local[11] * (int32_t)w_ptr[11] +
                                delta_local[12] * (int32_t)w_ptr[12] +
                                delta_local[13] * (int32_t)w_ptr[13] +
                                delta_local[14] * (int32_t)w_ptr[14] +
                                delta_local[15] * (int32_t)w_ptr[15];
                        } else if (a->dt_rank == 4) {
                            dt_acc =
                                delta_local[0] * (int32_t)w_ptr[0] +
                                delta_local[1] * (int32_t)w_ptr[1] +
                                delta_local[2] * (int32_t)w_ptr[2] +
                                delta_local[3] * (int32_t)w_ptr[3];
                        } else {
                            for (int r = 0; r < a->dt_rank; r++) {
                                dt_acc += delta_local[r] * (int32_t)w_ptr[r];
                            }
                        }

                        // Apply scale and bias, then softplus LUT
                        int64_t dt_scaled = (int64_t)dt_acc * (int64_t)a->dt_scale_q;
                        int32_t dt_val_q16_16 = (int32_t)(dt_scaled >> a->dt_scale_shift);
                        if (a->dt_proj_bias_tile_l1) {
                            dt_val_q16_16 += a->dt_proj_bias_tile_l1[m - tile0];
                        } else {
                            dt_val_q16_16 += a->dt_proj_bias_q16_16[m];
                        }
                        int32_t lut_idx = (dt_val_q16_16 * 10) >> 16;
                        lut_idx = (lut_idx > 127) ? 127 : ((lut_idx < -128) ? -128 : lut_idx);

                        a->dt_all[m * a->seq_len + t] = a->softplus_lut[lut_idx + 128];
                    }
                }
                // Ensure all cores are done with this tile before the next DMA overwrites the buffer.
                pi_cl_team_barrier();
            }
        } else {
            for (int t = 0; t < a->seq_len; t++) {
                const int32_t *delta_local = NULL;
                int32_t delta_local_buf[MAMBA_SSM_MAX_DT_RANK];
                const int32_t *proj_t = &a->proj_all[t * proj_size];
                if (a->dt_rank > MAMBA_SSM_MAX_DT_RANK) {
                    // Correctness fallback for very large `dt_rank`: compute delta_r on-the-fly.
                    for (int m = ch_start; m < ch_end; m++) {
                        const int8_t *w_ptr = &a->dt_proj_weight[m * a->dt_rank];
                        int32_t dt_acc = 0;
                        for (int r = 0; r < a->dt_rank; r++) {
                            int32_t delta_r = proj_t[r] >> 8;
                            dt_acc += delta_r * (int32_t)w_ptr[r];
                        }

                        int64_t dt_scaled = (int64_t)dt_acc * (int64_t)a->dt_scale_q;
                        int32_t dt_val_q16_16 = (int32_t)(dt_scaled >> a->dt_scale_shift);
                        dt_val_q16_16 += a->dt_proj_bias_q16_16[m];
                        int32_t lut_idx = (dt_val_q16_16 * 10) >> 16;
                        lut_idx = (lut_idx > 127) ? 127 : ((lut_idx < -128) ? -128 : lut_idx);

                        a->dt_all[m * a->seq_len + t] = a->softplus_lut[lut_idx + 128];
                    }
                    continue;
                }
                if (delta_all_l1) {
                    delta_local = delta_all_l1 + t * a->dt_rank;
                } else {
                    if (a->dt_rank == 16) {
                        delta_local_buf[0]  = proj_t[0]  >> 8;
                        delta_local_buf[1]  = proj_t[1]  >> 8;
                        delta_local_buf[2]  = proj_t[2]  >> 8;
                        delta_local_buf[3]  = proj_t[3]  >> 8;
                        delta_local_buf[4]  = proj_t[4]  >> 8;
                        delta_local_buf[5]  = proj_t[5]  >> 8;
                        delta_local_buf[6]  = proj_t[6]  >> 8;
                        delta_local_buf[7]  = proj_t[7]  >> 8;
                        delta_local_buf[8]  = proj_t[8]  >> 8;
                        delta_local_buf[9]  = proj_t[9]  >> 8;
                        delta_local_buf[10] = proj_t[10] >> 8;
                        delta_local_buf[11] = proj_t[11] >> 8;
                        delta_local_buf[12] = proj_t[12] >> 8;
                        delta_local_buf[13] = proj_t[13] >> 8;
                        delta_local_buf[14] = proj_t[14] >> 8;
                        delta_local_buf[15] = proj_t[15] >> 8;
                    } else if (a->dt_rank == 4) {
                        delta_local_buf[0] = proj_t[0] >> 8;
                        delta_local_buf[1] = proj_t[1] >> 8;
                        delta_local_buf[2] = proj_t[2] >> 8;
                        delta_local_buf[3] = proj_t[3] >> 8;
                    } else {
                        for (int r = 0; r < a->dt_rank; r++) {
                            delta_local_buf[r] = proj_t[r] >> 8;
                        }
                    }
                    delta_local = delta_local_buf;
                }

                for (int m = ch_start; m < ch_end; m++) {
                    const int8_t *w_ptr = &a->dt_proj_weight[m * a->dt_rank];
                    int32_t dt_acc = 0;
                    if (a->dt_rank == 16) {
                        dt_acc =
                            delta_local[0]  * (int32_t)w_ptr[0] +
                            delta_local[1]  * (int32_t)w_ptr[1] +
                            delta_local[2]  * (int32_t)w_ptr[2] +
                            delta_local[3]  * (int32_t)w_ptr[3] +
                            delta_local[4]  * (int32_t)w_ptr[4] +
                            delta_local[5]  * (int32_t)w_ptr[5] +
                            delta_local[6]  * (int32_t)w_ptr[6] +
                            delta_local[7]  * (int32_t)w_ptr[7] +
                            delta_local[8]  * (int32_t)w_ptr[8] +
                            delta_local[9]  * (int32_t)w_ptr[9] +
                            delta_local[10] * (int32_t)w_ptr[10] +
                            delta_local[11] * (int32_t)w_ptr[11] +
                            delta_local[12] * (int32_t)w_ptr[12] +
                            delta_local[13] * (int32_t)w_ptr[13] +
                            delta_local[14] * (int32_t)w_ptr[14] +
                            delta_local[15] * (int32_t)w_ptr[15];
                    } else if (a->dt_rank == 4) {
                        dt_acc =
                            delta_local[0] * (int32_t)w_ptr[0] +
                            delta_local[1] * (int32_t)w_ptr[1] +
                            delta_local[2] * (int32_t)w_ptr[2] +
                            delta_local[3] * (int32_t)w_ptr[3];
                    } else {
                        for (int r = 0; r < a->dt_rank; r++) {
                            dt_acc += delta_local[r] * (int32_t)w_ptr[r];
                        }
                    }

                    // Apply scale and bias, then softplus LUT
                    int64_t dt_scaled = (int64_t)dt_acc * (int64_t)a->dt_scale_q;
                    int32_t dt_val_q16_16 = (int32_t)(dt_scaled >> a->dt_scale_shift);
                    dt_val_q16_16 += a->dt_proj_bias_q16_16[m];
                    int32_t lut_idx = (dt_val_q16_16 * 10) >> 16;
                    lut_idx = (lut_idx > 127) ? 127 : ((lut_idx < -128) ? -128 : lut_idx);

                    a->dt_all[m * a->seq_len + t] = a->softplus_lut[lut_idx + 128];
                }
            }
        }
        pi_cl_team_barrier();  // BARRIER 2: All dt values ready

#ifdef ENABLE_MAMBA_SSM_EVENT_PROFILING
        if (core_id == 0 && b == 0) {
            uint32_t cycles_now = pi_perf_read(PI_PERF_CYCLES);
            uint32_t ld_stall_now = pi_perf_read(PI_PERF_LD_STALL);
            uint32_t tcdm_cont_now = pi_perf_read(PI_PERF_TCDM_CONT);
            uint32_t ld_ext_cyc_now = pi_perf_read(PI_PERF_LD_EXT_CYC);
            printf("PERF_SSM PH2: cycles=%u ld_stall=%u tcdm_cont=%u ld_ext_cyc=%u\n",
                   cycles_now - phase_cycles_start,
                   ld_stall_now - phase_ld_stall_start,
                   tcdm_cont_now - phase_tcdm_cont_start,
                   ld_ext_cyc_now - phase_ld_ext_cyc_start);
            phase_cycles_start = cycles_now;
            phase_ld_stall_start = ld_stall_now;
            phase_tcdm_cont_start = tcdm_cont_now;
            phase_ld_ext_cyc_start = ld_ext_cyc_now;
        }
#endif

        const int16_t *B_all_base = a->B_all;
        const int16_t *C_all_base = a->C_all;
#if !defined(DISABLE_MAMBA_SSM_BC_L1_STAGING)
        // If PH2 used the x_proj tile buffer for delta staging, repurpose it now to keep B/C in L1 for PH3.
        if (delta_all_l1 && a->x_proj_weight_tile_l1 && a->x_proj_tile_out > 0) {
            const size_t xproj_bytes = (size_t)a->x_proj_tile_out * (size_t)a->d_inner;
            const size_t bc_bytes = (size_t)a->seq_len * (size_t)a->d_state * sizeof(int16_t);
            if (xproj_bytes >= 2 * bc_bytes) {
                int16_t *B_l1 = (int16_t *)a->x_proj_weight_tile_l1;
                int16_t *C_l1 = (int16_t *)((int8_t *)B_l1 + bc_bytes);

                if (core_id == 0) {
                    pi_cl_dma_copy_t dma_b;
                    dma_b.dir = PI_CL_DMA_DIR_EXT2LOC;
                    dma_b.size = (uint32_t)bc_bytes;
                    dma_b.ext = (uint32_t)a->B_all;
                    dma_b.loc = (uint32_t)B_l1;
                    dma_b.merge = 0;
                    pi_cl_dma_memcpy(&dma_b);
                    pi_cl_dma_wait(&dma_b);

                    pi_cl_dma_copy_t dma_c;
                    dma_c.dir = PI_CL_DMA_DIR_EXT2LOC;
                    dma_c.size = (uint32_t)bc_bytes;
                    dma_c.ext = (uint32_t)a->C_all;
                    dma_c.loc = (uint32_t)C_l1;
                    dma_c.merge = 0;
                    pi_cl_dma_memcpy(&dma_c);
                    pi_cl_dma_wait(&dma_c);
                }
                pi_cl_team_barrier();
                B_all_base = B_l1;
                C_all_base = C_l1;
            }
        }
#endif

        // ---
        // STAGE 3: Register-Resident Channel Scan (PARALLEL over channels)
        // Each core handles a channel block, sequential time loop per channel.
        // ---
#if defined(ENABLE_MAMBA_SSM_PH3_X_L1_TIME_TILING)
        // Optional fallback: stream x[t] into L1 time tiles when full `ssm_in` doesn't fit in L1.
        // Reuses the unused tail of the x_proj tile buffer (and avoids overwriting optional B/C staging).
        if (a->enable_ph3_x_l1_time_tiling && a->x_proj_weight_tile_l1 && a->x_proj_tile_out > 0) {
            const size_t xproj_bytes = (size_t)a->x_proj_tile_out * (size_t)a->d_inner;
            const size_t bc_bytes = (size_t)a->seq_len * (size_t)a->d_state * sizeof(int16_t);
            const uintptr_t l1_base = (uintptr_t)a->x_proj_weight_tile_l1;
            uintptr_t x_tile_addr = l1_base;
            if ((uintptr_t)B_all_base == l1_base) {
                x_tile_addr += 2 * (uintptr_t)bc_bytes;
            }
            const uintptr_t align_mask = (uintptr_t)(MAMBA_SSM_L1_IO_ALIGN - 1);
            x_tile_addr = (x_tile_addr + align_mask) & ~align_mask;
            const uintptr_t l1_end = l1_base + (uintptr_t)xproj_bytes;

            if (x_tile_addr < l1_end) {
                const size_t avail_bytes = (size_t)(l1_end - x_tile_addr);
                int tile_len = (int)(avail_bytes / (size_t)a->d_inner);
                tile_len &= ~3;
                if (tile_len >= 8) {
                    if (tile_len > a->seq_len) tile_len = a->seq_len;
                    int8_t *x_tile_l1 = (int8_t *)x_tile_addr;

                    for (int t0 = 0; t0 < a->seq_len; t0 += tile_len) {
                        const int t1 = (t0 + tile_len < a->seq_len) ? (t0 + tile_len) : a->seq_len;
                        const int t_n = t1 - t0;

                        if (core_id == 0) {
                            pi_cl_dma_copy_t dma_x;
                            dma_x.dir = PI_CL_DMA_DIR_EXT2LOC;
                            dma_x.size = (uint32_t)((size_t)t_n * (size_t)a->d_inner);
                            dma_x.ext = (uint32_t)(&a->x_int8[(b * a->seq_len + t0) * a->d_inner]);
                            dma_x.loc = (uint32_t)x_tile_l1;
                            dma_x.merge = 0;
                            pi_cl_dma_memcpy(&dma_x);
                            pi_cl_dma_wait(&dma_x);
                        }
                        pi_cl_team_barrier();  // x tile ready

                        for (int m = ch_start; m < ch_end; m++) {
                            int16_t h_local[16];
                            int h_base = b * a->d_inner * a->d_state + m * a->d_state;
                            for (int d = 0; d < a->d_state && d < 16; d++) {
                                h_local[d] = a->h_state_q15[h_base + d];
                            }

                            int16_t D_val = a->D_q15[m];
                            int16_t A_local[16];
                            for (int d = 0; d < a->d_state && d < 16; d++) {
                                A_local[d] = a->A_q15[d * a->d_inner + m];
                            }
                            const int16_t *dt_m = &a->dt_all[m * a->seq_len];
                            const int8_t *z_base;
                            int z_stride;
                            if (a->z_layout == MAMBA_SSM_Z_LAYOUT_CHANNEL_MAJOR) {
                                z_base = a->z_int8 + (b * a->d_inner + m) * a->seq_len;
                                z_stride = 1;
                            } else if (a->z_layout == MAMBA_SSM_Z_LAYOUT_TIME_MAJOR) {
                                z_base = a->z_int8 + (b * a->seq_len) * a->d_inner + m;
                                z_stride = a->d_inner;
                            } else {
                                z_base = a->z_int8 + (b * a->seq_len) * 2 * a->d_inner + a->d_inner + m;
                                z_stride = 2 * a->d_inner;
                            }

                            const int8_t *x_base = x_tile_l1 + m;
                            int8_t *out_base = a->output_int8 + (b * a->seq_len) * a->d_inner + m;

                            #define CHANNEL_STATE_UPDATE(d) do {                     int16_t A_val = A_local[d];                     int32_t dt_A_q23 = (int32_t)dt_val * (int32_t)A_val;                     int32_t dt_A_q15 = dt_A_q23 >> 8;                     int32_t exp_idx = (dt_A_q15 * 10) >> 15;                     exp_idx = (exp_idx > 127) ? 127 : ((exp_idx < -128) ? -128 : exp_idx);                     int16_t dA_q15 = a->exp_lut[exp_idx + 128];                     int32_t dB_q23 = (int32_t)dt_val * (int32_t)B_t[d];                     int16_t dB_q15 = (int16_t)(dB_q23 >> 8);                     int32_t h_decay = ((int32_t)dA_q15 * (int32_t)h_local[d]) >> 15;                     int32_t h_input = ((int32_t)dB_q15 * (int32_t)x_i8);                     int32_t h_new = h_decay + (h_input >> 7);                     h_new = (h_new > 32767) ? 32767 : ((h_new < -32768) ? -32768 : h_new);                     h_local[d] = (int16_t)h_new;                     y_acc += ((int32_t)C_t[d] * h_new) >> 15;                 } while(0)

                            if (a->d_state == 4) {
                                for (int t = t0; t < t1; t++) {
                                    const int16_t *B_t = &B_all_base[t * 4];
                                    const int16_t *C_t = &C_all_base[t * 4];
                                    int8_t x_i8 = x_base[(t - t0) * a->d_inner];
                                    int8_t z_val = z_base[t * z_stride];
                                    int16_t dt_val = dt_m[t];

                                    int32_t y_acc = 0;
                                    CHANNEL_STATE_UPDATE(0);
                                    CHANNEL_STATE_UPDATE(1);
                                    CHANNEL_STATE_UPDATE(2);
                                    CHANNEL_STATE_UPDATE(3);

                                    y_acc += ((int32_t)D_val * (int32_t)x_i8);
                                    int64_t y_scaled = (int64_t)y_acc * (int64_t)a->output_scale_q;
                                    int32_t ssm_out = (int32_t)((y_scaled + (1LL << (OUTPUT_SHIFT - 1))) >> OUTPUT_SHIFT);
                                    ssm_out = (ssm_out > 127) ? 127 : ((ssm_out < -128) ? -128 : ssm_out);

                                    int16_t silu_q13 = a->silu_gate_lut_q13[(int)z_val + 128];
                                    int32_t gated = ((int32_t)ssm_out * (int32_t)silu_q13 + (1 << 12)) >> 13;
                                    gated = (gated > 127) ? 127 : ((gated < -128) ? -128 : gated);

                                    out_base[t * a->d_inner] = (int8_t)gated;
                                }
                            } else if (a->d_state == 8) {
                                for (int t = t0; t < t1; t++) {
                                    const int16_t *B_t = &B_all_base[t * 8];
                                    const int16_t *C_t = &C_all_base[t * 8];
                                    int8_t x_i8 = x_base[(t - t0) * a->d_inner];
                                    int8_t z_val = z_base[t * z_stride];
                                    int16_t dt_val = dt_m[t];

                                    int32_t y_acc = 0;
                                    CHANNEL_STATE_UPDATE(0); CHANNEL_STATE_UPDATE(1);
                                    CHANNEL_STATE_UPDATE(2); CHANNEL_STATE_UPDATE(3);
                                    CHANNEL_STATE_UPDATE(4); CHANNEL_STATE_UPDATE(5);
                                    CHANNEL_STATE_UPDATE(6); CHANNEL_STATE_UPDATE(7);

                                    y_acc += ((int32_t)D_val * (int32_t)x_i8);
                                    int64_t y_scaled = (int64_t)y_acc * (int64_t)a->output_scale_q;
                                    int32_t ssm_out = (int32_t)((y_scaled + (1LL << (OUTPUT_SHIFT - 1))) >> OUTPUT_SHIFT);
                                    ssm_out = (ssm_out > 127) ? 127 : ((ssm_out < -128) ? -128 : ssm_out);

                                    int16_t silu_q13 = a->silu_gate_lut_q13[(int)z_val + 128];
                                    int32_t gated = ((int32_t)ssm_out * (int32_t)silu_q13 + (1 << 12)) >> 13;
                                    gated = (gated > 127) ? 127 : ((gated < -128) ? -128 : gated);

                                    out_base[t * a->d_inner] = (int8_t)gated;
                                }
                            } else if (a->d_state == 16) {
                                for (int t = t0; t < t1; t++) {
                                    const int16_t *B_t = &B_all_base[t * 16];
                                    const int16_t *C_t = &C_all_base[t * 16];
                                    int8_t x_i8 = x_base[(t - t0) * a->d_inner];
                                    int8_t z_val = z_base[t * z_stride];
                                    int16_t dt_val = dt_m[t];

                                    int32_t y_acc = 0;
                                    CHANNEL_STATE_UPDATE(0);  CHANNEL_STATE_UPDATE(1);
                                    CHANNEL_STATE_UPDATE(2);  CHANNEL_STATE_UPDATE(3);
                                    CHANNEL_STATE_UPDATE(4);  CHANNEL_STATE_UPDATE(5);
                                    CHANNEL_STATE_UPDATE(6);  CHANNEL_STATE_UPDATE(7);
                                    CHANNEL_STATE_UPDATE(8);  CHANNEL_STATE_UPDATE(9);
                                    CHANNEL_STATE_UPDATE(10); CHANNEL_STATE_UPDATE(11);
                                    CHANNEL_STATE_UPDATE(12); CHANNEL_STATE_UPDATE(13);
                                    CHANNEL_STATE_UPDATE(14); CHANNEL_STATE_UPDATE(15);

                                    y_acc += ((int32_t)D_val * (int32_t)x_i8);
                                    int64_t y_scaled = (int64_t)y_acc * (int64_t)a->output_scale_q;
                                    int32_t ssm_out = (int32_t)((y_scaled + (1LL << (OUTPUT_SHIFT - 1))) >> OUTPUT_SHIFT);
                                    ssm_out = (ssm_out > 127) ? 127 : ((ssm_out < -128) ? -128 : ssm_out);

                                    int16_t silu_q13 = a->silu_gate_lut_q13[(int)z_val + 128];
                                    int32_t gated = ((int32_t)ssm_out * (int32_t)silu_q13 + (1 << 12)) >> 13;
                                    gated = (gated > 127) ? 127 : ((gated < -128) ? -128 : gated);

                                    out_base[t * a->d_inner] = (int8_t)gated;
                                }
                            } else {
                                for (int t = t0; t < t1; t++) {
                                    const int16_t *B_t = &B_all_base[t * a->d_state];
                                    const int16_t *C_t = &C_all_base[t * a->d_state];
                                    int8_t x_i8 = x_base[(t - t0) * a->d_inner];
                                    int8_t z_val = z_base[t * z_stride];
                                    int16_t dt_val = dt_m[t];

                                    int32_t y_acc = 0;
                                    for (int d = 0; d < a->d_state; d++) {
                                        int16_t A_val = A_local[d];
                                        int32_t dt_A_q23 = (int32_t)dt_val * (int32_t)A_val;
                                        int32_t dt_A_q15 = dt_A_q23 >> 8;
                                        int32_t exp_idx = (dt_A_q15 * 10) >> 15;
                                        exp_idx = (exp_idx > 127) ? 127 : ((exp_idx < -128) ? -128 : exp_idx);
                                        int16_t dA_q15 = a->exp_lut[exp_idx + 128];
                                        int32_t dB_q23 = (int32_t)dt_val * (int32_t)B_t[d];
                                        int16_t dB_q15 = (int16_t)(dB_q23 >> 8);
                                        int32_t h_decay = ((int32_t)dA_q15 * (int32_t)h_local[d]) >> 15;
                                        int32_t h_input = ((int32_t)dB_q15 * (int32_t)x_i8);
                                        int32_t h_new = h_decay + (h_input >> 7);
                                        h_new = (h_new > 32767) ? 32767 : ((h_new < -32768) ? -32768 : h_new);
                                        h_local[d] = (int16_t)h_new;
                                        y_acc += ((int32_t)C_t[d] * h_new) >> 15;
                                    }

                                    y_acc += ((int32_t)D_val * (int32_t)x_i8);
                                    int64_t y_scaled = (int64_t)y_acc * (int64_t)a->output_scale_q;
                                    int32_t ssm_out = (int32_t)((y_scaled + (1LL << (OUTPUT_SHIFT - 1))) >> OUTPUT_SHIFT);
                                    ssm_out = (ssm_out > 127) ? 127 : ((ssm_out < -128) ? -128 : ssm_out);

                                    int16_t silu_q13 = a->silu_gate_lut_q13[(int)z_val + 128];
                                    int32_t gated = ((int32_t)ssm_out * (int32_t)silu_q13 + (1 << 12)) >> 13;
                                    gated = (gated > 127) ? 127 : ((gated < -128) ? -128 : gated);

                                    out_base[t * a->d_inner] = (int8_t)gated;
                                }
                            }

                            #undef CHANNEL_STATE_UPDATE

                            for (int d = 0; d < a->d_state && d < 16; d++) {
                                a->h_state_q15[h_base + d] = h_local[d];
                            }
                        }

                        // Ensure all cores are done before the next DMA overwrites the tile.
                        if (t1 < a->seq_len) pi_cl_team_barrier();
                    }

                    goto ssm_ph3_done;
                }
            }
        }
#endif

        // ---
        // PH3 fallback (big-model path): stage x/z into L1 by channel tiles
        // When full-seq input staging doesn't fit, reuse the x_proj L1 tile buffer
        // (after optional B/C staging) to DMA small [tile_ch] channel slices for
        // the full sequence into TCDM.
        // ---
#if !defined(DISABLE_MAMBA_SSM_PH3_IO_L1_CH_TILING)
        if ((a->ph3_need_x_l1_ch_tiling || a->ph3_need_z_l1_ch_tiling) && a->x_proj_weight_tile_l1 && a->x_proj_tile_out > 0) {
            const size_t xproj_bytes = (size_t)a->x_proj_tile_out * (size_t)a->d_inner;
            const size_t bc_bytes = (size_t)a->seq_len * (size_t)a->d_state * sizeof(int16_t);
            const uintptr_t l1_base = (uintptr_t)a->x_proj_weight_tile_l1;
            const uintptr_t l1_end = l1_base + (uintptr_t)xproj_bytes;

            // Place tiles after any optional B/C staging in the same buffer.
            uintptr_t io_base = l1_base;
            if ((uintptr_t)B_all_base == l1_base) {
                io_base += 2 * (uintptr_t)bc_bytes;
            }
            const uintptr_t align_mask = (uintptr_t)(MAMBA_SSM_L1_IO_ALIGN - 1);
            io_base = (io_base + align_mask) & ~align_mask;

            if (io_base < l1_end) {
                const size_t avail_bytes = (size_t)(l1_end - io_base);
                const int need_x = a->ph3_need_x_l1_ch_tiling;
                const int need_z = a->ph3_need_z_l1_ch_tiling;
                const size_t bytes_per_ch_x = (size_t)a->seq_len;  // int8: one byte per timestep
                const size_t bytes_per_ch_z = (size_t)a->seq_len;  // int8: one byte per timestep
                const size_t bytes_per_ch_dt = (size_t)a->seq_len * sizeof(int16_t);
                const size_t pad = (size_t)MAMBA_SSM_L1_IO_PAD_BYTES + (size_t)MAMBA_SSM_L1_IO_ALIGN;
                int need_dt = 0;
#if defined(ENABLE_MAMBA_SSM_PH3_DT_L1_CH_TILING) && !defined(DISABLE_MAMBA_SSM_PH3_DT_L1_CH_TILING)
                need_dt = !a->dt_all_in_l1;
#endif

                int stage_x = 0;
                int stage_z = 0;
                int tile_ch = 0;
                int stage_dt = 0;

                // Prefer staging both x and z (same tile_ch), else stage one plane.
                if (need_x && need_z) {
                    // Try staging x+z+dt (best), otherwise fall back to x+dt, then x+z.
                    if (need_dt) {
                        const size_t per_ch = bytes_per_ch_x + bytes_per_ch_z + bytes_per_ch_dt;
                        const size_t overhead = 2 * pad;
                        if (avail_bytes > overhead) {
                            tile_ch = (int)((avail_bytes - overhead) / per_ch);
                            tile_ch &= ~3;
                            if (tile_ch >= 4) {
                                stage_x = 1;
                                stage_z = 1;
                                stage_dt = 1;
                            }
                        }
                        if (!stage_x) {
                            const size_t per_ch_xdt = bytes_per_ch_x + bytes_per_ch_dt;
                            const size_t overhead_xdt = pad;
                            if (avail_bytes > overhead_xdt) {
                                tile_ch = (int)((avail_bytes - overhead_xdt) / per_ch_xdt);
                                tile_ch &= ~3;
                                if (tile_ch >= 4) {
                                    stage_x = 1;
                                    stage_dt = 1;
                                }
                            }
                        }
                    }
                    if (!stage_x && !stage_z) {
                        const size_t per_ch = bytes_per_ch_x + bytes_per_ch_z;
                        const size_t overhead = pad;
                        if (avail_bytes > overhead) {
                            tile_ch = (int)((avail_bytes - overhead) / per_ch);
                            tile_ch &= ~3;
                            if (tile_ch >= 4) {
                                stage_x = 1;
                                stage_z = 1;
                            }
                        }
                    }
                }
                if (!stage_x && !stage_z && !stage_dt) {
                    if (need_x) {
                        if (need_dt) {
                            const size_t per_ch = bytes_per_ch_x + bytes_per_ch_dt;
                            const size_t overhead = pad;
                            if (avail_bytes > overhead) {
                                tile_ch = (int)((avail_bytes - overhead) / per_ch);
                                tile_ch &= ~3;
                                if (tile_ch >= 4) {
                                    stage_x = 1;
                                    stage_dt = 1;
                                }
                            }
                        }
                        if (!stage_x) {
                            tile_ch = (int)(avail_bytes / bytes_per_ch_x);
                            tile_ch &= ~3;
                            if (tile_ch >= 4) stage_x = 1;
                        }
                    }
                    if (!stage_x && need_z) {
                        if (need_dt) {
                            const size_t per_ch = bytes_per_ch_z + bytes_per_ch_dt;
                            const size_t overhead = pad;
                            if (avail_bytes > overhead) {
                                tile_ch = (int)((avail_bytes - overhead) / per_ch);
                                tile_ch &= ~3;
                                if (tile_ch >= 4) {
                                    stage_z = 1;
                                    stage_dt = 1;
                                }
                            }
                        }
                        if (!stage_z) {
                            tile_ch = (int)(avail_bytes / bytes_per_ch_z);
                            tile_ch &= ~3;
                            if (tile_ch >= 4) stage_z = 1;
                        }
                    }
                }

                if (tile_ch > a->d_inner) tile_ch = a->d_inner & ~3;

#if defined(ENABLE_MAMBA_SSM_PH3_IO_L1_TIME_TILING) && !defined(DISABLE_MAMBA_SSM_PH3_IO_L1_TIME_TILING)
                if (ssm_ph3_io_time_tiling(
                        a, core_id, b,
                        need_x, need_z,
                        stage_x, stage_z,
                        tile_ch,
                        io_base, l1_end,
                        avail_bytes, pad, align_mask,
                        B_all_base, C_all_base)) {
                    goto ssm_ph3_done;
                }
#if 0
                // Try a PH3 time-tiling fallback when full-seq channel-tiling can't stage the required planes.
                // This keeps the scan compute on TCDM without per-timestep barriers (only per time-tile).
                if (a->seq_len > MAMBA_SSM_PH3_IO_TIME_TILE) {
                    const int want_both = (need_x && need_z);
                    const int have_both = (stage_x && stage_z);
                    const int have_any = (stage_x || stage_z);

                    if ((want_both && !have_both) || (!want_both && !have_any)) {
                        int time_tile_len = MAMBA_SSM_PH3_IO_TIME_TILE;
                        if (time_tile_len > a->seq_len) time_tile_len = a->seq_len;

                        // Conservative sizing: stage only the required x/z planes for a [time_tile_len] window.
                        // Optionally keep h_state for the channel tile in L1 to avoid re-loading it per time tile.
                        size_t per_ch = 0;
                        if (need_x) per_ch += (size_t)time_tile_len;
                        if (need_z) per_ch += (size_t)time_tile_len;
#if defined(ENABLE_MAMBA_SSM_PH3_HSTATE_L1_CH_TILING) && !defined(DISABLE_MAMBA_SSM_PH3_HSTATE_L1_CH_TILING)
	                        per_ch += (size_t)a->d_state * sizeof(int16_t);
#endif

                        // Pad is only needed when staging both x and z (separate buffers).
                        const size_t overhead = (need_x && need_z) ? pad : 0;

                        int tile_ch_tt = 0;
                        if (per_ch > 0 && avail_bytes > overhead) {
                            tile_ch_tt = (int)((avail_bytes - overhead) / per_ch);
                            tile_ch_tt &= ~3;
                        }
                        if (tile_ch_tt > a->d_inner) tile_ch_tt = a->d_inner & ~3;

                        if (tile_ch_tt >= 4) {
                            // Allocate a temporary L1 layout:
                            //   [optional h_state_tile] [x_tile] [pad/align] [z_tile]
                            uintptr_t cur_tt = io_base;
                            int16_t *h_state_tile_l1 = NULL;
                            int8_t *x_tile_l1 = NULL;
                            int8_t *z_tile_l1 = NULL;

                            const size_t hstate_bytes_per_ch = (size_t)a->d_state * sizeof(int16_t);
                            const size_t xz_bytes_per_ch = (size_t)time_tile_len;

                            int use_hstate_tiling = 0;
#if defined(ENABLE_MAMBA_SSM_PH3_HSTATE_L1_CH_TILING) && !defined(DISABLE_MAMBA_SSM_PH3_HSTATE_L1_CH_TILING)
                            use_hstate_tiling = 1;
#endif

                            if (use_hstate_tiling) {
                                // Align for int16/int32 access.
                                cur_tt = (cur_tt + 3) & ~(uintptr_t)3;
                                h_state_tile_l1 = (int16_t *)cur_tt;
                                cur_tt += (uintptr_t)(hstate_bytes_per_ch * (size_t)tile_ch_tt);
                                // Align next buffer for TCDM bank friendliness.
                                cur_tt = (cur_tt + align_mask) & ~align_mask;
                            }

                            if (need_x) {
                                x_tile_l1 = (int8_t *)cur_tt;
                                cur_tt += (uintptr_t)(xz_bytes_per_ch * (size_t)tile_ch_tt);
                                if (need_z) {
                                    cur_tt += (uintptr_t)MAMBA_SSM_L1_IO_PAD_BYTES;
                                    cur_tt = (cur_tt + align_mask) & ~align_mask;
                                }
                            }

                            if (need_z) {
                                z_tile_l1 = (int8_t *)cur_tt;
                                cur_tt += (uintptr_t)(xz_bytes_per_ch * (size_t)tile_ch_tt);
                            }

                            // Bounds check (defensive when enabling optional h_state tiling).
                            if (cur_tt <= l1_end) {
                                const int z_tile_is_channel_major = need_z && (a->z_layout == MAMBA_SSM_Z_LAYOUT_CHANNEL_MAJOR);

                                for (int tile0 = 0; tile0 < a->d_inner; tile0 += tile_ch_tt) {
                                    int tile_len = a->d_inner - tile0;
                                    if (tile_len > tile_ch_tt) tile_len = tile_ch_tt;

                                    // Stage h_state for this channel tile once (optional).
                                    if (use_hstate_tiling) {
                                        if (core_id == 0) {
                                            pi_cl_dma_copy_t hs_dma;
                                            hs_dma.dir = PI_CL_DMA_DIR_EXT2LOC;
                                            hs_dma.merge = 0;
                                            hs_dma.size = (uint32_t)((size_t)tile_len * hstate_bytes_per_ch);
                                            const int16_t *src = a->h_state_q15 + (size_t)b * (size_t)a->d_inner * (size_t)a->d_state + (size_t)tile0 * (size_t)a->d_state;
                                            hs_dma.ext = (uint32_t)src;
                                            hs_dma.loc = (uint32_t)h_state_tile_l1;
                                            pi_cl_dma_memcpy(&hs_dma);
                                            pi_cl_dma_wait(&hs_dma);
                                        }
                                        pi_cl_team_barrier();
                                    }

                                    // Distribute channels in this tile across cores.
                                    const int tile_channels_per_core = (tile_len + NUM_CORES - 1) / NUM_CORES;
                                    int m_tile_start = tile0 + core_id * tile_channels_per_core;
                                    int m_tile_end = m_tile_start + tile_channels_per_core;
                                    if (m_tile_end > tile0 + tile_len) m_tile_end = tile0 + tile_len;
                                    if (m_tile_start > tile0 + tile_len) m_tile_start = tile0 + tile_len;

                                    for (int t0 = 0; t0 < a->seq_len; t0 += time_tile_len) {
                                        int t_len = a->seq_len - t0;
                                        if (t_len > time_tile_len) t_len = time_tile_len;

                                        if (core_id == 0) {
                                            if (need_x) {
                                                // x is time-major [B, L, d_inner]; stage a [t_len, tile_len] slice.
                                                pi_cl_dma_copy_2d_t x_dma;
                                                x_dma.dir = PI_CL_DMA_DIR_EXT2LOC;
                                                x_dma.merge = 0;
                                                x_dma.length = (uint32_t)tile_len;
                                                x_dma.stride = (uint32_t)a->d_inner;
                                                x_dma.size = (uint32_t)((size_t)t_len * (size_t)tile_len);

                                                const int8_t *src = a->x_int8 + ((size_t)b * (size_t)a->seq_len + (size_t)t0) * (size_t)a->d_inner + (size_t)tile0;
                                                x_dma.ext = (uint32_t)src;
                                                x_dma.loc = (uint32_t)x_tile_l1;
                                                pi_cl_dma_memcpy_2d(&x_dma);
                                                pi_cl_dma_wait(&x_dma);
                                            }

                                            if (need_z) {
                                                if (z_tile_is_channel_major) {
                                                    // z is [B, d_inner, L]; stage a [tile_len, t_len] slice (channel-major).
                                                    pi_cl_dma_copy_2d_t z_dma;
                                                    z_dma.dir = PI_CL_DMA_DIR_EXT2LOC;
                                                    z_dma.merge = 0;
                                                    z_dma.length = (uint32_t)t_len;
                                                    z_dma.stride = (uint32_t)a->seq_len;
                                                    z_dma.size = (uint32_t)((size_t)tile_len * (size_t)t_len);

                                                    const int8_t *src = a->z_int8 + ((size_t)b * (size_t)a->d_inner + (size_t)tile0) * (size_t)a->seq_len + (size_t)t0;
                                                    z_dma.ext = (uint32_t)src;
                                                    z_dma.loc = (uint32_t)z_tile_l1;
                                                    pi_cl_dma_memcpy_2d(&z_dma);
                                                    pi_cl_dma_wait(&z_dma);
                                                } else {
                                                    // z is time-major ([B, L, d_inner] or interleaved in xz_proj); stage a [t_len, tile_len] slice.
                                                    pi_cl_dma_copy_2d_t z_dma;
                                                    z_dma.dir = PI_CL_DMA_DIR_EXT2LOC;
                                                    z_dma.merge = 0;
                                                    z_dma.length = (uint32_t)tile_len;
                                                    z_dma.stride = (uint32_t)((a->z_layout == MAMBA_SSM_Z_LAYOUT_XZ_INTERLEAVED) ? (2 * a->d_inner) : a->d_inner);
                                                    z_dma.size = (uint32_t)((size_t)t_len * (size_t)tile_len);

                                                    const int8_t *src;
                                                    if (a->z_layout == MAMBA_SSM_Z_LAYOUT_XZ_INTERLEAVED) {
                                                        src = a->z_int8 + ((size_t)b * (size_t)a->seq_len + (size_t)t0) * (size_t)(2 * a->d_inner) + (size_t)a->d_inner + (size_t)tile0;
                                                    } else {
                                                        src = a->z_int8 + ((size_t)b * (size_t)a->seq_len + (size_t)t0) * (size_t)a->d_inner + (size_t)tile0;
                                                    }
                                                    z_dma.ext = (uint32_t)src;
                                                    z_dma.loc = (uint32_t)z_tile_l1;
                                                    pi_cl_dma_memcpy_2d(&z_dma);
                                                    pi_cl_dma_wait(&z_dma);
                                                }
                                            }
                                        }
                                        pi_cl_team_barrier();  // time tile ready

                                        for (int m = m_tile_start; m < m_tile_end; m++) {
                                            // Load hidden state into local registers
                                            int16_t h_local[16];
                                            int h_base = b * a->d_inner * a->d_state + m * a->d_state;
                                            const int state_off = (m - tile0) * a->d_state;
                                            const int16_t *h_src = use_hstate_tiling ? (h_state_tile_l1 + state_off) : (a->h_state_q15 + h_base);
                                            for (int d = 0; d < a->d_state && d < 16; d++) {
                                                h_local[d] = h_src[d];
                                            }

                                            // Channel-specific constants
                                            int16_t D_val = a->D_q15[m];
                                            int16_t A_local[16];
                                            for (int d = 0; d < a->d_state && d < 16; d++) {
                                                A_local[d] = a->A_q15[d * a->d_inner + m];
                                            }
                                            const int16_t *dt_m = &a->dt_all[m * a->seq_len];

                                            // x (staged) is time-major [t_len, tile_len] in L1.
                                            const int8_t *x_base = need_x ? (x_tile_l1 + (m - tile0)) : (a->x_int8 + ((size_t)b * (size_t)a->seq_len + (size_t)t0) * (size_t)a->d_inner + (size_t)m);
                                            int x_stride = need_x ? tile_len : a->d_inner;

                                            // z (staged) can be channel-major or time-major in L1 depending on z_layout.
                                            const int8_t *z_base;
                                            int z_stride;
                                            if (need_z) {
                                                if (z_tile_is_channel_major) {
                                                    z_base = z_tile_l1 + (size_t)(m - tile0) * (size_t)t_len;
                                                    z_stride = 1;
                                                } else {
                                                    z_base = z_tile_l1 + (m - tile0);
                                                    z_stride = tile_len;
                                                }
                                            } else {
                                                if (a->z_layout == MAMBA_SSM_Z_LAYOUT_CHANNEL_MAJOR) {
                                                    z_base = a->z_int8 + ((size_t)b * (size_t)a->d_inner + (size_t)m) * (size_t)a->seq_len + (size_t)t0;
                                                    z_stride = 1;
                                                } else if (a->z_layout == MAMBA_SSM_Z_LAYOUT_TIME_MAJOR) {
                                                    z_base = a->z_int8 + ((size_t)b * (size_t)a->seq_len + (size_t)t0) * (size_t)a->d_inner + (size_t)m;
                                                    z_stride = a->d_inner;
                                                } else {
                                                    z_base = a->z_int8 + ((size_t)b * (size_t)a->seq_len + (size_t)t0) * (size_t)(2 * a->d_inner) + (size_t)a->d_inner + (size_t)m;
                                                    z_stride = 2 * a->d_inner;
                                                }
                                            }

                                            // Output pointer for this time tile
                                            int8_t *out_base = a->output_int8 + ((size_t)b * (size_t)a->seq_len + (size_t)t0) * (size_t)a->d_inner + (size_t)m;

                                            // Sequential time loop - NO per-timestep barriers.
                                            #define CHANNEL_STATE_UPDATE_TT(d) do {                     int16_t A_val = A_local[d];                     int32_t dt_A_q23 = (int32_t)dt_val * (int32_t)A_val;                     int32_t dt_A_q15 = dt_A_q23 >> 8;                     int32_t exp_idx = (dt_A_q15 * 10) >> 15;                     exp_idx = (exp_idx > 127) ? 127 : ((exp_idx < -128) ? -128 : exp_idx);                     int16_t dA_q15 = a->exp_lut[exp_idx + 128];                     int32_t dB_q23 = (int32_t)dt_val * (int32_t)B_t[d];                     int16_t dB_q15 = (int16_t)(dB_q23 >> 8);                     int32_t h_decay = ((int32_t)dA_q15 * (int32_t)h_local[d]) >> 15;                     int32_t h_input = ((int32_t)dB_q15 * (int32_t)x_i8);                     int32_t h_new = h_decay + (h_input >> 7);                     h_new = (h_new > 32767) ? 32767 : ((h_new < -32768) ? -32768 : h_new);                     h_local[d] = (int16_t)h_new;                     y_acc += ((int32_t)C_t[d] * h_new) >> 15;                 } while(0)

                                            if (a->d_state == 4) {
                                                const int16_t *B_seg = &B_all_base[t0 * 4];
                                                const int16_t *C_seg = &C_all_base[t0 * 4];
                                                const int16_t *dt_seg = dt_m + t0;
                                                for (int tt = 0; tt < t_len; tt++) {
                                                    const int16_t *B_t = &B_seg[tt * 4];
                                                    const int16_t *C_t = &C_seg[tt * 4];
                                                    int8_t x_i8 = x_base[tt * x_stride];
                                                    int8_t z_val = z_base[tt * z_stride];
                                                    int16_t dt_val = dt_seg[tt];

                                                    int32_t y_acc = 0;
                                                    CHANNEL_STATE_UPDATE_TT(0);
                                                    CHANNEL_STATE_UPDATE_TT(1);
                                                    CHANNEL_STATE_UPDATE_TT(2);
                                                    CHANNEL_STATE_UPDATE_TT(3);

                                                    y_acc += ((int32_t)D_val * (int32_t)x_i8);
                                                    int64_t y_scaled = (int64_t)y_acc * (int64_t)a->output_scale_q;
                                                    int32_t ssm_out = (int32_t)((y_scaled + (1LL << (OUTPUT_SHIFT - 1))) >> OUTPUT_SHIFT);
                                                    ssm_out = (ssm_out > 127) ? 127 : ((ssm_out < -128) ? -128 : ssm_out);

                                                    int16_t silu_q13 = a->silu_gate_lut_q13[(int)z_val + 128];
                                                    int32_t gated = ((int32_t)ssm_out * (int32_t)silu_q13 + (1 << 12)) >> 13;
                                                    gated = (gated > 127) ? 127 : ((gated < -128) ? -128 : gated);

                                                    out_base[tt * a->d_inner] = (int8_t)gated;
                                                }
                                            } else if (a->d_state == 8) {
                                                const int16_t *B_seg = &B_all_base[t0 * 8];
                                                const int16_t *C_seg = &C_all_base[t0 * 8];
                                                const int16_t *dt_seg = dt_m + t0;
                                                for (int tt = 0; tt < t_len; tt++) {
                                                    const int16_t *B_t = &B_seg[tt * 8];
                                                    const int16_t *C_t = &C_seg[tt * 8];
                                                    int8_t x_i8 = x_base[tt * x_stride];
                                                    int8_t z_val = z_base[tt * z_stride];
                                                    int16_t dt_val = dt_seg[tt];

                                                    int32_t y_acc = 0;
                                                    CHANNEL_STATE_UPDATE_TT(0); CHANNEL_STATE_UPDATE_TT(1);
                                                    CHANNEL_STATE_UPDATE_TT(2); CHANNEL_STATE_UPDATE_TT(3);
                                                    CHANNEL_STATE_UPDATE_TT(4); CHANNEL_STATE_UPDATE_TT(5);
                                                    CHANNEL_STATE_UPDATE_TT(6); CHANNEL_STATE_UPDATE_TT(7);

                                                    y_acc += ((int32_t)D_val * (int32_t)x_i8);
                                                    int64_t y_scaled = (int64_t)y_acc * (int64_t)a->output_scale_q;
                                                    int32_t ssm_out = (int32_t)((y_scaled + (1LL << (OUTPUT_SHIFT - 1))) >> OUTPUT_SHIFT);
                                                    ssm_out = (ssm_out > 127) ? 127 : ((ssm_out < -128) ? -128 : ssm_out);

                                                    int16_t silu_q13 = a->silu_gate_lut_q13[(int)z_val + 128];
                                                    int32_t gated = ((int32_t)ssm_out * (int32_t)silu_q13 + (1 << 12)) >> 13;
                                                    gated = (gated > 127) ? 127 : ((gated < -128) ? -128 : gated);

                                                    out_base[tt * a->d_inner] = (int8_t)gated;
                                                }
                                            } else if (a->d_state == 16) {
                                                const int16_t *B_seg = &B_all_base[t0 * 16];
                                                const int16_t *C_seg = &C_all_base[t0 * 16];
                                                const int16_t *dt_seg = dt_m + t0;
                                                for (int tt = 0; tt < t_len; tt++) {
                                                    const int16_t *B_t = &B_seg[tt * 16];
                                                    const int16_t *C_t = &C_seg[tt * 16];
                                                    int8_t x_i8 = x_base[tt * x_stride];
                                                    int8_t z_val = z_base[tt * z_stride];
                                                    int16_t dt_val = dt_seg[tt];

                                                    int32_t y_acc = 0;
                                                    CHANNEL_STATE_UPDATE_TT(0);  CHANNEL_STATE_UPDATE_TT(1);
                                                    CHANNEL_STATE_UPDATE_TT(2);  CHANNEL_STATE_UPDATE_TT(3);
                                                    CHANNEL_STATE_UPDATE_TT(4);  CHANNEL_STATE_UPDATE_TT(5);
                                                    CHANNEL_STATE_UPDATE_TT(6);  CHANNEL_STATE_UPDATE_TT(7);
                                                    CHANNEL_STATE_UPDATE_TT(8);  CHANNEL_STATE_UPDATE_TT(9);
                                                    CHANNEL_STATE_UPDATE_TT(10); CHANNEL_STATE_UPDATE_TT(11);
                                                    CHANNEL_STATE_UPDATE_TT(12); CHANNEL_STATE_UPDATE_TT(13);
                                                    CHANNEL_STATE_UPDATE_TT(14); CHANNEL_STATE_UPDATE_TT(15);

                                                    y_acc += ((int32_t)D_val * (int32_t)x_i8);
                                                    int64_t y_scaled = (int64_t)y_acc * (int64_t)a->output_scale_q;
                                                    int32_t ssm_out = (int32_t)((y_scaled + (1LL << (OUTPUT_SHIFT - 1))) >> OUTPUT_SHIFT);
                                                    ssm_out = (ssm_out > 127) ? 127 : ((ssm_out < -128) ? -128 : ssm_out);

                                                    int16_t silu_q13 = a->silu_gate_lut_q13[(int)z_val + 128];
                                                    int32_t gated = ((int32_t)ssm_out * (int32_t)silu_q13 + (1 << 12)) >> 13;
                                                    gated = (gated > 127) ? 127 : ((gated < -128) ? -128 : gated);

                                                    out_base[tt * a->d_inner] = (int8_t)gated;
                                                }
                                            } else {
                                                const int16_t *B_seg = &B_all_base[t0 * a->d_state];
                                                const int16_t *C_seg = &C_all_base[t0 * a->d_state];
                                                const int16_t *dt_seg = dt_m + t0;
                                                for (int tt = 0; tt < t_len; tt++) {
                                                    const int16_t *B_t = &B_seg[tt * a->d_state];
                                                    const int16_t *C_t = &C_seg[tt * a->d_state];
                                                    int8_t x_i8 = x_base[tt * x_stride];
                                                    int8_t z_val = z_base[tt * z_stride];
                                                    int16_t dt_val = dt_seg[tt];

                                                    int32_t y_acc = 0;
                                                    for (int d = 0; d < a->d_state; d++) {
                                                        int16_t A_val = A_local[d];
                                                        int32_t dt_A_q23 = (int32_t)dt_val * (int32_t)A_val;
                                                        int32_t dt_A_q15 = dt_A_q23 >> 8;
                                                        int32_t exp_idx = (dt_A_q15 * 10) >> 15;
                                                        exp_idx = (exp_idx > 127) ? 127 : ((exp_idx < -128) ? -128 : exp_idx);
                                                        int16_t dA_q15 = a->exp_lut[exp_idx + 128];
                                                        int32_t dB_q23 = (int32_t)dt_val * (int32_t)B_t[d];
                                                        int16_t dB_q15 = (int16_t)(dB_q23 >> 8);
                                                        int32_t h_decay = ((int32_t)dA_q15 * (int32_t)h_local[d]) >> 15;
                                                        int32_t h_input = ((int32_t)dB_q15 * (int32_t)x_i8);
                                                        int32_t h_new = h_decay + (h_input >> 7);
                                                        h_new = (h_new > 32767) ? 32767 : ((h_new < -32768) ? -32768 : h_new);
                                                        h_local[d] = (int16_t)h_new;
                                                        y_acc += ((int32_t)C_t[d] * h_new) >> 15;
                                                    }

                                                    y_acc += ((int32_t)D_val * (int32_t)x_i8);
                                                    int64_t y_scaled = (int64_t)y_acc * (int64_t)a->output_scale_q;
                                                    int32_t ssm_out = (int32_t)((y_scaled + (1LL << (OUTPUT_SHIFT - 1))) >> OUTPUT_SHIFT);
                                                    ssm_out = (ssm_out > 127) ? 127 : ((ssm_out < -128) ? -128 : ssm_out);

                                                    int16_t silu_q13 = a->silu_gate_lut_q13[(int)z_val + 128];
                                                    int32_t gated = ((int32_t)ssm_out * (int32_t)silu_q13 + (1 << 12)) >> 13;
                                                    gated = (gated > 127) ? 127 : ((gated < -128) ? -128 : gated);

                                                    out_base[tt * a->d_inner] = (int8_t)gated;
                                                }
                                            }

                                            #undef CHANNEL_STATE_UPDATE_TT

                                            // Write back hidden state (to L1 tile or L2).
                                            int16_t *h_dst = use_hstate_tiling ? (h_state_tile_l1 + state_off) : (a->h_state_q15 + h_base);
                                            for (int d = 0; d < a->d_state && d < 16; d++) {
                                                h_dst[d] = h_local[d];
                                            }
                                        }

                                        // Ensure all cores are done before the next DMA overwrites the time tile.
                                        pi_cl_team_barrier();
                                    }

                                    // Flush h_state tile back to L2 once per channel tile (optional).
                                    if (use_hstate_tiling) {
                                        pi_cl_team_barrier();
                                        if (core_id == 0) {
                                            pi_cl_dma_copy_t hs_dma;
                                            hs_dma.dir = PI_CL_DMA_DIR_LOC2EXT;
                                            hs_dma.merge = 0;
                                            hs_dma.size = (uint32_t)((size_t)tile_len * hstate_bytes_per_ch);
                                            int16_t *dst = a->h_state_q15 + (size_t)b * (size_t)a->d_inner * (size_t)a->d_state + (size_t)tile0 * (size_t)a->d_state;
                                            hs_dma.ext = (uint32_t)dst;
                                            hs_dma.loc = (uint32_t)h_state_tile_l1;
                                            pi_cl_dma_memcpy(&hs_dma);
                                            pi_cl_dma_wait(&hs_dma);
                                        }
                                        pi_cl_team_barrier();
                                    }
                                }

                                goto ssm_ph3_done;
                            }
                        }
                    }
                }
#endif  // 0
#endif

                if ((stage_x || stage_z) && tile_ch >= 4) {
                    uintptr_t cur = io_base;
                    int8_t *x_tile_l1 = NULL;
                    int8_t *z_tile_l1 = NULL;
                    int16_t *dt_tile_l1 = NULL;

                    if (stage_x) {
                        x_tile_l1 = (int8_t *)cur;
                        cur += (uintptr_t)((size_t)a->seq_len * (size_t)tile_ch);
                        if (stage_z || stage_dt) {
                            cur += (uintptr_t)MAMBA_SSM_L1_IO_PAD_BYTES;
                            cur = (cur + align_mask) & ~align_mask;
                        }
                    }
                    if (stage_z) {
                        z_tile_l1 = (int8_t *)cur;
                        cur += (uintptr_t)((size_t)a->seq_len * (size_t)tile_ch);
                        if (stage_dt) {
                            cur += (uintptr_t)MAMBA_SSM_L1_IO_PAD_BYTES;
                            cur = (cur + align_mask) & ~align_mask;
                        }
                    }

                    if (stage_dt) {
                        uintptr_t dt_addr = cur;
                        dt_addr = (dt_addr + 3) & ~(uintptr_t)3;  // align for int16/int32 access
                        dt_tile_l1 = (int16_t *)dt_addr;
                    }

                    const int z_tile_is_channel_major = stage_z && (a->z_layout == MAMBA_SSM_Z_LAYOUT_CHANNEL_MAJOR);

                    for (int tile0 = 0; tile0 < a->d_inner; tile0 += tile_ch) {
                        int tile_len = a->d_inner - tile0;
                        if (tile_len > tile_ch) tile_len = tile_ch;

                        if (core_id == 0) {
                            if (stage_x) {
                                // x is time-major [B, L, d_inner]; stage a [L, tile_len] slice.
                                pi_cl_dma_copy_2d_t x_dma;
                                x_dma.dir = PI_CL_DMA_DIR_EXT2LOC;
                                x_dma.merge = 0;
                                x_dma.length = (uint32_t)tile_len;
                                x_dma.stride = (uint32_t)a->d_inner;
                                x_dma.size = (uint32_t)((size_t)a->seq_len * (size_t)tile_len);

                                const int8_t *src = a->x_int8 + (b * a->seq_len) * a->d_inner + tile0;
                                x_dma.ext = (uint32_t)src;
                                x_dma.loc = (uint32_t)x_tile_l1;
                                pi_cl_dma_memcpy_2d(&x_dma);
                                pi_cl_dma_wait(&x_dma);
                            }

                            if (stage_z) {
                                if (z_tile_is_channel_major) {
                                    // z is [B, d_inner, L]; stage a [tile_len, L] slice (channel-major).
                                    pi_cl_dma_copy_2d_t z_dma;
                                    z_dma.dir = PI_CL_DMA_DIR_EXT2LOC;
                                    z_dma.merge = 0;
                                    z_dma.length = (uint32_t)a->seq_len;
                                    z_dma.stride = (uint32_t)a->seq_len;
                                    z_dma.size = (uint32_t)((size_t)tile_len * (size_t)a->seq_len);

                                    const int8_t *src = a->z_int8 + (b * a->d_inner + tile0) * a->seq_len;
                                    z_dma.ext = (uint32_t)src;
                                    z_dma.loc = (uint32_t)z_tile_l1;
                                    pi_cl_dma_memcpy_2d(&z_dma);
                                    pi_cl_dma_wait(&z_dma);
                                } else {
                                    // z is time-major ([B, L, d_inner] or interleaved in xz_proj); stage a [L, tile_len] slice.
                                    pi_cl_dma_copy_2d_t z_dma;
                                    z_dma.dir = PI_CL_DMA_DIR_EXT2LOC;
                                    z_dma.merge = 0;
                                    z_dma.length = (uint32_t)tile_len;
                                    z_dma.stride = (uint32_t)((a->z_layout == MAMBA_SSM_Z_LAYOUT_XZ_INTERLEAVED) ? (2 * a->d_inner) : a->d_inner);
                                    z_dma.size = (uint32_t)((size_t)a->seq_len * (size_t)tile_len);

                                    const int8_t *src;
                                    if (a->z_layout == MAMBA_SSM_Z_LAYOUT_XZ_INTERLEAVED) {
                                        src = a->z_int8 + (b * a->seq_len) * 2 * a->d_inner + a->d_inner + tile0;
                                    } else {
                                        src = a->z_int8 + (b * a->seq_len) * a->d_inner + tile0;
                                    }
                                    z_dma.ext = (uint32_t)src;
                                    z_dma.loc = (uint32_t)z_tile_l1;
                                    pi_cl_dma_memcpy_2d(&z_dma);
                                    pi_cl_dma_wait(&z_dma);
                                }
                            }

                            if (stage_dt) {
                                // dt_all is channel-major [d_inner, L]; stage a [tile_len, L] slice (contiguous).
                                pi_cl_dma_copy_t dt_dma;
                                dt_dma.dir = PI_CL_DMA_DIR_EXT2LOC;
                                dt_dma.merge = 0;
                                dt_dma.size = (uint32_t)((size_t)tile_len * (size_t)a->seq_len * sizeof(int16_t));
                                const int16_t *src = a->dt_all + (size_t)tile0 * (size_t)a->seq_len;
                                dt_dma.ext = (uint32_t)src;
                                dt_dma.loc = (uint32_t)dt_tile_l1;
                                pi_cl_dma_memcpy(&dt_dma);
                                pi_cl_dma_wait(&dt_dma);
                            }
                        }
                        pi_cl_team_barrier();  // tile ready

                        // Distribute this channel tile across cores (avoid serializing tiles by using the
                        // global ch_start/ch_end partitioning).
                        const int tile_channels_per_core = (tile_len + NUM_CORES - 1) / NUM_CORES;
                        int m_tile_start = tile0 + core_id * tile_channels_per_core;
                        int m_tile_end = m_tile_start + tile_channels_per_core;
                        if (m_tile_end > tile0 + tile_len) m_tile_end = tile0 + tile_len;
                        if (m_tile_start > tile0 + tile_len) m_tile_start = tile0 + tile_len;

                        for (int m = m_tile_start; m < m_tile_end; m++) {
                            // Load hidden state into local registers
                            int16_t h_local[16];
                            int h_base = b * a->d_inner * a->d_state + m * a->d_state;
                            for (int d = 0; d < a->d_state && d < 16; d++) {
                                h_local[d] = a->h_state_q15[h_base + d];
                            }

                            // Channel-specific constants
                            int16_t D_val = a->D_q15[m];
                            int16_t A_local[16];
                            for (int d = 0; d < a->d_state && d < 16; d++) {
                                A_local[d] = a->A_q15[d * a->d_inner + m];
                            }
                            const int16_t *dt_m = &a->dt_all[m * a->seq_len];
                            if (stage_dt) {
                                dt_m = dt_tile_l1 + (size_t)(m - tile0) * (size_t)a->seq_len;
                            }

                            // x (optionally staged)
                            const int8_t *x_base = a->x_int8 + (b * a->seq_len) * a->d_inner + m;
                            int x_stride = a->d_inner;
                            if (stage_x) {
                                x_base = x_tile_l1 + (m - tile0);
                                x_stride = tile_len;
                            }

                            // z (optionally staged)
                            const int8_t *z_base;
                            int z_stride;
                            if (stage_z) {
                                if (z_tile_is_channel_major) {
                                    z_base = z_tile_l1 + (m - tile0) * a->seq_len;
                                    z_stride = 1;
                                } else {
                                    z_base = z_tile_l1 + (m - tile0);
                                    z_stride = tile_len;
                                }
                            } else {
                                if (a->z_layout == MAMBA_SSM_Z_LAYOUT_CHANNEL_MAJOR) {
                                    z_base = a->z_int8 + (b * a->d_inner + m) * a->seq_len;
                                    z_stride = 1;
                                } else if (a->z_layout == MAMBA_SSM_Z_LAYOUT_TIME_MAJOR) {
                                    z_base = a->z_int8 + (b * a->seq_len) * a->d_inner + m;
                                    z_stride = a->d_inner;
                                } else {
                                    z_base = a->z_int8 + (b * a->seq_len) * 2 * a->d_inner + a->d_inner + m;
                                    z_stride = 2 * a->d_inner;
                                }
                            }

                            int8_t *out_base = a->output_int8 + (b * a->seq_len) * a->d_inner + m;

                            #define CHANNEL_STATE_UPDATE_TILE(d) do {                     int16_t A_val = A_local[d];                     int32_t dt_A_q23 = (int32_t)dt_val * (int32_t)A_val;                     int32_t dt_A_q15 = dt_A_q23 >> 8;                     int32_t exp_idx = (dt_A_q15 * 10) >> 15;                     exp_idx = (exp_idx > 127) ? 127 : ((exp_idx < -128) ? -128 : exp_idx);                     int16_t dA_q15 = a->exp_lut[exp_idx + 128];                     int32_t dB_q23 = (int32_t)dt_val * (int32_t)B_t[d];                     int16_t dB_q15 = (int16_t)(dB_q23 >> 8);                     int32_t h_decay = ((int32_t)dA_q15 * (int32_t)h_local[d]) >> 15;                     int32_t h_input = ((int32_t)dB_q15 * (int32_t)x_i8);                     int32_t h_new = h_decay + (h_input >> 7);                     h_new = (h_new > 32767) ? 32767 : ((h_new < -32768) ? -32768 : h_new);                     h_local[d] = (int16_t)h_new;                     y_acc += ((int32_t)C_t[d] * h_new) >> 15;                 } while(0)

                            if (a->d_state == 4) {
                                for (int t = 0; t < a->seq_len; t++) {
                                    const int16_t *B_t = &B_all_base[t * 4];
                                    const int16_t *C_t = &C_all_base[t * 4];
                                    int8_t x_i8 = x_base[t * x_stride];
                                    int8_t z_val = z_base[t * z_stride];
                                    int16_t dt_val = dt_m[t];

                                    int32_t y_acc = 0;
                                    CHANNEL_STATE_UPDATE_TILE(0);
                                    CHANNEL_STATE_UPDATE_TILE(1);
                                    CHANNEL_STATE_UPDATE_TILE(2);
                                    CHANNEL_STATE_UPDATE_TILE(3);

                                    y_acc += ((int32_t)D_val * (int32_t)x_i8);
                                    int64_t y_scaled = (int64_t)y_acc * (int64_t)a->output_scale_q;
                                    int32_t ssm_out = (int32_t)((y_scaled + (1LL << (OUTPUT_SHIFT - 1))) >> OUTPUT_SHIFT);
                                    ssm_out = (ssm_out > 127) ? 127 : ((ssm_out < -128) ? -128 : ssm_out);

                                    int16_t silu_q13 = a->silu_gate_lut_q13[(int)z_val + 128];
                                    int32_t gated = ((int32_t)ssm_out * (int32_t)silu_q13 + (1 << 12)) >> 13;
                                    gated = (gated > 127) ? 127 : ((gated < -128) ? -128 : gated);

                                    out_base[t * a->d_inner] = (int8_t)gated;
                                }
                            } else if (a->d_state == 8) {
                                for (int t = 0; t < a->seq_len; t++) {
                                    const int16_t *B_t = &B_all_base[t * 8];
                                    const int16_t *C_t = &C_all_base[t * 8];
                                    int8_t x_i8 = x_base[t * x_stride];
                                    int8_t z_val = z_base[t * z_stride];
                                    int16_t dt_val = dt_m[t];

                                    int32_t y_acc = 0;
                                    CHANNEL_STATE_UPDATE_TILE(0); CHANNEL_STATE_UPDATE_TILE(1);
                                    CHANNEL_STATE_UPDATE_TILE(2); CHANNEL_STATE_UPDATE_TILE(3);
                                    CHANNEL_STATE_UPDATE_TILE(4); CHANNEL_STATE_UPDATE_TILE(5);
                                    CHANNEL_STATE_UPDATE_TILE(6); CHANNEL_STATE_UPDATE_TILE(7);

                                    y_acc += ((int32_t)D_val * (int32_t)x_i8);
                                    int64_t y_scaled = (int64_t)y_acc * (int64_t)a->output_scale_q;
                                    int32_t ssm_out = (int32_t)((y_scaled + (1LL << (OUTPUT_SHIFT - 1))) >> OUTPUT_SHIFT);
                                    ssm_out = (ssm_out > 127) ? 127 : ((ssm_out < -128) ? -128 : ssm_out);

                                    int16_t silu_q13 = a->silu_gate_lut_q13[(int)z_val + 128];
                                    int32_t gated = ((int32_t)ssm_out * (int32_t)silu_q13 + (1 << 12)) >> 13;
                                    gated = (gated > 127) ? 127 : ((gated < -128) ? -128 : gated);

                                    out_base[t * a->d_inner] = (int8_t)gated;
                                }
                            } else if (a->d_state == 16) {
                                for (int t = 0; t < a->seq_len; t++) {
                                    const int16_t *B_t = &B_all_base[t * 16];
                                    const int16_t *C_t = &C_all_base[t * 16];
                                    int8_t x_i8 = x_base[t * x_stride];
                                    int8_t z_val = z_base[t * z_stride];
                                    int16_t dt_val = dt_m[t];

                                    int32_t y_acc = 0;
                                    CHANNEL_STATE_UPDATE_TILE(0);  CHANNEL_STATE_UPDATE_TILE(1);
                                    CHANNEL_STATE_UPDATE_TILE(2);  CHANNEL_STATE_UPDATE_TILE(3);
                                    CHANNEL_STATE_UPDATE_TILE(4);  CHANNEL_STATE_UPDATE_TILE(5);
                                    CHANNEL_STATE_UPDATE_TILE(6);  CHANNEL_STATE_UPDATE_TILE(7);
                                    CHANNEL_STATE_UPDATE_TILE(8);  CHANNEL_STATE_UPDATE_TILE(9);
                                    CHANNEL_STATE_UPDATE_TILE(10); CHANNEL_STATE_UPDATE_TILE(11);
                                    CHANNEL_STATE_UPDATE_TILE(12); CHANNEL_STATE_UPDATE_TILE(13);
                                    CHANNEL_STATE_UPDATE_TILE(14); CHANNEL_STATE_UPDATE_TILE(15);

                                    y_acc += ((int32_t)D_val * (int32_t)x_i8);
                                    int64_t y_scaled = (int64_t)y_acc * (int64_t)a->output_scale_q;
                                    int32_t ssm_out = (int32_t)((y_scaled + (1LL << (OUTPUT_SHIFT - 1))) >> OUTPUT_SHIFT);
                                    ssm_out = (ssm_out > 127) ? 127 : ((ssm_out < -128) ? -128 : ssm_out);

                                    int16_t silu_q13 = a->silu_gate_lut_q13[(int)z_val + 128];
                                    int32_t gated = ((int32_t)ssm_out * (int32_t)silu_q13 + (1 << 12)) >> 13;
                                    gated = (gated > 127) ? 127 : ((gated < -128) ? -128 : gated);

                                    out_base[t * a->d_inner] = (int8_t)gated;
                                }
                            } else {
                                for (int t = 0; t < a->seq_len; t++) {
                                    const int16_t *B_t = &B_all_base[t * a->d_state];
                                    const int16_t *C_t = &C_all_base[t * a->d_state];
                                    int8_t x_i8 = x_base[t * x_stride];
                                    int8_t z_val = z_base[t * z_stride];
                                    int16_t dt_val = dt_m[t];

                                    int32_t y_acc = 0;
                                    for (int d = 0; d < a->d_state; d++) {
                                        int16_t A_val = A_local[d];
                                        int32_t dt_A_q23 = (int32_t)dt_val * (int32_t)A_val;
                                        int32_t dt_A_q15 = dt_A_q23 >> 8;
                                        int32_t exp_idx = (dt_A_q15 * 10) >> 15;
                                        exp_idx = (exp_idx > 127) ? 127 : ((exp_idx < -128) ? -128 : exp_idx);
                                        int16_t dA_q15 = a->exp_lut[exp_idx + 128];
                                        int32_t dB_q23 = (int32_t)dt_val * (int32_t)B_t[d];
                                        int16_t dB_q15 = (int16_t)(dB_q23 >> 8);
                                        int32_t h_decay = ((int32_t)dA_q15 * (int32_t)h_local[d]) >> 15;
                                        int32_t h_input = ((int32_t)dB_q15 * (int32_t)x_i8);
                                        int32_t h_new = h_decay + (h_input >> 7);
                                        h_new = (h_new > 32767) ? 32767 : ((h_new < -32768) ? -32768 : h_new);
                                        h_local[d] = (int16_t)h_new;
                                        y_acc += ((int32_t)C_t[d] * h_new) >> 15;
                                    }

                                    y_acc += ((int32_t)D_val * (int32_t)x_i8);
                                    int64_t y_scaled = (int64_t)y_acc * (int64_t)a->output_scale_q;
                                    int32_t ssm_out = (int32_t)((y_scaled + (1LL << (OUTPUT_SHIFT - 1))) >> OUTPUT_SHIFT);
                                    ssm_out = (ssm_out > 127) ? 127 : ((ssm_out < -128) ? -128 : ssm_out);

                                    int16_t silu_q13 = a->silu_gate_lut_q13[(int)z_val + 128];
                                    int32_t gated = ((int32_t)ssm_out * (int32_t)silu_q13 + (1 << 12)) >> 13;
                                    gated = (gated > 127) ? 127 : ((gated < -128) ? -128 : gated);

                                    out_base[t * a->d_inner] = (int8_t)gated;
                                }
                            }

                            #undef CHANNEL_STATE_UPDATE_TILE

                            // Write back hidden state
                            for (int d = 0; d < a->d_state && d < 16; d++) {
                                a->h_state_q15[h_base + d] = h_local[d];
                            }
                        }

                        // Ensure all cores are done before the next DMA overwrites the tile.
                        pi_cl_team_barrier();
                    }

                    goto ssm_ph3_done;
                }
            }
        }
#endif

        for (int m = ch_start; m < ch_end; m++) {
            // Load hidden state into local registers
            int16_t h_local[16];  // Support up to d_state=16 (FEMBA uses 16)
            int h_base = b * a->d_inner * a->d_state + m * a->d_state;
            for (int d = 0; d < a->d_state && d < 16; d++) {
                h_local[d] = a->h_state_q15[h_base + d];
            }

            // Load channel-specific constants
            int16_t D_val = a->D_q15[m];
            int16_t A_local[16];
            for (int d = 0; d < a->d_state && d < 16; d++) {
                A_local[d] = a->A_q15[d * a->d_inner + m];
            }
            const int16_t *dt_m = &a->dt_all[m * a->seq_len];
            const int8_t *z_base;
            int z_stride;
            if (a->z_layout == MAMBA_SSM_Z_LAYOUT_CHANNEL_MAJOR) {
                z_base = a->z_int8 + (b * a->d_inner + m) * a->seq_len;
                z_stride = 1;
            } else if (a->z_layout == MAMBA_SSM_Z_LAYOUT_TIME_MAJOR) {
                z_base = a->z_int8 + (b * a->seq_len) * a->d_inner + m;
                z_stride = a->d_inner;
            } else {
                z_base = a->z_int8 + (b * a->seq_len) * 2 * a->d_inner + a->d_inner + m;
                z_stride = 2 * a->d_inner;
            }

            // Sequential time loop - NO BARRIERS!
            const int8_t *x_base = a->x_int8 + (b * a->seq_len) * a->d_inner + m;
            int8_t *out_base = a->output_int8 + (b * a->seq_len) * a->d_inner + m;

            // Hoist the d_state dispatch out of the inner time loop (common case: d_state=4).
            #define CHANNEL_STATE_UPDATE(d) do {                     int16_t A_val = A_local[d];                     int32_t dt_A_q23 = (int32_t)dt_val * (int32_t)A_val;                     int32_t dt_A_q15 = dt_A_q23 >> 8;                     int32_t exp_idx = (dt_A_q15 * 10) >> 15;                     exp_idx = (exp_idx > 127) ? 127 : ((exp_idx < -128) ? -128 : exp_idx);                     int16_t dA_q15 = a->exp_lut[exp_idx + 128];                     int32_t dB_q23 = (int32_t)dt_val * (int32_t)B_t[d];                     int16_t dB_q15 = (int16_t)(dB_q23 >> 8);                     int32_t h_decay = ((int32_t)dA_q15 * (int32_t)h_local[d]) >> 15;                     int32_t h_input = ((int32_t)dB_q15 * (int32_t)x_i8);                     int32_t h_new = h_decay + (h_input >> 7);                     h_new = (h_new > 32767) ? 32767 : ((h_new < -32768) ? -32768 : h_new);                     h_local[d] = (int16_t)h_new;                     y_acc += ((int32_t)C_t[d] * h_new) >> 15;                 } while(0)

	            if (a->d_state == 4) {
	                for (int t = 0; t < a->seq_len; t++) {
	                    const int16_t *B_t = &B_all_base[t * 4];
	                    const int16_t *C_t = &C_all_base[t * 4];
	                    int8_t x_i8 = x_base[t * a->d_inner];
	                    int8_t z_val = z_base[t * z_stride];
	                    int16_t dt_val = dt_m[t];

	                    int32_t y_acc = 0;
	                    CHANNEL_STATE_UPDATE(0);
	                    CHANNEL_STATE_UPDATE(1);
	                    CHANNEL_STATE_UPDATE(2);
	                    CHANNEL_STATE_UPDATE(3);

                    // Feedthrough
                    y_acc += ((int32_t)D_val * (int32_t)x_i8);

                    // Scale SSM output to INT8
                    int64_t y_scaled = (int64_t)y_acc * (int64_t)a->output_scale_q;
                    int32_t ssm_out = (int32_t)((y_scaled + (1LL << (OUTPUT_SHIFT - 1))) >> OUTPUT_SHIFT);
                    ssm_out = (ssm_out > 127) ? 127 : ((ssm_out < -128) ? -128 : ssm_out);

                    // Gating: output = ssm_out * SiLU(z)
                    int16_t silu_q13 = a->silu_gate_lut_q13[(int)z_val + 128];
	                    int32_t gated = ((int32_t)ssm_out * (int32_t)silu_q13 + (1 << 12)) >> 13;
	                    gated = (gated > 127) ? 127 : ((gated < -128) ? -128 : gated);

	                    out_base[t * a->d_inner] = (int8_t)gated;
	                }
	            } else if (a->d_state == 8) {
	                for (int t = 0; t < a->seq_len; t++) {
	                    const int16_t *B_t = &B_all_base[t * 8];
	                    const int16_t *C_t = &C_all_base[t * 8];
	                    int8_t x_i8 = x_base[t * a->d_inner];
	                    int8_t z_val = z_base[t * z_stride];
	                    int16_t dt_val = dt_m[t];

	                    int32_t y_acc = 0;
	                    CHANNEL_STATE_UPDATE(0); CHANNEL_STATE_UPDATE(1);
	                    CHANNEL_STATE_UPDATE(2); CHANNEL_STATE_UPDATE(3);
                    CHANNEL_STATE_UPDATE(4); CHANNEL_STATE_UPDATE(5);
                    CHANNEL_STATE_UPDATE(6); CHANNEL_STATE_UPDATE(7);

                    y_acc += ((int32_t)D_val * (int32_t)x_i8);
                    int64_t y_scaled = (int64_t)y_acc * (int64_t)a->output_scale_q;
                    int32_t ssm_out = (int32_t)((y_scaled + (1LL << (OUTPUT_SHIFT - 1))) >> OUTPUT_SHIFT);
                    ssm_out = (ssm_out > 127) ? 127 : ((ssm_out < -128) ? -128 : ssm_out);

                    int16_t silu_q13 = a->silu_gate_lut_q13[(int)z_val + 128];
	                    int32_t gated = ((int32_t)ssm_out * (int32_t)silu_q13 + (1 << 12)) >> 13;
	                    gated = (gated > 127) ? 127 : ((gated < -128) ? -128 : gated);

	                    out_base[t * a->d_inner] = (int8_t)gated;
	                }
	            } else if (a->d_state == 16) {
	                for (int t = 0; t < a->seq_len; t++) {
	                    const int16_t *B_t = &B_all_base[t * 16];
	                    const int16_t *C_t = &C_all_base[t * 16];
	                    int8_t x_i8 = x_base[t * a->d_inner];
	                    int8_t z_val = z_base[t * z_stride];
	                    int16_t dt_val = dt_m[t];

	                    int32_t y_acc = 0;
	                    CHANNEL_STATE_UPDATE(0);  CHANNEL_STATE_UPDATE(1);
	                    CHANNEL_STATE_UPDATE(2);  CHANNEL_STATE_UPDATE(3);
                    CHANNEL_STATE_UPDATE(4);  CHANNEL_STATE_UPDATE(5);
                    CHANNEL_STATE_UPDATE(6);  CHANNEL_STATE_UPDATE(7);
                    CHANNEL_STATE_UPDATE(8);  CHANNEL_STATE_UPDATE(9);
                    CHANNEL_STATE_UPDATE(10); CHANNEL_STATE_UPDATE(11);
                    CHANNEL_STATE_UPDATE(12); CHANNEL_STATE_UPDATE(13);
                    CHANNEL_STATE_UPDATE(14); CHANNEL_STATE_UPDATE(15);

                    y_acc += ((int32_t)D_val * (int32_t)x_i8);
                    int64_t y_scaled = (int64_t)y_acc * (int64_t)a->output_scale_q;
                    int32_t ssm_out = (int32_t)((y_scaled + (1LL << (OUTPUT_SHIFT - 1))) >> OUTPUT_SHIFT);
                    ssm_out = (ssm_out > 127) ? 127 : ((ssm_out < -128) ? -128 : ssm_out);

                    int16_t silu_q13 = a->silu_gate_lut_q13[(int)z_val + 128];
	                    int32_t gated = ((int32_t)ssm_out * (int32_t)silu_q13 + (1 << 12)) >> 13;
	                    gated = (gated > 127) ? 127 : ((gated < -128) ? -128 : gated);

	                    out_base[t * a->d_inner] = (int8_t)gated;
	                }
	            } else {
	                for (int t = 0; t < a->seq_len; t++) {
	                    const int16_t *B_t = &B_all_base[t * a->d_state];
	                    const int16_t *C_t = &C_all_base[t * a->d_state];
	                    int8_t x_i8 = x_base[t * a->d_inner];
	                    int8_t z_val = z_base[t * z_stride];
	                    int16_t dt_val = dt_m[t];

	                    int32_t y_acc = 0;
	                    for (int d = 0; d < a->d_state; d++) {
                        int16_t A_val = A_local[d];
                        int32_t dt_A_q23 = (int32_t)dt_val * (int32_t)A_val;
                        int32_t dt_A_q15 = dt_A_q23 >> 8;
                        int32_t exp_idx = (dt_A_q15 * 10) >> 15;
                        exp_idx = (exp_idx > 127) ? 127 : ((exp_idx < -128) ? -128 : exp_idx);
                        int16_t dA_q15 = a->exp_lut[exp_idx + 128];
                        int32_t dB_q23 = (int32_t)dt_val * (int32_t)B_t[d];
                        int16_t dB_q15 = (int16_t)(dB_q23 >> 8);
                        int32_t h_decay = ((int32_t)dA_q15 * (int32_t)h_local[d]) >> 15;
                        int32_t h_input = ((int32_t)dB_q15 * (int32_t)x_i8);
                        int32_t h_new = h_decay + (h_input >> 7);
                        h_new = (h_new > 32767) ? 32767 : ((h_new < -32768) ? -32768 : h_new);
                        h_local[d] = (int16_t)h_new;
                        y_acc += ((int32_t)C_t[d] * h_new) >> 15;
                    }

                    y_acc += ((int32_t)D_val * (int32_t)x_i8);
                    int64_t y_scaled = (int64_t)y_acc * (int64_t)a->output_scale_q;
                    int32_t ssm_out = (int32_t)((y_scaled + (1LL << (OUTPUT_SHIFT - 1))) >> OUTPUT_SHIFT);
                    ssm_out = (ssm_out > 127) ? 127 : ((ssm_out < -128) ? -128 : ssm_out);

                    int16_t silu_q13 = a->silu_gate_lut_q13[(int)z_val + 128];
	                    int32_t gated = ((int32_t)ssm_out * (int32_t)silu_q13 + (1 << 12)) >> 13;
	                    gated = (gated > 127) ? 127 : ((gated < -128) ? -128 : gated);

	                    out_base[t * a->d_inner] = (int8_t)gated;
	                }
	            }

            #undef CHANNEL_STATE_UPDATE

            // Write back hidden state
            for (int d = 0; d < a->d_state && d < 16; d++) {
                a->h_state_q15[h_base + d] = h_local[d];
            }
        }
ssm_ph3_done:

#if defined(ENABLE_MAMBA_SSM_EVENT_PROFILING)
        // Ensure all cores have completed Stage 3 before reporting.
        pi_cl_team_barrier();
        if (core_id == 0 && b == 0) {
            uint32_t cycles_now = pi_perf_read(PI_PERF_CYCLES);
            uint32_t ld_stall_now = pi_perf_read(PI_PERF_LD_STALL);
            uint32_t tcdm_cont_now = pi_perf_read(PI_PERF_TCDM_CONT);
            uint32_t ld_ext_cyc_now = pi_perf_read(PI_PERF_LD_EXT_CYC);
            printf("PERF_SSM PH3: cycles=%u ld_stall=%u tcdm_cont=%u ld_ext_cyc=%u\n",
                   cycles_now - phase_cycles_start,
                   ld_stall_now - phase_ld_stall_start,
                   tcdm_cont_now - phase_tcdm_cont_start,
                   ld_ext_cyc_now - phase_ld_ext_cyc_start);
        }
#elif !defined(DISABLE_MAMBA_SSM_BATCH_SYNC)
        // Needed if batch>1 since proj_all/dt_all/B/C are reused across batches.
        if (a->batch > 1) pi_cl_team_barrier();
#endif
    }
// Final barrier (only needed if caller expects synchronization)
    pi_cl_team_barrier();
}

// Parallel Transpose worker: [B, L, 2*d_inner] x-part -> [B, d_inner, L]
typedef struct {
    const int8_t *src;
    int8_t *dst;
    int batch;
    int seq_len;
    int d_inner;
} transpose_x_args_t;
static void transpose_x_worker(void *arg) {
    transpose_x_args_t *a = (transpose_x_args_t *)arg;
    int core_id = pi_core_id();
    // Parallelize across channels
    int chunk = (a->d_inner + NUM_CORES - 1) / NUM_CORES;
    int start_c = core_id * chunk;
    int end_c = (start_c + chunk < a->d_inner) ? (start_c + chunk) : a->d_inner;

    for (int b = 0; b < a->batch; b++) {
        for (int c = start_c; c < end_c; c++) {
            for (int l = 0; l < a->seq_len; l++) {
                // src: xz_proj[(b*L + l) * 2*d_inner + c]
                // dst: x_transposed[b * d_inner * L + c * L + l]
                a->dst[b * a->d_inner * a->seq_len + c * a->seq_len + l] =
                    a->src[(b * a->seq_len + l) * 2 * a->d_inner + c];
            }
        }
    }
    pi_cl_team_barrier();
}

// Parallel flip worker: reverse sequence dimension [B, L, d_model]
typedef struct {
    const int8_t *input;
    int8_t *output;
    int batch;
    int seq_len;
    int d_model;
} flip_input_args_t;
static void flip_input_worker(void *arg) {
    flip_input_args_t *a = (flip_input_args_t *)arg;
    int core_id = pi_core_id();
    int chunk = (a->seq_len + NUM_CORES - 1) / NUM_CORES;
    int start_t = core_id * chunk;
    int end_t = (start_t + chunk < a->seq_len) ? (start_t + chunk) : a->seq_len;

    for (int b = 0; b < a->batch; b++) {
        for (int t = start_t; t < end_t; t++) {
            int src_idx = (b * a->seq_len + t) * a->d_model;
            int dst_idx = (b * a->seq_len + (a->seq_len - 1 - t)) * a->d_model;
            memcpy(&a->output[dst_idx], &a->input[src_idx], a->d_model);
        }
    }
    pi_cl_team_barrier();
}

// This guard keeps a single definition when both paths are present.
#ifndef FLIP_ADD_DEFINED
#define FLIP_ADD_DEFINED
// Parallel flip+add worker: output[t] += rev_out_raw[L-1-t] + residual[t] with saturation
typedef struct {
    int8_t *output;
    const int8_t *rev_out;
    const int8_t *residual;  // Input residual (NULL to skip)
    int batch;
    int seq_len;
    int d_model;
} flip_add_args_t;
static void flip_add_worker(void *arg) {
    flip_add_args_t *a = (flip_add_args_t *)arg;
    int core_id = pi_core_id();
    int chunk = (a->seq_len + NUM_CORES - 1) / NUM_CORES;
    int start_t = core_id * chunk;
    int end_t = (start_t + chunk < a->seq_len) ? (start_t + chunk) : a->seq_len;

    for (int b = 0; b < a->batch; b++) {
        for (int t = start_t; t < end_t; t++) {
            int out_idx = (b * a->seq_len + t) * a->d_model;
            int rev_idx = (b * a->seq_len + (a->seq_len - 1 - t)) * a->d_model;
            int8_t *out_ptr = &a->output[out_idx];
            const int8_t *rev_ptr = &a->rev_out[rev_idx];
            for (int d = 0; d < a->d_model; d++) {
                int32_t sum = (int32_t)out_ptr[d] + (int32_t)rev_ptr[d];
                if (a->residual) sum += (int32_t)a->residual[out_idx + d];
                if (sum > 127) sum = 127;
                if (sum < -128) sum = -128;
                out_ptr[d] = (int8_t)sum;
            }
        }
    }
    pi_cl_team_barrier();
}
#endif  // FLIP_ADD_DEFINED

% endif  ## End of Mamba/SSM workers conditional

typedef struct {
    const int8_t *input, *weights;
    const float *bias;
    int8_t *output;
    uint16_t in_features, out_features, batch_tokens;
    uint16_t output_stride;
    uint16_t seq_len;
    uint8_t output_layout;  // 0: [tokens, out_features], 1: [batch, out_features, seq_len]
    float scale_in, scale_w, scale_out;
} linear_int8_args_t;

// Linear with output_stride support for chunked projection execution
// If output_stride == 0 or == out_features, uses normal contiguous layout
// If output_stride > out_features, writes to strided positions (for chunked output)
static void linear_int8_worker(void *arg) {
    linear_int8_args_t *a = (linear_int8_args_t *)arg;
    uint16_t ostride = (a->output_stride > 0) ? a->output_stride : a->out_features;
    if (!a->output_layout) {
        if (a->batch_tokens == 1) {
            network_linear_int8(a->input, a->weights, a->bias, a->output, a->in_features, a->out_features, a->scale_in, a->scale_w, a->scale_out);
        } else {
            int core_id = pi_core_id();
            int tokens_per_core = (a->batch_tokens + NUM_CORES - 1) / NUM_CORES;
            int start = core_id * tokens_per_core;
            int end = (start + tokens_per_core > a->batch_tokens) ? a->batch_tokens : (start + tokens_per_core);
            if (start < a->batch_tokens) {
                for (int t = start; t < end; t++) {
                    network_linear_int8_sequential(a->input + t*a->in_features, a->weights, a->bias, a->output + t*ostride, a->in_features, a->out_features, a->scale_in, a->scale_w, a->scale_out);
                }
            }
            pi_cl_team_barrier();
        }
        return;
    }

    int core_id = pi_core_id();
    int tokens_per_core = (a->batch_tokens + NUM_CORES - 1) / NUM_CORES;
    int start = core_id * tokens_per_core;
    int end = (start + tokens_per_core > a->batch_tokens) ? a->batch_tokens : (start + tokens_per_core);
    if (start < a->batch_tokens) {
        for (int t = start; t < end; t++) {
            int b = t / a->seq_len;
            int l = t - b * a->seq_len;
            int8_t *out_ptr = a->output + (b * a->out_features) * a->seq_len + l;
            linear_int8_sequential_strided(
                a->input + t * a->in_features, a->weights, a->bias, out_ptr,
                a->in_features, a->out_features, a->seq_len,
                a->scale_in, a->scale_w, a->scale_out
            );
        }
    }
    pi_cl_team_barrier();
}

% if 'bimamba' in ops_present or 'mamba_block' in ops_present:
// Linear worker with reverse sequence order (used to avoid explicit flip)
typedef struct {
    const int8_t *input;
    const int8_t *weights;
    const float *bias;
    int8_t *output;
    uint16_t in_features;
    uint16_t out_features;
    uint16_t batch;
    uint16_t seq_len;
    uint16_t output_stride;
    uint8_t output_layout;  // 0: [tokens, out_features], 1: [batch, out_features, seq_len]
    float scale_in;
    float scale_w;
    float scale_out;
} linear_int8_rev_input_args_t;
static void linear_int8_rev_input_worker(void *arg) {
    linear_int8_rev_input_args_t *a = (linear_int8_rev_input_args_t *)arg;
    int core_id = pi_core_id();
    int total_tokens = a->batch * a->seq_len;
    int tokens_per_core = (total_tokens + NUM_CORES - 1) / NUM_CORES;
    int start = core_id * tokens_per_core;
    int end = (start + tokens_per_core > total_tokens) ? total_tokens : (start + tokens_per_core);
    uint16_t ostride = (a->output_stride > 0) ? a->output_stride : a->out_features;

    for (int t = start; t < end; t++) {
        int b = t / a->seq_len;
        int t_in = t - b * a->seq_len;
        int rev_t = a->seq_len - 1 - t_in;
        const int8_t *in_ptr = a->input + (b * a->seq_len + rev_t) * a->in_features;
        int8_t *out_ptr;
        if (a->output_layout) {
            out_ptr = a->output + (b * a->out_features) * a->seq_len + t_in;
            linear_int8_sequential_strided(in_ptr, a->weights, a->bias, out_ptr,
                                           a->in_features, a->out_features, a->seq_len,
                                           a->scale_in, a->scale_w, a->scale_out);
        } else {
            out_ptr = a->output + t * ostride;
            network_linear_int8_sequential(in_ptr, a->weights, a->bias, out_ptr,
                                           a->in_features, a->out_features,
                                           a->scale_in, a->scale_w, a->scale_out);
        }
    }
    pi_cl_team_barrier();
}

// 2-bit packed weight linear worker
// Uses weights packed as 4 values per byte: {-1, 0, 1} -> {0, 1, 2}
typedef struct { const int8_t *input; const uint8_t *weights_packed; const float *bias; int8_t *output; uint16_t in_features, out_features, batch_tokens; uint16_t output_stride; float scale_in, scale_w, scale_out; } linear_2bit_args_t;
static void linear_2bit_worker(void *arg) {
    linear_2bit_args_t *a = (linear_2bit_args_t *)arg;
    uint16_t ostride = (a->output_stride > 0) ? a->output_stride : a->out_features;
    if (a->batch_tokens == 1) {
        network_linear_2bit_int8(a->input, a->weights_packed, a->bias, a->output, a->in_features, a->out_features, a->scale_in, a->scale_w, a->scale_out);
    } else {
        int core_id = pi_core_id();
        int tokens_per_core = (a->batch_tokens + NUM_CORES - 1) / NUM_CORES;
        int start = core_id * tokens_per_core;
        int end = (start + tokens_per_core > a->batch_tokens) ? a->batch_tokens : (start + tokens_per_core);
        if (start < a->batch_tokens) {
            for (int t = start; t < end; t++) {
                network_linear_2bit_int8_sequential(a->input + t*a->in_features, a->weights_packed, a->bias, a->output + t*ostride, a->in_features, a->out_features, a->scale_in, a->scale_w, a->scale_out);
            }
        }
        pi_cl_team_barrier();
    }
}

// 2-bit linear worker with reverse sequence order (used to avoid explicit flip)
typedef struct {
    const int8_t *input;
    const uint8_t *weights_packed;
    const float *bias;
    int8_t *output;
    uint16_t in_features;
    uint16_t out_features;
    uint16_t batch;
    uint16_t seq_len;
    uint16_t output_stride;
    float scale_in;
    float scale_w;
    float scale_out;
} linear_2bit_rev_input_args_t;
static void linear_2bit_rev_input_worker(void *arg) {
    linear_2bit_rev_input_args_t *a = (linear_2bit_rev_input_args_t *)arg;
    int core_id = pi_core_id();
    int total_tokens = a->batch * a->seq_len;
    int tokens_per_core = (total_tokens + NUM_CORES - 1) / NUM_CORES;
    int start = core_id * tokens_per_core;
    int end = (start + tokens_per_core > total_tokens) ? total_tokens : (start + tokens_per_core);
    uint16_t ostride = (a->output_stride > 0) ? a->output_stride : a->out_features;

    for (int t = start; t < end; t++) {
        int b = t / a->seq_len;
        int t_in = t - b * a->seq_len;
        int rev_t = a->seq_len - 1 - t_in;
        const int8_t *in_ptr = a->input + (b * a->seq_len + rev_t) * a->in_features;
        int8_t *out_ptr = a->output + t * ostride;
        network_linear_2bit_int8_sequential(in_ptr, a->weights_packed, a->bias, out_ptr,
                                            a->in_features, a->out_features,
                                            a->scale_in, a->scale_w, a->scale_out);
    }
    pi_cl_team_barrier();
}
% endif  ## End BiMamba/2bit workers

// ---
// Pipelined Linear Projection (Double Buffered Async DMA)
// ---
// This function overlaps DMA with compute using ping-pong weight buffers.
// It must be called from Core 8 (cluster controller) context.
// Tiles output features and pipelines: load_next_tile || compute_current_tile

typedef struct {
    const int8_t *input;           // [batch_tokens, in_features] in L2
    const int8_t *weights;         // [out_features, in_features] in L2
    const int32_t *bias;           // [out_features] in L2 (can be NULL)
    int8_t *output;                // [batch_tokens, out_features] in L2
    int8_t *l1_ping;               // L1 ping buffer for weight tile
    int8_t *l1_pong;               // L1 pong buffer for weight tile
    int in_features;
    int out_features;
    int output_stride;             // Stride between output rows (0 = use out_features)
    int batch_tokens;
    int reverse_input;            // 1 to read input in reverse order per batch
    int seq_len;                  // Sequence length per batch (required for reverse_input)
    int output_layout;            // 0: [tokens, out_features], 1: [batch, out_features, seq_len]
    int tile_out_features;         // Output features per tile
    float scale_in, scale_w, scale_out;
#ifdef ENABLE_PERF_COUNTERS
    layer_perf_t *perf_counter;    // Performance counters (optional)
#endif
} linear_pipelined_cfg_t;

// Worker for pipelined linear - processes one tile's outputs
typedef struct {
    const int8_t *input;           // Input in L2 [batch_tokens, in_features]
    const int8_t *weights_l1;      // Weight tile in L1 [tile_out, in_features]
    const int32_t *bias;           // Full bias in L2 (indexed by out_start)
    int8_t *output;                // Output in L2 [batch_tokens, out_features]
    int in_features;
    int out_features;              // Total output features (for output stride, or use output_stride if > 0)
    int output_stride;             // Explicit stride between output rows (0 = use out_features)
    int tile_out_features;         // Features in this tile
    int out_start;                 // Starting output feature index
    int batch_tokens;
    int reverse_input;
    int seq_len;
    int output_layout;            // 0: [tokens, out_features], 1: [batch, out_features, seq_len]
    float combined_scale;
} linear_pipe_tile_args_t;

static void linear_pipe_tile_worker(void *arg) {
    linear_pipe_tile_args_t *a = (linear_pipe_tile_args_t *)arg;
    int core_id = pi_core_id();

    // Distribute tile's output features across cores
    int out_per_core = (a->tile_out_features + NUM_CORES - 1) / NUM_CORES;
    out_per_core = (out_per_core + 3) & ~3;  // Align to 4 for unrolling
    int local_start = core_id * out_per_core;
    int local_end = local_start + out_per_core;
    if (local_end > a->tile_out_features) local_end = a->tile_out_features;

    if (local_start >= a->tile_out_features) {
        pi_cl_team_barrier();
        return;
    }

    const int simd_count = a->in_features >> 2;
    const int remainder = a->in_features & 0x3;
    const int rem_base = simd_count << 2;
    const float combined_scale = a->combined_scale;
    // Use explicit output_stride if set, otherwise default to out_features
    const int row_stride = (a->output_stride > 0) ? a->output_stride : a->out_features;
    const int out_ch_stride = a->output_layout ? a->seq_len : 1;

    // Process 4 timesteps at a time
    int t = 0;
    for (; t + 4 <= a->batch_tokens; t += 4) {
        int idx0 = t + 0;
        int idx1 = t + 1;
        int idx2 = t + 2;
        int idx3 = t + 3;
        if (a->reverse_input) {
            int b0 = idx0 / a->seq_len;
            int b1 = idx1 / a->seq_len;
            int b2 = idx2 / a->seq_len;
            int b3 = idx3 / a->seq_len;
            idx0 = b0 * a->seq_len + (a->seq_len - 1 - (idx0 - b0 * a->seq_len));
            idx1 = b1 * a->seq_len + (a->seq_len - 1 - (idx1 - b1 * a->seq_len));
            idx2 = b2 * a->seq_len + (a->seq_len - 1 - (idx2 - b2 * a->seq_len));
            idx3 = b3 * a->seq_len + (a->seq_len - 1 - (idx3 - b3 * a->seq_len));
        }
        const v4s *pIn0 = (const v4s *)(a->input + idx0 * a->in_features);
        const v4s *pIn1 = (const v4s *)(a->input + idx1 * a->in_features);
        const v4s *pIn2 = (const v4s *)(a->input + idx2 * a->in_features);
        const v4s *pIn3 = (const v4s *)(a->input + idx3 * a->in_features);

        // Global output position includes out_start offset, with proper layout
        int8_t *pOut0;
        int8_t *pOut1;
        int8_t *pOut2;
        int8_t *pOut3;
        if (a->output_layout) {
            int b0 = (t + 0) / a->seq_len;
            int l0 = (t + 0) - b0 * a->seq_len;
            int b1 = (t + 1) / a->seq_len;
            int l1 = (t + 1) - b1 * a->seq_len;
            int b2 = (t + 2) / a->seq_len;
            int l2 = (t + 2) - b2 * a->seq_len;
            int b3 = (t + 3) / a->seq_len;
            int l3 = (t + 3) - b3 * a->seq_len;
            pOut0 = a->output + (b0 * a->out_features + a->out_start) * a->seq_len + l0;
            pOut1 = a->output + (b1 * a->out_features + a->out_start) * a->seq_len + l1;
            pOut2 = a->output + (b2 * a->out_features + a->out_start) * a->seq_len + l2;
            pOut3 = a->output + (b3 * a->out_features + a->out_start) * a->seq_len + l3;
        } else {
            pOut0 = a->output + (t + 0) * row_stride + a->out_start;
            pOut1 = a->output + (t + 1) * row_stride + a->out_start;
            pOut2 = a->output + (t + 2) * row_stride + a->out_start;
            pOut3 = a->output + (t + 3) * row_stride + a->out_start;
        }

        int o = local_start;
        for (; o + 4 <= local_end; o += 4) {
            // Weight pointers into L1 tile (local indexing)
            const v4s *pW0 = (const v4s *)(a->weights_l1 + (o + 0) * a->in_features);
            const v4s *pW1 = (const v4s *)(a->weights_l1 + (o + 1) * a->in_features);
            const v4s *pW2 = (const v4s *)(a->weights_l1 + (o + 2) * a->in_features);
            const v4s *pW3 = (const v4s *)(a->weights_l1 + (o + 3) * a->in_features);

            int32_t acc00 = 0, acc01 = 0, acc02 = 0, acc03 = 0;
            int32_t acc10 = 0, acc11 = 0, acc12 = 0, acc13 = 0;
            int32_t acc20 = 0, acc21 = 0, acc22 = 0, acc23 = 0;
            int32_t acc30 = 0, acc31 = 0, acc32 = 0, acc33 = 0;

            for (int k = 0; k < simd_count; k++) {
                v4s w0 = pW0[k], w1 = pW1[k], w2 = pW2[k], w3 = pW3[k];
                v4s in0 = pIn0[k], in1 = pIn1[k], in2 = pIn2[k], in3 = pIn3[k];

                acc00 = SumDotpSS(in0, w0, acc00); acc01 = SumDotpSS(in0, w1, acc01);
                acc02 = SumDotpSS(in0, w2, acc02); acc03 = SumDotpSS(in0, w3, acc03);
                acc10 = SumDotpSS(in1, w0, acc10); acc11 = SumDotpSS(in1, w1, acc11);
                acc12 = SumDotpSS(in1, w2, acc12); acc13 = SumDotpSS(in1, w3, acc13);
                acc20 = SumDotpSS(in2, w0, acc20); acc21 = SumDotpSS(in2, w1, acc21);
                acc22 = SumDotpSS(in2, w2, acc22); acc23 = SumDotpSS(in2, w3, acc23);
                acc30 = SumDotpSS(in3, w0, acc30); acc31 = SumDotpSS(in3, w1, acc31);
                acc32 = SumDotpSS(in3, w2, acc32); acc33 = SumDotpSS(in3, w3, acc33);
            }
            if (remainder) {
                const int8_t *in0_tail = (const int8_t *)(a->input + idx0 * a->in_features + rem_base);
                const int8_t *in1_tail = (const int8_t *)(a->input + idx1 * a->in_features + rem_base);
                const int8_t *in2_tail = (const int8_t *)(a->input + idx2 * a->in_features + rem_base);
                const int8_t *in3_tail = (const int8_t *)(a->input + idx3 * a->in_features + rem_base);
                const int8_t *w0_tail = (const int8_t *)(a->weights_l1 + (o + 0) * a->in_features + rem_base);
                const int8_t *w1_tail = (const int8_t *)(a->weights_l1 + (o + 1) * a->in_features + rem_base);
                const int8_t *w2_tail = (const int8_t *)(a->weights_l1 + (o + 2) * a->in_features + rem_base);
                const int8_t *w3_tail = (const int8_t *)(a->weights_l1 + (o + 3) * a->in_features + rem_base);
                for (int r = 0; r < remainder; r++) {
                    int8_t in0 = in0_tail[r], in1 = in1_tail[r], in2 = in2_tail[r], in3 = in3_tail[r];
                    int8_t w0 = w0_tail[r], w1 = w1_tail[r], w2 = w2_tail[r], w3 = w3_tail[r];
                    acc00 += (int32_t)in0 * w0; acc01 += (int32_t)in0 * w1;
                    acc02 += (int32_t)in0 * w2; acc03 += (int32_t)in0 * w3;
                    acc10 += (int32_t)in1 * w0; acc11 += (int32_t)in1 * w1;
                    acc12 += (int32_t)in1 * w2; acc13 += (int32_t)in1 * w3;
                    acc20 += (int32_t)in2 * w0; acc21 += (int32_t)in2 * w1;
                    acc22 += (int32_t)in2 * w2; acc23 += (int32_t)in2 * w3;
                    acc30 += (int32_t)in3 * w0; acc31 += (int32_t)in3 * w1;
                    acc32 += (int32_t)in3 * w2; acc33 += (int32_t)in3 * w3;
                }
            }

            // Add bias (global index = out_start + o)
            if (a->bias) {
                int global_o = a->out_start + o;
                int32_t b0 = a->bias[global_o + 0], b1 = a->bias[global_o + 1];
                int32_t b2 = a->bias[global_o + 2], b3 = a->bias[global_o + 3];
                acc00 += b0; acc01 += b1; acc02 += b2; acc03 += b3;
                acc10 += b0; acc11 += b1; acc12 += b2; acc13 += b3;
                acc20 += b0; acc21 += b1; acc22 += b2; acc23 += b3;
                acc30 += b0; acc31 += b1; acc32 += b2; acc33 += b3;
            }

            // Requantize and store
            #define RQ(acc) do { int32_t q = qround((float)(acc) * combined_scale); \
                               (acc) = (q > 127 ? 127 : (q < -128 ? -128 : q)); } while(0)
            RQ(acc00); RQ(acc01); RQ(acc02); RQ(acc03);
            RQ(acc10); RQ(acc11); RQ(acc12); RQ(acc13);
            RQ(acc20); RQ(acc21); RQ(acc22); RQ(acc23);
            RQ(acc30); RQ(acc31); RQ(acc32); RQ(acc33);
            #undef RQ

            pOut0[(o + 0) * out_ch_stride] = acc00; pOut0[(o + 1) * out_ch_stride] = acc01;
            pOut0[(o + 2) * out_ch_stride] = acc02; pOut0[(o + 3) * out_ch_stride] = acc03;
            pOut1[(o + 0) * out_ch_stride] = acc10; pOut1[(o + 1) * out_ch_stride] = acc11;
            pOut1[(o + 2) * out_ch_stride] = acc12; pOut1[(o + 3) * out_ch_stride] = acc13;
            pOut2[(o + 0) * out_ch_stride] = acc20; pOut2[(o + 1) * out_ch_stride] = acc21;
            pOut2[(o + 2) * out_ch_stride] = acc22; pOut2[(o + 3) * out_ch_stride] = acc23;
            pOut3[(o + 0) * out_ch_stride] = acc30; pOut3[(o + 1) * out_ch_stride] = acc31;
            pOut3[(o + 2) * out_ch_stride] = acc32; pOut3[(o + 3) * out_ch_stride] = acc33;
        }

        // Remainder outputs
        for (; o < local_end; o++) {
            const v4s *pW = (const v4s *)(a->weights_l1 + o * a->in_features);
            int32_t acc0 = 0, acc1 = 0, acc2 = 0, acc3 = 0;
            for (int k = 0; k < simd_count; k++) {
                v4s w = pW[k];
                acc0 = SumDotpSS(pIn0[k], w, acc0); acc1 = SumDotpSS(pIn1[k], w, acc1);
                acc2 = SumDotpSS(pIn2[k], w, acc2); acc3 = SumDotpSS(pIn3[k], w, acc3);
            }
            if (remainder) {
                const int8_t *in0_tail = (const int8_t *)(a->input + idx0 * a->in_features + rem_base);
                const int8_t *in1_tail = (const int8_t *)(a->input + idx1 * a->in_features + rem_base);
                const int8_t *in2_tail = (const int8_t *)(a->input + idx2 * a->in_features + rem_base);
                const int8_t *in3_tail = (const int8_t *)(a->input + idx3 * a->in_features + rem_base);
                const int8_t *w_tail = (const int8_t *)(a->weights_l1 + o * a->in_features + rem_base);
                for (int r = 0; r < remainder; r++) {
                    int8_t wv = w_tail[r];
                    acc0 += (int32_t)in0_tail[r] * wv;
                    acc1 += (int32_t)in1_tail[r] * wv;
                    acc2 += (int32_t)in2_tail[r] * wv;
                    acc3 += (int32_t)in3_tail[r] * wv;
                }
            }
            if (a->bias) {
                int32_t b = a->bias[a->out_start + o];
                acc0 += b; acc1 += b; acc2 += b; acc3 += b;
            }
            int32_t q0 = qround((float)acc0 * combined_scale);
            int32_t q1 = qround((float)acc1 * combined_scale);
            int32_t q2 = qround((float)acc2 * combined_scale);
            int32_t q3 = qround((float)acc3 * combined_scale);
            pOut0[o * out_ch_stride] = (int8_t)(q0 > 127 ? 127 : (q0 < -128 ? -128 : q0));
            pOut1[o * out_ch_stride] = (int8_t)(q1 > 127 ? 127 : (q1 < -128 ? -128 : q1));
            pOut2[o * out_ch_stride] = (int8_t)(q2 > 127 ? 127 : (q2 < -128 ? -128 : q2));
            pOut3[o * out_ch_stride] = (int8_t)(q3 > 127 ? 127 : (q3 < -128 ? -128 : q3));
        }
    }

    // Remainder timesteps
    for (; t < a->batch_tokens; t++) {
        int idx = t;
        if (a->reverse_input) {
            int b = idx / a->seq_len;
            idx = b * a->seq_len + (a->seq_len - 1 - (idx - b * a->seq_len));
        }
        const v4s *pIn = (const v4s *)(a->input + idx * a->in_features);
        int8_t *pOut;
        if (a->output_layout) {
            int b = t / a->seq_len;
            int l = t - b * a->seq_len;
            pOut = a->output + (b * a->out_features + a->out_start) * a->seq_len + l;
        } else {
            pOut = a->output + t * row_stride + a->out_start;
        }

        for (int o = local_start; o < local_end; o++) {
            const v4s *pW = (const v4s *)(a->weights_l1 + o * a->in_features);
            int32_t acc = 0;
            for (int k = 0; k < simd_count; k++) {
                acc = SumDotpSS(pIn[k], pW[k], acc);
            }
            if (remainder) {
                const int8_t *in_tail = (const int8_t *)(a->input + idx * a->in_features + rem_base);
                const int8_t *w_tail = (const int8_t *)(a->weights_l1 + o * a->in_features + rem_base);
                for (int r = 0; r < remainder; r++) {
                    acc += (int32_t)in_tail[r] * w_tail[r];
                }
            }
            if (a->bias) acc += a->bias[a->out_start + o];
            int32_t q = qround((float)acc * combined_scale);
            pOut[o * out_ch_stride] = (int8_t)(q > 127 ? 127 : (q < -128 ? -128 : q));
        }
    }

    pi_cl_team_barrier();
}

// Orchestrator function - called from Core 8 context
// Implements double-buffered async DMA pipeline
static void linear_pipelined_execute(linear_pipelined_cfg_t *cfg) {
    const int n_tiles = (cfg->out_features + cfg->tile_out_features - 1) / cfg->tile_out_features;
    const int tile_weight_bytes = cfg->tile_out_features * cfg->in_features;
    const float combined_scale = cfg->scale_in * cfg->scale_w / cfg->scale_out;

    // DMA descriptors for ping/pong
    pi_cl_dma_copy_t dma_ping, dma_pong;

    // Setup worker args (will be modified per tile)
    linear_pipe_tile_args_t worker_args = {
        .input = cfg->input,
        .bias = cfg->bias,
        .output = cfg->output,
        .in_features = cfg->in_features,
        .out_features = cfg->out_features,
        .output_stride = cfg->output_stride,  // For chunked output with strided rows
        .batch_tokens = cfg->batch_tokens,
        .reverse_input = cfg->reverse_input,
        .seq_len = cfg->seq_len,
        .output_layout = cfg->output_layout,
        .combined_scale = combined_scale
    };

    // --- PROLOGUE: Start DMA for first tile ---
    int out_start_0 = 0;
    int tile_out_0 = (cfg->tile_out_features < cfg->out_features) ? cfg->tile_out_features : cfg->out_features;

    dma_ping.dir = PI_CL_DMA_DIR_EXT2LOC;
    dma_ping.size = tile_out_0 * cfg->in_features;
    dma_ping.ext = (uint32_t)(cfg->weights + out_start_0 * cfg->in_features);
    dma_ping.loc = (uint32_t)cfg->l1_ping;
    dma_ping.merge = 0;
    pi_cl_dma_memcpy(&dma_ping);

    // --- MAIN PIPELINE LOOP ---
    for (int tile = 0; tile < n_tiles; tile++) {
        int out_start = tile * cfg->tile_out_features;
        int tile_out = cfg->tile_out_features;
        if (out_start + tile_out > cfg->out_features) {
            tile_out = cfg->out_features - out_start;
        }

        // Determine current buffer (ping for even tiles, pong for odd)
        int8_t *current_l1 = (tile % 2 == 0) ? cfg->l1_ping : cfg->l1_pong;
        pi_cl_dma_copy_t *current_dma = (tile % 2 == 0) ? &dma_ping : &dma_pong;

        // 1. Wait for current tile's DMA to complete
#ifdef ENABLE_PERF_COUNTERS
        if (cfg->perf_counter) perf_dma_load_start();
#endif
        pi_cl_dma_wait(current_dma);
#ifdef ENABLE_PERF_COUNTERS
        if (cfg->perf_counter) cfg->perf_counter->dma_load_cycles += perf_dma_load_end();
#endif

        // 2. Start DMA for NEXT tile (if any) into the OTHER buffer
        if (tile + 1 < n_tiles) {
            int next_out_start = (tile + 1) * cfg->tile_out_features;
            int next_tile_out = cfg->tile_out_features;
            if (next_out_start + next_tile_out > cfg->out_features) {
                next_tile_out = cfg->out_features - next_out_start;
            }

            int8_t *next_l1 = (tile % 2 == 0) ? cfg->l1_pong : cfg->l1_ping;
            pi_cl_dma_copy_t *next_dma = (tile % 2 == 0) ? &dma_pong : &dma_ping;

            next_dma->dir = PI_CL_DMA_DIR_EXT2LOC;
            next_dma->size = next_tile_out * cfg->in_features;
            next_dma->ext = (uint32_t)(cfg->weights + next_out_start * cfg->in_features);
            next_dma->loc = (uint32_t)next_l1;
            next_dma->merge = 0;
            pi_cl_dma_memcpy(next_dma);  // Async start
        }

        // 3. COMPUTE current tile while next tile DMA is in flight
        worker_args.weights_l1 = current_l1;
        worker_args.tile_out_features = tile_out;
        worker_args.out_start = out_start;

#ifdef ENABLE_PERF_COUNTERS
        if (cfg->perf_counter) perf_compute_start();
#endif
        pi_cl_team_fork(NUM_CORES, linear_pipe_tile_worker, &worker_args);
#ifdef ENABLE_PERF_COUNTERS
        if (cfg->perf_counter) cfg->perf_counter->compute_cycles += perf_compute_end();
#endif
    }
}

<%
mamba_blocks_for_helpers = [s for s in layer_specs if s.get('op') == 'mamba_wrapper']
# Extract chunking/double-buffer flag from mamba_slab_sizes if available
mamba_needs_chunking = mamba_slab_sizes.get('needs_chunking', False) if mamba_slab_sizes else False
mamba_enable_double_buffer = mamba_slab_sizes.get('enable_double_buffer', False) if mamba_slab_sizes else False
%>
% if mamba_blocks_for_helpers:
/* ---
 * Mamba L3 Streaming Helpers
 * Used by bidirectional Mamba for double-buffered weight streaming
 * --- */

// Async prefetch handle for double-buffered chunk streaming
typedef struct {
    int8_t *dst;
    void *src;
    int bytes;
    int pending;
    pi_cl_ram_req_t req;
} chunk_prefetch_t;

// Load small weights (conv1d, SSM params) from L3 to shared slab
static void mamba_stream_small_weights(
    network_cl_args_t *a,
    void *conv1d_weight_l3, int conv1d_weight_bytes,
    void *conv1d_bias_l3, int conv1d_bias_bytes,
    void *silu_lut_l3, int silu_lut_bytes,
    void *silu_gate_lut_q13_l3, int silu_gate_lut_q13_bytes,
    void *softplus_lut_l3, int softplus_lut_bytes,
    void *exp_lut_l3, int exp_lut_bytes,
    void *x_proj_l3, int x_proj_bytes,
    void *dt_proj_l3, int dt_proj_bytes,
    void *dt_proj_bias_l3, int dt_proj_bias_bytes,
    void *A_q15_l3, int A_q15_bytes,
    void *D_q15_l3, int D_q15_bytes
) {
    cl_ram_read(a->mamba_cur_conv1d_weight, conv1d_weight_l3, conv1d_weight_bytes);
    cl_ram_read(a->mamba_cur_conv1d_bias, conv1d_bias_l3, conv1d_bias_bytes);
    cl_ram_read(a->mamba_cur_silu_lut, silu_lut_l3, silu_lut_bytes);
    cl_ram_read(a->mamba_cur_silu_gate_lut_q13, silu_gate_lut_q13_l3, silu_gate_lut_q13_bytes);
    cl_ram_read(a->mamba_cur_softplus_lut, softplus_lut_l3, softplus_lut_bytes);
    cl_ram_read(a->mamba_cur_exp_lut, exp_lut_l3, exp_lut_bytes);
    cl_ram_read(a->mamba_cur_x_proj_weight, x_proj_l3, x_proj_bytes);
    cl_ram_read(a->mamba_cur_dt_proj_weight, dt_proj_l3, dt_proj_bytes);
    cl_ram_read(a->mamba_cur_dt_proj_bias_q16_16, dt_proj_bias_l3, dt_proj_bias_bytes);
    cl_ram_read(a->mamba_cur_A_q15, A_q15_l3, A_q15_bytes);
    cl_ram_read(a->mamba_cur_D_q15, D_q15_l3, D_q15_bytes);
}

// Async prefetch handle for direction-level small weights pipelining
// Allows prefetching next direction's small weights during current direction's out_proj
#define NUM_SMALL_WEIGHT_BUFFERS 11
typedef struct {
    pi_cl_ram_req_t reqs[NUM_SMALL_WEIGHT_BUFFERS];
    int bytes[NUM_SMALL_WEIGHT_BUFFERS];
    int pending;
} small_weights_prefetch_t;

// Start async prefetch of all small weights for next direction (non-blocking)
// Call this during out_proj of current direction to overlap DMA with compute
static void mamba_stream_small_weights_async_start(
    small_weights_prefetch_t *handle,
    network_cl_args_t *a,
    void *conv1d_weight_l3, int conv1d_weight_bytes,
    void *conv1d_bias_l3, int conv1d_bias_bytes,
    void *silu_lut_l3, int silu_lut_bytes,
    void *silu_gate_lut_q13_l3, int silu_gate_lut_q13_bytes,
    void *softplus_lut_l3, int softplus_lut_bytes,
    void *exp_lut_l3, int exp_lut_bytes,
    void *x_proj_l3, int x_proj_bytes,
    void *dt_proj_l3, int dt_proj_bytes,
    void *dt_proj_bias_l3, int dt_proj_bias_bytes,
    void *A_q15_l3, int A_q15_bytes,
    void *D_q15_l3, int D_q15_bytes
) {
    struct pi_device *ram_ptr = get_ram_ptr();
    handle->pending = 1;

    // Store sizes for validation
    handle->bytes[0] = conv1d_weight_bytes;
    handle->bytes[1] = conv1d_bias_bytes;
    handle->bytes[2] = silu_lut_bytes;
    handle->bytes[3] = silu_gate_lut_q13_bytes;
    handle->bytes[4] = softplus_lut_bytes;
    handle->bytes[5] = exp_lut_bytes;
    handle->bytes[6] = x_proj_bytes;
    handle->bytes[7] = dt_proj_bytes;
    handle->bytes[8] = dt_proj_bias_bytes;
    handle->bytes[9] = A_q15_bytes;
    handle->bytes[10] = D_q15_bytes;

    // Initiate all DMA transfers (non-blocking)
    // These will run in parallel with compute on the current direction
    pi_cl_ram_read(ram_ptr, (uint32_t)conv1d_weight_l3, a->mamba_cur_conv1d_weight, conv1d_weight_bytes, &handle->reqs[0]);
    pi_cl_ram_read(ram_ptr, (uint32_t)conv1d_bias_l3, a->mamba_cur_conv1d_bias, conv1d_bias_bytes, &handle->reqs[1]);
    pi_cl_ram_read(ram_ptr, (uint32_t)silu_lut_l3, a->mamba_cur_silu_lut, silu_lut_bytes, &handle->reqs[2]);
    pi_cl_ram_read(ram_ptr, (uint32_t)silu_gate_lut_q13_l3, a->mamba_cur_silu_gate_lut_q13, silu_gate_lut_q13_bytes, &handle->reqs[3]);
    pi_cl_ram_read(ram_ptr, (uint32_t)softplus_lut_l3, a->mamba_cur_softplus_lut, softplus_lut_bytes, &handle->reqs[4]);
    pi_cl_ram_read(ram_ptr, (uint32_t)exp_lut_l3, a->mamba_cur_exp_lut, exp_lut_bytes, &handle->reqs[5]);
    pi_cl_ram_read(ram_ptr, (uint32_t)x_proj_l3, a->mamba_cur_x_proj_weight, x_proj_bytes, &handle->reqs[6]);
    pi_cl_ram_read(ram_ptr, (uint32_t)dt_proj_l3, a->mamba_cur_dt_proj_weight, dt_proj_bytes, &handle->reqs[7]);
    pi_cl_ram_read(ram_ptr, (uint32_t)dt_proj_bias_l3, a->mamba_cur_dt_proj_bias_q16_16, dt_proj_bias_bytes, &handle->reqs[8]);
    pi_cl_ram_read(ram_ptr, (uint32_t)A_q15_l3, a->mamba_cur_A_q15, A_q15_bytes, &handle->reqs[9]);
    pi_cl_ram_read(ram_ptr, (uint32_t)D_q15_l3, a->mamba_cur_D_q15, D_q15_bytes, &handle->reqs[10]);
}

// Wait for all async small weight prefetches to complete (blocking)
// Call this before using the weights in the next direction
static void mamba_stream_small_weights_async_wait(small_weights_prefetch_t *handle) {
    if (!handle->pending) return;

    // Wait for all DMA transfers to complete
    for (int i = 0; i < NUM_SMALL_WEIGHT_BUFFERS; i++) {
        if (handle->bytes[i] > 0) {
            pi_cl_ram_read_wait(&handle->reqs[i]);
        }
    }
    handle->pending = 0;
}

// Load a chunk of in_proj weights from L3
static int mamba_stream_in_proj_chunk_to(
    int8_t *dst, void *in_proj_l3, int chunk_idx,
    int chunk_out_features, int total_out_features, int in_features
) {
    int chunk_start = chunk_idx * chunk_out_features;
    int chunk_end = chunk_start + chunk_out_features;
    if (chunk_end > total_out_features) chunk_end = total_out_features;
    int this_chunk_size = chunk_end - chunk_start;
    int bytes = this_chunk_size * in_features;
    void *src = (int8_t *)in_proj_l3 + chunk_start * in_features;
    cl_ram_read(dst, src, bytes);
    return bytes;
}

// Async prefetch for in_proj chunk (non-blocking)
static void mamba_stream_in_proj_chunk_async_start(
    chunk_prefetch_t *handle, int8_t *dst, void *in_proj_l3,
    int chunk_idx, int chunk_out_features, int total_out_features, int in_features
) {
    int chunk_start = chunk_idx * chunk_out_features;
    int chunk_end = chunk_start + chunk_out_features;
    if (chunk_end > total_out_features) chunk_end = total_out_features;
    int this_chunk_size = chunk_end - chunk_start;
    handle->bytes = this_chunk_size * in_features;
    handle->src = (int8_t *)in_proj_l3 + chunk_start * in_features;
    handle->dst = dst;
    handle->pending = 1;
    struct pi_device *ram_ptr = get_ram_ptr();
    pi_cl_ram_read(ram_ptr, (uint32_t)handle->src, handle->dst, handle->bytes, &handle->req);
}

// Wait for async in_proj prefetch to complete
static void mamba_stream_in_proj_chunk_async_wait(chunk_prefetch_t *handle) {
    if (handle->pending && handle->bytes > 0) {
        pi_cl_ram_read_wait(&handle->req);
        handle->pending = 0;
    }
}

// Load a chunk of out_proj weights from L3
static int mamba_stream_out_proj_chunk_to(
    int8_t *dst, void *out_proj_l3, int chunk_idx,
    int chunk_out_features, int total_out_features, int in_features
) {
    int chunk_start = chunk_idx * chunk_out_features;
    int chunk_end = chunk_start + chunk_out_features;
    if (chunk_end > total_out_features) chunk_end = total_out_features;
    int this_chunk_size = chunk_end - chunk_start;
    int bytes = this_chunk_size * in_features;
    void *src = (int8_t *)out_proj_l3 + chunk_start * in_features;
    cl_ram_read(dst, src, bytes);
    return bytes;
}
% endif

/* End Helper Functions */

typedef struct { const int8_t *input, *weights; const float *bias; float *output; uint16_t in_features, out_features; float scale_in, scale_w; } linear_fp32_args_t;
static void linear_fp32_worker(void *arg) { linear_fp32_args_t *a = (linear_fp32_args_t *)arg; network_linear_int8_to_fp32(a->input, a->weights, a->bias, a->output, a->in_features, a->out_features, a->scale_in, a->scale_w); }

static void network_cl_worker(void *arg) {} // Legacy stub

<%
mamba_blocks = [s for s in layer_specs if s.get('op') == 'mamba_wrapper']
%>
% if mamba_blocks:
/* ---
 * Mamba SSM + Flip-Add Worker Types and Functions
 * Used by bidirectional Mamba execution
 * --- */

// SSM gated args for block-parallel SSM execution
typedef struct {
    const int8_t *x_int8;           // Input [batch, seq_len, d_inner]
    int8_t *output_int8;            // Output [batch, seq_len, d_inner] (gated)
    const int8_t *xz_proj;          // Full in_proj output [batch, seq_len, 2*d_inner] for z access
    const int8_t *x_proj_weight;    // [dt_rank + 2*d_state, d_inner]
    const int8_t *dt_proj_weight;   // [d_inner, dt_rank]
    const int32_t *dt_proj_bias_q16_16;
    const int16_t *A_q15;           // [d_state, d_inner]
    const int16_t *D_q15;           // [d_inner]
    const int16_t *softplus_lut;
    const int16_t *exp_lut;
    const int16_t *silu_gate_lut_q13;
    int16_t *h_state_q15;           // Hidden state [batch, d_inner, d_state]
    int32_t *proj_all;              // [seq_len, dt_rank + 2*d_state] - all x_proj outputs
    int16_t *dt_all;                // [d_inner, seq_len] - all dt values (channel-major for scan)
    int16_t *B_all;                 // [seq_len, d_state] - B for all timesteps (unused, passed for interface compat)
    int16_t *C_all;                 // [seq_len, d_state] - C for all timesteps (unused, passed for interface compat)
    int batch, seq_len, d_inner, d_state, dt_rank;
    int32_t dt_scale_q, bc_scale_factor, output_scale_q;
    int dt_scale_shift;
    volatile unsigned int *core0_instr_out;  // For IPC measurement
} ssm_gated_args_t;

// Block-parallel SSM worker (loop inversion strategy)
static void ssm_gated_worker(void *arg) {
    ssm_gated_args_t *a = (ssm_gated_args_t *)arg;
    int core_id = pi_core_id();

    if (core_id == 0 && a->core0_instr_out != NULL) {
        pi_perf_conf(1 << PI_PERF_INSTR);
        pi_perf_reset();
        pi_perf_start();
    }

    const int BC_SHIFT = 16;
    const int OUTPUT_SHIFT = 24;
    const int simd_count = a->d_inner >> 2;
    const int proj_size = a->dt_rank + 2 * a->d_state;

    // Stage 1: x_proj for all timesteps (parallel across outputs)
    const int total_proj_outputs = a->seq_len * proj_size;
    const int proj_per_core = (total_proj_outputs + NUM_CORES - 1) / NUM_CORES;
    const int proj_start = core_id * proj_per_core;
    const int proj_end = (proj_start + proj_per_core > total_proj_outputs) ? total_proj_outputs : (proj_start + proj_per_core);

    // 4x loop unrolling: process 4 output features per timestep together
    // Reuse input vector load across all 4 outputs
    for (int t = 0; t < a->seq_len; t++) {
        const int8_t *x_t = a->x_int8 + t * a->d_inner;
        const v4s *pA = (const v4s *)x_t;
        int base_idx = t * proj_size;

        // Find which features this core should process for this timestep
        int t_proj_start = (proj_start > base_idx) ? proj_start - base_idx : 0;
        int t_proj_end = (proj_end > base_idx + proj_size) ? proj_size :
                         ((proj_end > base_idx) ? proj_end - base_idx : 0);

        if (t_proj_start >= t_proj_end) continue;  // Skip if no work for this timestep

        int f = t_proj_start;
        // Process 4 features at once with SIMD + 4x unrolling
        for (; f + 3 < t_proj_end; f += 4) {
            const int8_t *w_f0 = a->x_proj_weight + (f + 0) * a->d_inner;
            const int8_t *w_f1 = a->x_proj_weight + (f + 1) * a->d_inner;
            const int8_t *w_f2 = a->x_proj_weight + (f + 2) * a->d_inner;
            const int8_t *w_f3 = a->x_proj_weight + (f + 3) * a->d_inner;
            const v4s *pB0 = (const v4s *)w_f0;
            const v4s *pB1 = (const v4s *)w_f1;
            const v4s *pB2 = (const v4s *)w_f2;
            const v4s *pB3 = (const v4s *)w_f3;

            int32_t acc0 = 0, acc1 = 0, acc2 = 0, acc3 = 0;
            for (int i = 0; i < simd_count; i++) {
                v4s x = pA[i];  // Load once, use 4 times
                acc0 = SumDotpSS(x, pB0[i], acc0);
                acc1 = SumDotpSS(x, pB1[i], acc1);
                acc2 = SumDotpSS(x, pB2[i], acc2);
                acc3 = SumDotpSS(x, pB3[i], acc3);
            }
            // Handle remainder
            for (int k = simd_count * 4; k < a->d_inner; k++) {
                int8_t xk = x_t[k];
                acc0 += (int32_t)xk * (int32_t)w_f0[k];
                acc1 += (int32_t)xk * (int32_t)w_f1[k];
                acc2 += (int32_t)xk * (int32_t)w_f2[k];
                acc3 += (int32_t)xk * (int32_t)w_f3[k];
            }
            a->proj_all[base_idx + f + 0] = acc0;
            a->proj_all[base_idx + f + 1] = acc1;
            a->proj_all[base_idx + f + 2] = acc2;
            a->proj_all[base_idx + f + 3] = acc3;
        }
        // Handle remaining features (less than 4)
        for (; f < t_proj_end; f++) {
            const int8_t *w_f_ptr = a->x_proj_weight + f * a->d_inner;
            const v4s *pB = (const v4s *)w_f_ptr;
            int32_t acc = 0;
            for (int i = 0; i < simd_count; i++) {
                acc = SumDotpSS(pA[i], pB[i], acc);
            }
            for (int k = simd_count * 4; k < a->d_inner; k++) {
                acc += (int32_t)x_t[k] * (int32_t)w_f_ptr[k];
            }
            a->proj_all[base_idx + f] = acc;
        }
    }
    pi_cl_team_barrier();

    // Stage 1.5: Pre-extract B and C into int16 arrays (parallel across timesteps)
    // This avoids redundant int32 multiply + shift in Stage 3 inner loop
    const int bc_per_core = (a->seq_len + NUM_CORES - 1) / NUM_CORES;
    const int bc_t_start = core_id * bc_per_core;
    const int bc_t_end = (bc_t_start + bc_per_core > a->seq_len) ? a->seq_len : (bc_t_start + bc_per_core);

    for (int t = bc_t_start; t < bc_t_end; t++) {
        const int32_t *proj_t = a->proj_all + t * proj_size;
        int16_t *B_t = a->B_all + t * a->d_state;
        int16_t *C_t = a->C_all + t * a->d_state;
        for (int n = 0; n < a->d_state; n++) {
            int64_t b_scaled = ((int64_t)proj_t[a->dt_rank + n] * a->bc_scale_factor) >> BC_SHIFT;
            b_scaled = (b_scaled > 32767) ? 32767 : ((b_scaled < -32768) ? -32768 : b_scaled);
            B_t[n] = (int16_t)b_scaled;

            int64_t c_scaled = ((int64_t)proj_t[a->dt_rank + a->d_state + n] * a->bc_scale_factor) >> BC_SHIFT;
            c_scaled = (c_scaled > 32767) ? 32767 : ((c_scaled < -32768) ? -32768 : c_scaled);
            C_t[n] = (int16_t)c_scaled;
        }
    }
    pi_cl_team_barrier();

    // Stage 2: dt_proj for all timesteps (parallel across outputs)
    const int total_dt_outputs = a->seq_len * a->d_inner;
    const int dt_per_core = (total_dt_outputs + NUM_CORES - 1) / NUM_CORES;
    const int dt_start = core_id * dt_per_core;
    const int dt_end = (dt_start + dt_per_core > total_dt_outputs) ? total_dt_outputs : (dt_start + dt_per_core);

    // 4x loop unrolling: process 4 channels per timestep, reusing delta_proj
    for (int t = 0; t < a->seq_len; t++) {
        const int32_t *delta_proj = a->proj_all + t * proj_size;

        // Find which channels this core should process for this timestep
        int base_idx = t * a->d_inner;
        int t_dt_start = (dt_start > base_idx) ? dt_start - base_idx : 0;
        int t_dt_end = (dt_end > base_idx + a->d_inner) ? a->d_inner :
                       ((dt_end > base_idx) ? dt_end - base_idx : 0);

        if (t_dt_start >= t_dt_end) continue;

        int d = t_dt_start;
        // Process 4 channels at once
        for (; d + 3 < t_dt_end; d += 4) {
            const int8_t *dt_w_d0 = a->dt_proj_weight + (d + 0) * a->dt_rank;
            const int8_t *dt_w_d1 = a->dt_proj_weight + (d + 1) * a->dt_rank;
            const int8_t *dt_w_d2 = a->dt_proj_weight + (d + 2) * a->dt_rank;
            const int8_t *dt_w_d3 = a->dt_proj_weight + (d + 3) * a->dt_rank;

            int32_t acc0 = a->dt_proj_bias_q16_16[d + 0];
            int32_t acc1 = a->dt_proj_bias_q16_16[d + 1];
            int32_t acc2 = a->dt_proj_bias_q16_16[d + 2];
            int32_t acc3 = a->dt_proj_bias_q16_16[d + 3];

            for (int k = 0; k < a->dt_rank; k++) {
                int32_t delta_k = delta_proj[k];  // Load once, use 4 times
                acc0 += delta_k * (int32_t)dt_w_d0[k];
                acc1 += delta_k * (int32_t)dt_w_d1[k];
                acc2 += delta_k * (int32_t)dt_w_d2[k];
                acc3 += delta_k * (int32_t)dt_w_d3[k];
            }

            // Apply softplus via LUT for all 4 channels
            int32_t dt_raw0 = acc0 >> a->dt_scale_shift;
            int32_t dt_raw1 = acc1 >> a->dt_scale_shift;
            int32_t dt_raw2 = acc2 >> a->dt_scale_shift;
            int32_t dt_raw3 = acc3 >> a->dt_scale_shift;

            #define APPLY_SOFTPLUS(raw, result) do { \
                if ((raw) < 0) (raw) = 0; \
                if ((raw) > 32767) { (result) = 32767; } \
                else { int lut_idx = (raw) >> 7; if (lut_idx > 255) lut_idx = 255; \
                       (result) = a->softplus_lut[lut_idx]; } \
            } while(0)

            int16_t dt_val0, dt_val1, dt_val2, dt_val3;
            APPLY_SOFTPLUS(dt_raw0, dt_val0);
            APPLY_SOFTPLUS(dt_raw1, dt_val1);
            APPLY_SOFTPLUS(dt_raw2, dt_val2);
            APPLY_SOFTPLUS(dt_raw3, dt_val3);
            #undef APPLY_SOFTPLUS

            a->dt_all[(d + 0) * a->seq_len + t] = dt_val0;
            a->dt_all[(d + 1) * a->seq_len + t] = dt_val1;
            a->dt_all[(d + 2) * a->seq_len + t] = dt_val2;
            a->dt_all[(d + 3) * a->seq_len + t] = dt_val3;
        }
        // Handle remaining channels
        for (; d < t_dt_end; d++) {
            const int8_t *dt_w_d = a->dt_proj_weight + d * a->dt_rank;
            int32_t acc = a->dt_proj_bias_q16_16[d];
            for (int k = 0; k < a->dt_rank; k++) {
                acc += delta_proj[k] * (int32_t)dt_w_d[k];
            }
            int32_t dt_raw_q16 = acc >> a->dt_scale_shift;
            if (dt_raw_q16 < 0) dt_raw_q16 = 0;
            int16_t dt_val;
            if (dt_raw_q16 > 32767) {
                dt_val = 32767;
            } else {
                int lut_idx = dt_raw_q16 >> 7;
                if (lut_idx > 255) lut_idx = 255;
                dt_val = a->softplus_lut[lut_idx];
            }
            a->dt_all[d * a->seq_len + t] = dt_val;
        }
    }
    pi_cl_team_barrier();

    // Stage 3: SSM scan (parallel across channels, sequential within channel)
    const int channels_per_core = (a->d_inner + NUM_CORES - 1) / NUM_CORES;
    const int ch_start = core_id * channels_per_core;
    const int ch_end = (ch_start + channels_per_core > a->d_inner) ? a->d_inner : (ch_start + channels_per_core);

    for (int d = ch_start; d < ch_end; d++) {
        int16_t *h_d = a->h_state_q15 + d * a->d_state;
        const int16_t *dt_d = a->dt_all + d * a->seq_len;
        const int16_t D_val = a->D_q15[d];

        // Load hidden state into local registers for faster access
        int16_t h_local[16];  // Support up to d_state=16
        for (int n = 0; n < a->d_state && n < 16; n++) {
            h_local[n] = h_d[n];
        }

        for (int t = 0; t < a->seq_len; t++) {
            const int16_t *B_t = a->B_all + t * a->d_state;
            const int16_t *C_t = a->C_all + t * a->d_state;
            int16_t dt_val = dt_d[t];
            int8_t x_val = a->x_int8[t * a->d_inner + d];

            int32_t y_acc = 0;
            // Fully unrolled state update using pre-extracted B_t and C_t
            #define STATE_UPDATE(n) do { \
                int16_t B_n = B_t[n]; \
                int16_t C_n = C_t[n]; \
                int16_t a_n = a->A_q15[(n) * a->d_inner + d]; \
                int32_t dA_idx = ((int32_t)(-a_n) * (int32_t)dt_val) >> 15; \
                dA_idx = (dA_idx < 0) ? 0 : ((dA_idx > 255) ? 255 : dA_idx); \
                int16_t dA = a->exp_lut[dA_idx]; \
                int32_t dB = ((int32_t)dt_val * (int32_t)B_n) >> 15; \
                int32_t new_h = ((int32_t)dA * h_local[n]) >> 15; \
                new_h += ((int32_t)dB * x_val) >> 7; \
                new_h = (new_h > 32767) ? 32767 : ((new_h < -32768) ? -32768 : new_h); \
                h_local[n] = (int16_t)new_h; \
                y_acc += (new_h * (int32_t)C_n) >> 15; \
            } while(0)

            if (a->d_state == 16) {
                STATE_UPDATE(0);  STATE_UPDATE(1);  STATE_UPDATE(2);  STATE_UPDATE(3);
                STATE_UPDATE(4);  STATE_UPDATE(5);  STATE_UPDATE(6);  STATE_UPDATE(7);
                STATE_UPDATE(8);  STATE_UPDATE(9);  STATE_UPDATE(10); STATE_UPDATE(11);
                STATE_UPDATE(12); STATE_UPDATE(13); STATE_UPDATE(14); STATE_UPDATE(15);
            } else if (a->d_state == 4) {
                STATE_UPDATE(0); STATE_UPDATE(1); STATE_UPDATE(2); STATE_UPDATE(3);
            } else {
                for (int n = 0; n < a->d_state; n++) {
                    int16_t B_n = B_t[n];
                    int16_t C_n = C_t[n];
                    int16_t a_n = a->A_q15[n * a->d_inner + d];
                    int32_t dA_idx = ((int32_t)(-a_n) * (int32_t)dt_val) >> 15;
                    if (dA_idx < 0) dA_idx = 0;
                    if (dA_idx > 255) dA_idx = 255;
                    int16_t dA = a->exp_lut[dA_idx];
                    int32_t dB = ((int32_t)dt_val * (int32_t)B_n) >> 15;
                    int32_t new_h = ((int32_t)dA * h_local[n]) >> 15;
                    new_h += ((int32_t)dB * x_val) >> 7;
                    if (new_h > 32767) new_h = 32767;
                    if (new_h < -32768) new_h = -32768;
                    h_local[n] = (int16_t)new_h;
                    y_acc += (new_h * (int32_t)C_n) >> 15;
                }
            }
            #undef STATE_UPDATE

            y_acc += ((int32_t)D_val * x_val) >> 7;

            // Gating: y * silu(z)
            int8_t z_val = a->xz_proj[t * (2 * a->d_inner) + a->d_inner + d];
            int z_idx = (int)z_val + 128;
            if (z_idx < 0) z_idx = 0;
            if (z_idx > 255) z_idx = 255;
            int16_t silu_z = a->silu_gate_lut_q13[z_idx];
            int32_t gated = (y_acc * silu_z) >> 13;
            int32_t scaled = (gated * a->output_scale_q) >> OUTPUT_SHIFT;
            int8_t clamped = (int8_t)(scaled < -128 ? -128 : (scaled > 127 ? 127 : scaled));
            a->output_int8[t * a->d_inner + d] = clamped;
        }

        // Write back hidden state
        for (int n = 0; n < a->d_state && n < 16; n++) {
            h_d[n] = h_local[n];
        }
    }

    if (core_id == 0 && a->core0_instr_out != NULL) {
        pi_perf_stop();
        *a->core0_instr_out = pi_perf_read(PI_PERF_INSTR);
    }
    pi_cl_team_barrier();
}

// Flip-add args for combining forward + flipped reverse outputs + input residual
// May already be defined in SSM workers section above
#ifndef FLIP_ADD_DEFINED
#define FLIP_ADD_DEFINED
typedef struct {
    int8_t *output;       // In-place: fwd_out += flip(rev_out) + residual
    const int8_t *rev_out;
    const int8_t *residual; // Input residual (NULL to skip)
    int batch, seq_len, d_model;
} flip_add_args_t;

// Flip-add worker: Combines forward output with flipped reverse output and input residual
// Implements: output = fwd_out + flip(rev_out) + residual (FEMBA-style architecture)
static void flip_add_worker(void *arg) {
    flip_add_args_t *a = (flip_add_args_t *)arg;
    int core_id = pi_core_id();
    const int total_tokens = a->batch * a->seq_len;
    const int tokens_per_core = (total_tokens + NUM_CORES - 1) / NUM_CORES;
    const int start = core_id * tokens_per_core;
    const int end = (start + tokens_per_core > total_tokens) ? total_tokens : start + tokens_per_core;

    for (int t = start; t < end; t++) {
        int b = t / a->seq_len;
        int s = t % a->seq_len;
        int rev_s = a->seq_len - 1 - s;
        int out_idx = b * a->seq_len * a->d_model + s * a->d_model;
        int rev_idx = b * a->seq_len * a->d_model + rev_s * a->d_model;
        for (int d = 0; d < a->d_model; d++) {
            int32_t sum = (int32_t)a->output[out_idx + d] + (int32_t)a->rev_out[rev_idx + d];
            // Add input residual if provided (FEMBA architecture)
            if (a->residual) {
                sum += (int32_t)a->residual[out_idx + d];
            }
            if (sum > 127) sum = 127;
            if (sum < -128) sum = -128;
            a->output[out_idx + d] = (int8_t)sum;
        }
    }
    pi_cl_team_barrier();
}
#endif  // FLIP_ADD_DEFINED

// Flip input args for reversing sequence before REV direction
typedef struct {
    const int8_t *input;
    int8_t *output;
    int batch, seq_len, d_model;
} flip_input_args_t;

// Flip input worker: Reverse sequence order
static void flip_input_worker(void *arg) {
    flip_input_args_t *a = (flip_input_args_t *)arg;
    int core_id = pi_core_id();
    const int total_tokens = a->batch * a->seq_len;
    const int tokens_per_core = (total_tokens + NUM_CORES - 1) / NUM_CORES;
    const int start = core_id * tokens_per_core;
    const int end = (start + tokens_per_core > total_tokens) ? total_tokens : start + tokens_per_core;

    for (int t = start; t < end; t++) {
        int b = t / a->seq_len;
        int s = t % a->seq_len;
        int rev_s = a->seq_len - 1 - s;
        int src_idx = b * a->seq_len * a->d_model + s * a->d_model;
        int dst_idx = b * a->seq_len * a->d_model + rev_s * a->d_model;
        for (int d = 0; d < a->d_model; d++) {
            a->output[dst_idx + d] = a->input[src_idx + d];
        }
    }
    pi_cl_team_barrier();
}

// Transpose args: [B, L, 2*d_inner] x-part -> [B, d_inner, L]
typedef struct {
    const int8_t *input;
    int8_t *output;
    int batch, seq_len, d_inner;
} transpose_x_args_t;

// Transpose worker: Extract x part and transpose
static void transpose_x_worker(void *arg) {
    transpose_x_args_t *a = (transpose_x_args_t *)arg;
    int core_id = pi_core_id();
    const int total_elems = a->batch * a->d_inner * a->seq_len;
    const int elems_per_core = (total_elems + NUM_CORES - 1) / NUM_CORES;
    const int start = core_id * elems_per_core;
    const int end = (start + elems_per_core > total_elems) ? total_elems : start + elems_per_core;

    for (int idx = start; idx < end; idx++) {
        int b = idx / (a->d_inner * a->seq_len);
        int rem = idx % (a->d_inner * a->seq_len);
        int d = rem / a->seq_len;
        int t = rem % a->seq_len;
        // Input is [B, L, 2*d_inner], output is [B, d_inner, L]
        int src_idx = b * a->seq_len * 2 * a->d_inner + t * 2 * a->d_inner + d;
        int dst_idx = b * a->d_inner * a->seq_len + d * a->seq_len + t;
        a->output[dst_idx] = a->input[src_idx];
    }
    pi_cl_team_barrier();
}

// Conv1D + SiLU + Transpose args
typedef struct {
    const int8_t *input;      // [B, d_inner, L] transposed
    const int8_t *weights;    // [d_inner, kernel_size]
    const int32_t *bias;      // [d_inner]
    const int8_t *silu_lut;   // [256]
    int8_t *output;           // [B, L, d_inner] back to time-major
    int batch, channels, length, kernel_size;
    int causal;
    float scale_input, scale_weight, scale_output;
} conv1d_silu_transpose_args_t;

// Conv1D + SiLU + Transpose worker with SIMD optimization for kernel_size=4
static void conv1d_silu_transpose_worker(void *arg) {
    conv1d_silu_transpose_args_t *a = (conv1d_silu_transpose_args_t *)arg;
    int core_id = pi_core_id();

    // Each output is [B, L, d_inner] - parallelize over output elements
    const int total_outputs = a->batch * a->length * a->channels;
    const int outputs_per_core = (total_outputs + NUM_CORES - 1) / NUM_CORES;
    const int start = core_id * outputs_per_core;
    const int end = (start + outputs_per_core > total_outputs) ? total_outputs : start + outputs_per_core;

    const int32_t scale_q = (int32_t)(a->scale_input * a->scale_weight / a->scale_output * 256.0f);
    const int ks = a->kernel_size;
    const int ks_m1 = ks - 1;

    for (int idx = start; idx < end; idx++) {
        int b = idx / (a->length * a->channels);
        int rem = idx % (a->length * a->channels);
        int t = rem / a->channels;
        int d = rem % a->channels;

        // Input base for this channel: [B, d_inner, L] layout
        const int8_t *x_base = a->input + b * a->channels * a->length + d * a->length;
        const int8_t *w_d = a->weights + d * ks;

        int32_t acc = a->bias[d];

        // SIMD path for kernel_size=4 when no padding needed (t >= 3)
        if (ks == 4 && t >= ks_m1) {
            // All 4 input values are valid and contiguous
            int t_start = t - ks_m1;
            const v4s *pX = (const v4s *)(x_base + t_start);
            const v4s *pW = (const v4s *)w_d;
            acc = SumDotpSS(*pX, *pW, acc);
        } else {
            // Scalar fallback for padded region or non-4 kernel
            for (int k = 0; k < ks; k++) {
                int t_in = t - ks_m1 + k;
                int8_t x_val = (t_in >= 0 && t_in < a->length) ? x_base[t_in] : 0;
                acc += (int32_t)x_val * (int32_t)w_d[k];
            }
        }

        // Scale and apply SiLU
        int32_t scaled = (acc * scale_q) >> 8;
        scaled = (scaled < -128) ? -128 : ((scaled > 127) ? 127 : scaled);
        int8_t silu_out = a->silu_lut[(int)scaled + 128];

        // Output is [B, L, d_inner]
        a->output[b * a->length * a->channels + t * a->channels + d] = silu_out;
    }
    pi_cl_team_barrier();
}
% endif

/* ---
 * Mamba Wrapper Implementation Helper
 * Generated for bidirectional Mamba layers - handles L3 streaming and execution
 * --- */
% if mamba_blocks:
<%
    num_mamba_blocks = len(mamba_blocks)
%>
// Cross-block prefetch handle - persists across block executions
static small_weights_prefetch_t cross_block_prefetch = {0};
static int cross_block_prefetch_valid = 0;

void _execute_mamba_wrapper_impl(int layer_idx, const LayerSpec *layer,
                                         network_cl_args_t *a, layer_runtime_ctx_t *ctx) {
    // Bidirectional MambaWrapper implementation
    // Architecture: x -> fwd_mamba -> out1
    //               x -> flip -> rev_mamba -> flip -> out2
    //               final = out1 + out2

#ifndef MINIMAL_OUTPUT
    printf("CL: %s Bidirectional MambaWrapper executing...\n", layer->name);
#endif

    // Get parameters from LayerSpec
    const int B = layer->params.mamba_wrapper.batch;
    const int L = layer->params.mamba_wrapper.seq_len;
    const int d_model = layer->params.mamba_wrapper.d_model;
    const int d_inner = layer->params.mamba_wrapper.d_inner;
    const int d_state = layer->params.mamba_wrapper.d_state;
    const int dt_rank = layer->params.mamba_wrapper.dt_rank;
    const int kernel_size = layer->params.mamba_wrapper.kernel_size;
    const float scale_in = layer->params.mamba_wrapper.scale_in;
    const float scale_out = layer->params.mamba_wrapper.scale_out;

    // Calculate buffer sizes
    const int xz_proj_size = B * L * 2 * d_inner;
    const int x_trans_size = B * d_inner * L;
    const int ssm_inout_size = B * L * d_inner;
    const int proj_size = dt_rank + 2 * d_state;
    const int int8_per_dir = xz_proj_size + x_trans_size + ssm_inout_size;
    const int bidir_buf_size = B * L * d_model;

    // Scratch buffer allocation from shared Mamba scratch
    int8_t *scratch = (int8_t *)a->mamba_shared_scratch;
    if (!scratch || a->mamba_shared_scratch_size < (size_t)(int8_per_dir + bidir_buf_size)) {
        printf("CL: ERROR - %s: insufficient mamba_shared_scratch (%zu < %d)\n",
               layer->name, a->mamba_shared_scratch_size, int8_per_dir + bidir_buf_size);
        return;
    }

    // Buffer layout
    int8_t *xz_proj = scratch;
    int8_t *x_transposed = xz_proj + xz_proj_size;
    int8_t *ssm_in = x_transposed + x_trans_size;
    int8_t *ssm_out = x_transposed;  // Reuse after transpose
    int8_t *flipped_input = scratch + int8_per_dir;
    int8_t *fwd_out = ctx->output_buffer_l2;  // Forward output goes directly to output buffer

    // L1 buffer for tiled linear operations
    int8_t *l1_weights = a->l1_buffer;
    const int TILE_OUT = 32;
    const int l1_tile_size = d_inner * TILE_OUT * 2;
    if (!l1_weights || a->l1_buffer_size < (size_t)l1_tile_size) {
        l1_weights = NULL;
    }

    // Determine which Mamba block this is (0-indexed)
    int block_idx = -1;
% for idx, spec in enumerate([s for s in layer_specs if s.get('op') == 'mamba_wrapper']):
    if (layer_idx == ${layer_specs.index(spec)}) block_idx = ${idx};
% endfor

    // Get L3 weight addresses based on block index
    void *fwd_in_proj_l3 = NULL, *fwd_conv1d_l3 = NULL, *fwd_conv1d_bias_l3 = NULL;
    void *fwd_silu_lut_l3 = NULL, *fwd_gate_lut_l3 = NULL;
    void *fwd_softplus_lut_l3 = NULL, *fwd_exp_lut_l3 = NULL;
    void *fwd_x_proj_l3 = NULL, *fwd_dt_proj_l3 = NULL, *fwd_dt_bias_l3 = NULL;
    void *fwd_A_l3 = NULL, *fwd_D_l3 = NULL, *fwd_out_proj_l3 = NULL;
    void *rev_in_proj_l3 = NULL, *rev_conv1d_l3 = NULL, *rev_conv1d_bias_l3 = NULL;
    void *rev_silu_lut_l3 = NULL, *rev_gate_lut_l3 = NULL;
    void *rev_softplus_lut_l3 = NULL, *rev_exp_lut_l3 = NULL;
    void *rev_x_proj_l3 = NULL, *rev_dt_proj_l3 = NULL, *rev_dt_bias_l3 = NULL;
    void *rev_A_l3 = NULL, *rev_D_l3 = NULL, *rev_out_proj_l3 = NULL;

% for idx, spec in enumerate([s for s in layer_specs if s.get('op') == 'mamba_wrapper']):
    if (block_idx == ${idx}) {
        fwd_in_proj_l3 = a->${spec['c_name']}_fwd_in_proj_weight_l3;
        fwd_conv1d_l3 = a->${spec['c_name']}_fwd_conv1d_weight_l3;
        fwd_conv1d_bias_l3 = a->${spec['c_name']}_fwd_conv1d_bias_l3;
        fwd_silu_lut_l3 = a->${spec['c_name']}_fwd_silu_lut_l3;
        fwd_gate_lut_l3 = a->${spec['c_name']}_fwd_silu_gate_lut_q13_l3;
        fwd_softplus_lut_l3 = a->${spec['c_name']}_fwd_softplus_lut_l3;
        fwd_exp_lut_l3 = a->${spec['c_name']}_fwd_exp_lut_l3;
        fwd_x_proj_l3 = a->${spec['c_name']}_fwd_x_proj_weight_l3;
        fwd_dt_proj_l3 = a->${spec['c_name']}_fwd_dt_proj_weight_l3;
        fwd_dt_bias_l3 = a->${spec['c_name']}_fwd_dt_proj_bias_q16_16_l3;
        fwd_A_l3 = a->${spec['c_name']}_fwd_A_q15_l3;
        fwd_D_l3 = a->${spec['c_name']}_fwd_D_q15_l3;
        fwd_out_proj_l3 = a->${spec['c_name']}_fwd_out_proj_weight_l3;
        rev_in_proj_l3 = a->${spec['c_name']}_rev_in_proj_weight_l3;
        rev_conv1d_l3 = a->${spec['c_name']}_rev_conv1d_weight_l3;
        rev_conv1d_bias_l3 = a->${spec['c_name']}_rev_conv1d_bias_l3;
        rev_silu_lut_l3 = a->${spec['c_name']}_rev_silu_lut_l3;
        rev_gate_lut_l3 = a->${spec['c_name']}_rev_silu_gate_lut_q13_l3;
        rev_softplus_lut_l3 = a->${spec['c_name']}_rev_softplus_lut_l3;
        rev_exp_lut_l3 = a->${spec['c_name']}_rev_exp_lut_l3;
        rev_x_proj_l3 = a->${spec['c_name']}_rev_x_proj_weight_l3;
        rev_dt_proj_l3 = a->${spec['c_name']}_rev_dt_proj_weight_l3;
        rev_dt_bias_l3 = a->${spec['c_name']}_rev_dt_proj_bias_q16_16_l3;
        rev_A_l3 = a->${spec['c_name']}_rev_A_q15_l3;
        rev_D_l3 = a->${spec['c_name']}_rev_D_q15_l3;
        rev_out_proj_l3 = a->${spec['c_name']}_rev_out_proj_weight_l3;
    }
% endfor

#ifndef MINIMAL_OUTPUT
    printf("CL: [DEBUG] block_idx=%d, fwd_in_proj_l3=%p, fwd_conv1d_l3=%p\n",
           block_idx, fwd_in_proj_l3, fwd_conv1d_l3);
#endif

    // ========== FORWARD DIRECTION ==========
#ifndef MINIMAL_OUTPUT
    printf("CL: [FWD] Streaming small weights from L3...\n");
#endif

    // Stream small weights (conv1d, LUTs, SSM params) to shared slab
    // CROSS-BLOCK PREFETCH: Check if previous block prefetched our FWD weights
    if (cross_block_prefetch_valid) {
        // Wait for prefetch started by previous block
        mamba_stream_small_weights_async_wait(&cross_block_prefetch);
        cross_block_prefetch_valid = 0;
#ifndef MINIMAL_OUTPUT
        printf("CL: [FWD] Using cross-block prefetched weights\n");
#endif
    } else {
        // First block or no prefetch: synchronous load
        mamba_stream_small_weights(a,
            fwd_conv1d_l3, d_inner * kernel_size,
            fwd_conv1d_bias_l3, d_inner * sizeof(int32_t),
            fwd_silu_lut_l3, 256,
            fwd_gate_lut_l3, 256 * sizeof(int16_t),
            fwd_softplus_lut_l3, 256 * sizeof(int16_t),
            fwd_exp_lut_l3, 256 * sizeof(int16_t),
            fwd_x_proj_l3, proj_size * d_inner,
            fwd_dt_proj_l3, dt_rank * d_inner,
            fwd_dt_bias_l3, d_inner * sizeof(int32_t),
            fwd_A_l3, d_inner * d_state * sizeof(int16_t),
            fwd_D_l3, d_inner * sizeof(int16_t)
        );
    }

    // Input projection streaming parameters
    const int total_in_proj_out = 2 * d_inner;
% if mamba_needs_chunking:
    // CHUNKED MODE: Large model, stream in_proj weights in chunks
    const int in_proj_chunk_size = (total_in_proj_out + 14) / 15;  // ~15 chunks
    const int in_proj_num_chunks = (total_in_proj_out + in_proj_chunk_size - 1) / in_proj_chunk_size;

#ifndef MINIMAL_OUTPUT
    printf("CL: [FWD] in_proj CHUNKED: %d chunks of %d features\n", in_proj_num_chunks, in_proj_chunk_size);
#endif

% if mamba_enable_double_buffer:
    // Step 1: Input projection (chunked with double-buffering)
    {
        int8_t *ping_buf = a->mamba_cur_in_proj_weight;
        int8_t *pong_buf = a->mamba_cur_in_proj_weight_alt;
        int8_t *cur_weights = ping_buf;
        static chunk_prefetch_t prefetch_handle = {0};
        if (prefetch_handle.pending) {
            mamba_stream_in_proj_chunk_async_wait(&prefetch_handle);
        }

#ifndef MINIMAL_OUTPUT
        printf("CL: [DEBUG FWD] ping_buf=%p, fwd_in_proj_l3=%p\n", (void*)ping_buf, fwd_in_proj_l3);
#endif
        // Load first chunk synchronously
        mamba_stream_in_proj_chunk_to(ping_buf, fwd_in_proj_l3,
                                       0, in_proj_chunk_size, total_in_proj_out, d_model);

        for (int chunk = 0; chunk < in_proj_num_chunks; chunk++) {
            int chunk_start = chunk * in_proj_chunk_size;
            int chunk_end = chunk_start + in_proj_chunk_size;
            if (chunk_end > total_in_proj_out) chunk_end = total_in_proj_out;
            int this_chunk_out = chunk_end - chunk_start;

            // Start async prefetch of next chunk
            int8_t *next_buf = (cur_weights == ping_buf) ? pong_buf : ping_buf;
            if (chunk + 1 < in_proj_num_chunks) {
                mamba_stream_in_proj_chunk_async_start(&prefetch_handle, next_buf, fwd_in_proj_l3,
                                                        chunk + 1, in_proj_chunk_size, total_in_proj_out, d_model);
            }

            // Execute linear for current chunk with L2â†’L1 pipelining if available
            const int l1_tile_out = 32;  // 32 output features per L1 tile
            const int l1_tile_bytes = l1_tile_out * d_model;

            if (l1_weights && a->l1_buffer_size >= (size_t)(2 * l1_tile_bytes)) {
                // PIPELINED: Use L2â†’L1 double-buffering for maximum throughput
                linear_pipelined_cfg_t pipe_cfg = {
                    .input = ctx->input_buffer_l2,
                    .weights = cur_weights,
                    .bias = NULL,
                    .output = xz_proj + chunk_start,
                    .l1_ping = l1_weights,
                    .l1_pong = l1_weights + l1_tile_bytes,
                    .in_features = d_model,
                    .out_features = this_chunk_out,
                    .output_stride = total_in_proj_out,
                    .batch_tokens = B * L,
                    .tile_out_features = l1_tile_out,
                    .scale_in = scale_in,
                    .scale_w = scale_in * 0.05f,  // Approximate weight scale
                    .scale_out = scale_in
                };
                linear_pipelined_execute(&pipe_cfg);
            } else {
                // FALLBACK: Direct L2 compute (no L1 available)
                linear_int8_args_t proj_args = {
                    .input = ctx->input_buffer_l2,
                    .weights = cur_weights,
                    .bias = NULL,
                    .output = xz_proj + chunk_start,
                    .in_features = d_model,
                    .out_features = this_chunk_out,
                    .batch_tokens = B * L,
                    .output_stride = total_in_proj_out,
                    .seq_len = 0,
                    .output_layout = 0,
                    .scale_in = scale_in,
                    .scale_w = scale_in * 0.05f,  // Approximate weight scale
                    .scale_out = scale_in
                };
                pi_cl_team_fork(NUM_CORES, linear_int8_worker, &proj_args);
            }

            // Wait for prefetch and swap buffers
            if (chunk + 1 < in_proj_num_chunks) {
                mamba_stream_in_proj_chunk_async_wait(&prefetch_handle);
                cur_weights = next_buf;
            }
        }
    }
% else:
    // Step 1: Input projection (chunked, no double-buffering)
    {
        int8_t *weights_buf = a->mamba_cur_in_proj_weight;
        for (int chunk = 0; chunk < in_proj_num_chunks; chunk++) {
            int chunk_start = chunk * in_proj_chunk_size;
            int chunk_end = chunk_start + in_proj_chunk_size;
            if (chunk_end > total_in_proj_out) chunk_end = total_in_proj_out;
            int this_chunk_out = chunk_end - chunk_start;

            // Load current chunk synchronously
            mamba_stream_in_proj_chunk_to(weights_buf, fwd_in_proj_l3,
                                           chunk, in_proj_chunk_size, total_in_proj_out, d_model);

            // Execute linear for current chunk with L2â†’L1 pipelining if available
            const int l1_tile_out = 32;
            const int l1_tile_bytes = l1_tile_out * d_model;

            if (l1_weights && a->l1_buffer_size >= (size_t)(2 * l1_tile_bytes)) {
                linear_pipelined_cfg_t pipe_cfg = {
                    .input = ctx->input_buffer_l2,
                    .weights = weights_buf,
                    .bias = NULL,
                    .output = xz_proj + chunk_start,
                    .l1_ping = l1_weights,
                    .l1_pong = l1_weights + l1_tile_bytes,
                    .in_features = d_model,
                    .out_features = this_chunk_out,
                    .output_stride = total_in_proj_out,
                    .batch_tokens = B * L,
                    .tile_out_features = l1_tile_out,
                    .scale_in = scale_in,
                    .scale_w = scale_in * 0.05f,
                    .scale_out = scale_in
                };
                linear_pipelined_execute(&pipe_cfg);
            } else {
                linear_int8_args_t proj_args = {
                    .input = ctx->input_buffer_l2,
                    .weights = weights_buf,
                    .bias = NULL,
                    .output = xz_proj + chunk_start,
                    .in_features = d_model,
                    .out_features = this_chunk_out,
                    .batch_tokens = B * L,
                    .output_stride = total_in_proj_out,
                    .seq_len = 0,
                    .output_layout = 0,
                    .scale_in = scale_in,
                    .scale_w = scale_in * 0.05f,
                    .scale_out = scale_in
                };
                pi_cl_team_fork(NUM_CORES, linear_int8_worker, &proj_args);
            }
        }
    }
% endif
% else:
    // NON-CHUNKED MODE: Small model, load all in_proj weights at once
#ifndef MINIMAL_OUTPUT
    printf("CL: [FWD] in_proj NON-CHUNKED: loading all %d bytes\n", total_in_proj_out * d_model);
#endif

    // Step 1: Load full in_proj weights and execute linear in one go
    {
        int8_t *in_proj_buf = a->mamba_cur_in_proj_weight;
        cl_ram_read(in_proj_buf, fwd_in_proj_l3, total_in_proj_out * d_model);

        // Execute full linear projection with L2â†’L1 pipelining if available
        const int l1_tile_out = 32;
        const int l1_tile_bytes = l1_tile_out * d_model;

        if (l1_weights && a->l1_buffer_size >= (size_t)(2 * l1_tile_bytes)) {
            linear_pipelined_cfg_t pipe_cfg = {
                .input = ctx->input_buffer_l2,
                .weights = in_proj_buf,
                .bias = NULL,
                .output = xz_proj,
                .l1_ping = l1_weights,
                .l1_pong = l1_weights + l1_tile_bytes,
                .in_features = d_model,
                .out_features = total_in_proj_out,
                .output_stride = total_in_proj_out,
                .batch_tokens = B * L,
                .tile_out_features = l1_tile_out,
                .scale_in = scale_in,
                .scale_w = scale_in * 0.05f,
                .scale_out = scale_in
            };
            linear_pipelined_execute(&pipe_cfg);
        } else {
            linear_int8_args_t proj_args = {
                .input = ctx->input_buffer_l2,
                .weights = in_proj_buf,
                .bias = NULL,
                .output = xz_proj,
                .in_features = d_model,
                .out_features = total_in_proj_out,
                .batch_tokens = B * L,
                .output_stride = total_in_proj_out,
                .seq_len = 0,
                .output_layout = 0,
                .scale_in = scale_in,
                .scale_w = scale_in * 0.05f,
                .scale_out = scale_in
            };
            pi_cl_team_fork(NUM_CORES, linear_int8_worker, &proj_args);
        }
    }
% endif

#ifndef MINIMAL_OUTPUT
    printf("CL: [FWD] After in_proj, xz_proj first 8: [%d,%d,%d,%d,%d,%d,%d,%d]\n",
           xz_proj[0], xz_proj[1], xz_proj[2], xz_proj[3],
           xz_proj[4], xz_proj[5], xz_proj[6], xz_proj[7]);
#endif

    // Step 2: Transpose x from [B*L, d_inner] to [B, d_inner, L]
    {
        transpose_x_args_t trans_args = {
            .input = xz_proj,
            .output = x_transposed,
            .batch = B,
            .seq_len = L,
            .d_inner = d_inner
        };
        pi_cl_team_fork(NUM_CORES, transpose_x_worker, &trans_args);
    }

    // Step 3: Fused Conv1D + SiLU + Transpose -> ssm_in [B, L, d_inner]
    {
        conv1d_silu_transpose_args_t cst_args = {
            .input = x_transposed,
            .weights = a->mamba_cur_conv1d_weight,
            .bias = a->mamba_cur_conv1d_bias,
            .silu_lut = a->mamba_cur_silu_lut,
            .output = ssm_in,
            .batch = B,
            .channels = d_inner,
            .length = L,
            .kernel_size = kernel_size,
            .causal = 1,
            .scale_input = scale_in,
            .scale_weight = scale_in * 0.1f,  // Approximate
            .scale_output = scale_in
        };
        pi_cl_team_fork(NUM_CORES, conv1d_silu_transpose_worker, &cst_args);
    }

#ifndef MINIMAL_OUTPUT
    printf("CL: [FWD] After conv1d+silu, ssm_in first 8: [%d,%d,%d,%d,%d,%d,%d,%d]\n",
           ssm_in[0], ssm_in[1], ssm_in[2], ssm_in[3],
           ssm_in[4], ssm_in[5], ssm_in[6], ssm_in[7]);
#endif

    // Step 4: SSM with gating
    {
        // OPTIMIZATION: Reuse xz_proj's x-part for small SSM buffers
        // After transpose, x-part is free (x values copied to x_transposed, z remains for gating)
        // This saves ~95KB of L2 memory
        int16_t *h_state_q15 = (int16_t *)xz_proj;  // Reuses x-part
        int32_t *proj_all = (int32_t *)(h_state_q15 + B * d_inner * d_state);
        int16_t *B_all = (int16_t *)(proj_all + L * proj_size);
        int16_t *C_all = B_all + L * d_state;
        // dt_all uses dedicated scratch space (runs concurrently with gating)
        void *ssm_scratch = (void *)(ssm_in + ssm_inout_size);
        int16_t *dt_all = (int16_t *)ssm_scratch;

        memset(h_state_q15, 0, B * d_inner * d_state * sizeof(int16_t));

        ssm_gated_args_t ssm_args = {
            .x_int8 = ssm_in,
            .output_int8 = ssm_out,
            .xz_proj = xz_proj,  // Full in_proj output [B*L, 2*d_inner] for z access
            .x_proj_weight = a->mamba_cur_x_proj_weight,
            .dt_proj_weight = a->mamba_cur_dt_proj_weight,
            .dt_proj_bias_q16_16 = a->mamba_cur_dt_proj_bias_q16_16,
            .A_q15 = a->mamba_cur_A_q15,
            .D_q15 = a->mamba_cur_D_q15,
            .softplus_lut = a->mamba_cur_softplus_lut,
            .exp_lut = a->mamba_cur_exp_lut,
            .silu_gate_lut_q13 = a->mamba_cur_silu_gate_lut_q13,
            .h_state_q15 = h_state_q15,
            .proj_all = proj_all,
            .dt_all = dt_all,
            .B_all = B_all,
            .C_all = C_all,
            .batch = B, .seq_len = L, .d_inner = d_inner, .d_state = d_state, .dt_rank = dt_rank,
            .dt_scale_q = 1, .bc_scale_factor = 1, .output_scale_q = 1, .dt_scale_shift = 0,
            .core0_instr_out = NULL  // IPC measurement disabled
        };
        pi_cl_team_fork(NUM_CORES, ssm_gated_worker, &ssm_args);
    }

#ifndef MINIMAL_OUTPUT
    printf("CL: [FWD] After SSM+gating, ssm_out first 8: [%d,%d,%d,%d,%d,%d,%d,%d]\n",
           ssm_out[0], ssm_out[1], ssm_out[2], ssm_out[3],
           ssm_out[4], ssm_out[5], ssm_out[6], ssm_out[7]);
#endif

    // DIRECTION PIPELINING: Start async prefetch of REV small weights
    // This overlaps DMA with FWD out_proj compute
    static small_weights_prefetch_t rev_small_weights_prefetch = {0};
    mamba_stream_small_weights_async_start(&rev_small_weights_prefetch, a,
        rev_conv1d_l3, d_inner * kernel_size,
        rev_conv1d_bias_l3, d_inner * sizeof(int32_t),
        rev_silu_lut_l3, 256,
        rev_gate_lut_l3, 256 * sizeof(int16_t),
        rev_softplus_lut_l3, 256 * sizeof(int16_t),
        rev_exp_lut_l3, 256 * sizeof(int16_t),
        rev_x_proj_l3, proj_size * d_inner,
        rev_dt_proj_l3, dt_rank * d_inner,
        rev_dt_bias_l3, d_inner * sizeof(int32_t),
        rev_A_l3, d_inner * d_state * sizeof(int16_t),
        rev_D_l3, d_inner * sizeof(int16_t)
    );
#ifndef MINIMAL_OUTPUT
    printf("CL: [FWD] Started async REV small weights prefetch\n");
#endif

    // Step 5: Output projection -> fwd_out
% if mamba_needs_chunking:
    // CHUNKED MODE: Large model, use chunked streaming
    {
        const int out_proj_chunk_size = (d_model + 7) / 8;  // ~8 chunks
        const int out_proj_num_chunks = (d_model + out_proj_chunk_size - 1) / out_proj_chunk_size;
        int8_t *cur_out_weights = a->mamba_cur_out_proj_weight;

        // Load first chunk
        mamba_stream_out_proj_chunk_to(cur_out_weights, fwd_out_proj_l3,
                                        0, out_proj_chunk_size, d_model, d_inner);

        for (int chunk = 0; chunk < out_proj_num_chunks; chunk++) {
            int chunk_start = chunk * out_proj_chunk_size;
            int chunk_end = chunk_start + out_proj_chunk_size;
            if (chunk_end > d_model) chunk_end = d_model;
            int this_chunk_out = chunk_end - chunk_start;

            // Execute linear for current chunk with L2â†’L1 pipelining if available
            const int l1_tile_out_proj = 32;
            const int l1_tile_bytes_out = l1_tile_out_proj * d_inner;

            if (l1_weights && a->l1_buffer_size >= (size_t)(2 * l1_tile_bytes_out)) {
                linear_pipelined_cfg_t out_pipe_cfg = {
                    .input = ssm_out,
                    .weights = cur_out_weights,
                    .bias = NULL,
                    .output = fwd_out + chunk_start,
                    .l1_ping = l1_weights,
                    .l1_pong = l1_weights + l1_tile_bytes_out,
                    .in_features = d_inner,
                    .out_features = this_chunk_out,
                    .output_stride = d_model,
                    .batch_tokens = B * L,
                    .tile_out_features = l1_tile_out_proj,
                    .scale_in = scale_in,
                    .scale_w = scale_in * 0.02f,
                    .scale_out = scale_out
                };
                linear_pipelined_execute(&out_pipe_cfg);
            } else {
                linear_int8_args_t out_args = {
                    .input = ssm_out,
                    .weights = cur_out_weights,
                    .bias = NULL,
                    .output = fwd_out + chunk_start,
                    .in_features = d_inner,
                    .out_features = this_chunk_out,
                    .batch_tokens = B * L,
                    .output_stride = d_model,
                    .seq_len = 0,
                    .output_layout = 0,
                    .scale_in = scale_in,
                    .scale_w = scale_in * 0.02f,
                    .scale_out = scale_out
                };
                pi_cl_team_fork(NUM_CORES, linear_int8_worker, &out_args);
            }

            // Stream next chunk
            if (chunk + 1 < out_proj_num_chunks) {
                mamba_stream_out_proj_chunk_to(cur_out_weights, fwd_out_proj_l3,
                                                chunk + 1, out_proj_chunk_size, d_model, d_inner);
            }
        }
    }
% else:
    // NON-CHUNKED MODE: Small model, load all out_proj weights at once
    {
        int8_t *out_proj_buf = a->mamba_cur_out_proj_weight;
        cl_ram_read(out_proj_buf, fwd_out_proj_l3, d_model * d_inner);

        // Execute with L2â†’L1 pipelining if available
        const int l1_tile_out_proj = 32;
        const int l1_tile_bytes_out = l1_tile_out_proj * d_inner;

        if (l1_weights && a->l1_buffer_size >= (size_t)(2 * l1_tile_bytes_out)) {
            linear_pipelined_cfg_t out_pipe_cfg = {
                .input = ssm_out,
                .weights = out_proj_buf,
                .bias = NULL,
                .output = fwd_out,
                .l1_ping = l1_weights,
                .l1_pong = l1_weights + l1_tile_bytes_out,
                .in_features = d_inner,
                .out_features = d_model,
                .output_stride = d_model,
                .batch_tokens = B * L,
                .tile_out_features = l1_tile_out_proj,
                .scale_in = scale_in,
                .scale_w = scale_in * 0.02f,
                .scale_out = scale_out
            };
            linear_pipelined_execute(&out_pipe_cfg);
        } else {
            linear_int8_args_t out_args = {
                .input = ssm_out,
                .weights = out_proj_buf,
                .bias = NULL,
                .output = fwd_out,
                .in_features = d_inner,
                .out_features = d_model,
                .batch_tokens = B * L,
                .output_stride = d_model,
                .seq_len = 0,
                .output_layout = 0,
                .scale_in = scale_in,
                .scale_w = scale_in * 0.02f,
                .scale_out = scale_out
            };
            pi_cl_team_fork(NUM_CORES, linear_int8_worker, &out_args);
        }
    }
% endif

#ifndef MINIMAL_OUTPUT
    printf("CL: [FWD] Complete, output first 8: [%d,%d,%d,%d,%d,%d,%d,%d]\n",
           fwd_out[0], fwd_out[1], fwd_out[2], fwd_out[3],
           fwd_out[4], fwd_out[5], fwd_out[6], fwd_out[7]);
#endif

    // DIRECTION PIPELINING: Wait for REV small weights prefetch to complete
    // At this point, DMA should have overlapped with FWD out_proj compute
    mamba_stream_small_weights_async_wait(&rev_small_weights_prefetch);
#ifndef MINIMAL_OUTPUT
    printf("CL: [FWD] REV small weights prefetch complete (pipelined)\n");
#endif

    // ========== REVERSE DIRECTION ==========
#ifndef MINIMAL_OUTPUT
    printf("CL: [REV] Small weights already prefetched during FWD out_proj\n");
#endif

    // Step 0: Flip input sequence
    {
        flip_input_args_t flip_args = {
            .input = ctx->input_buffer_l2,
            .output = flipped_input,
            .batch = B,
            .seq_len = L,
            .d_model = d_model
        };
        pi_cl_team_fork(NUM_CORES, flip_input_worker, &flip_args);
    }

    // REV small weights already loaded via async prefetch during FWD out_proj
    // (Removed synchronous mamba_stream_small_weights call)

    // Step 1-5 for reverse direction (using flipped_input as input)
    // Output goes to flipped_input (reused as rev_out_raw after in_proj)
    int8_t *rev_out_raw = flipped_input;

    // In projection
% if mamba_needs_chunking:
% if mamba_enable_double_buffer:
    // CHUNKED MODE: Large model, use chunked streaming with double-buffering
    {
        int8_t *ping_buf = a->mamba_cur_in_proj_weight;
        int8_t *pong_buf = a->mamba_cur_in_proj_weight_alt;
        int8_t *cur_weights = ping_buf;
        static chunk_prefetch_t prefetch_handle = {0};
        if (prefetch_handle.pending) {
            mamba_stream_in_proj_chunk_async_wait(&prefetch_handle);
        }

        mamba_stream_in_proj_chunk_to(ping_buf, rev_in_proj_l3,
                                       0, in_proj_chunk_size, total_in_proj_out, d_model);

        for (int chunk = 0; chunk < in_proj_num_chunks; chunk++) {
            int chunk_start = chunk * in_proj_chunk_size;
            int chunk_end = chunk_start + in_proj_chunk_size;
            if (chunk_end > total_in_proj_out) chunk_end = total_in_proj_out;
            int this_chunk_out = chunk_end - chunk_start;

            int8_t *next_buf = (cur_weights == ping_buf) ? pong_buf : ping_buf;
            if (chunk + 1 < in_proj_num_chunks) {
                mamba_stream_in_proj_chunk_async_start(&prefetch_handle, next_buf, rev_in_proj_l3,
                                                        chunk + 1, in_proj_chunk_size, total_in_proj_out, d_model);
            }

            // Execute with L2â†’L1 pipelining if available
            const int l1_tile_out_rev = 32;
            const int l1_tile_bytes_rev = l1_tile_out_rev * d_model;

            if (l1_weights && a->l1_buffer_size >= (size_t)(2 * l1_tile_bytes_rev)) {
                linear_pipelined_cfg_t pipe_cfg_rev = {
                    .input = flipped_input,
                    .weights = cur_weights,
                    .bias = NULL,
                    .output = xz_proj + chunk_start,
                    .l1_ping = l1_weights,
                    .l1_pong = l1_weights + l1_tile_bytes_rev,
                    .in_features = d_model,
                    .out_features = this_chunk_out,
                    .output_stride = total_in_proj_out,
                    .batch_tokens = B * L,
                    .tile_out_features = l1_tile_out_rev,
                    .scale_in = scale_in,
                    .scale_w = scale_in * 0.05f,
                    .scale_out = scale_in
                };
                linear_pipelined_execute(&pipe_cfg_rev);
            } else {
                linear_int8_args_t proj_args = {
                    .input = flipped_input,
                    .weights = cur_weights,
                    .bias = NULL,
                    .output = xz_proj + chunk_start,
                    .in_features = d_model,
                    .out_features = this_chunk_out,
                    .batch_tokens = B * L,
                    .output_stride = total_in_proj_out,
                    .seq_len = 0,
                    .output_layout = 0,
                    .scale_in = scale_in,
                    .scale_w = scale_in * 0.05f,
                    .scale_out = scale_in
                };
                pi_cl_team_fork(NUM_CORES, linear_int8_worker, &proj_args);
            }

            if (chunk + 1 < in_proj_num_chunks) {
                mamba_stream_in_proj_chunk_async_wait(&prefetch_handle);
                cur_weights = next_buf;
            }
        }
    }
% else:
    // Step 1: Input projection (chunked, no double-buffering)
    {
        int8_t *weights_buf = a->mamba_cur_in_proj_weight;
        for (int chunk = 0; chunk < in_proj_num_chunks; chunk++) {
            int chunk_start = chunk * in_proj_chunk_size;
            int chunk_end = chunk_start + in_proj_chunk_size;
            if (chunk_end > total_in_proj_out) chunk_end = total_in_proj_out;
            int this_chunk_out = chunk_end - chunk_start;

            // Load current chunk synchronously
            mamba_stream_in_proj_chunk_to(weights_buf, rev_in_proj_l3,
                                           chunk, in_proj_chunk_size, total_in_proj_out, d_model);

            // Execute with L2â†’L1 pipelining if available
            const int l1_tile_out_rev = 32;
            const int l1_tile_bytes_rev = l1_tile_out_rev * d_model;

            if (l1_weights && a->l1_buffer_size >= (size_t)(2 * l1_tile_bytes_rev)) {
                linear_pipelined_cfg_t pipe_cfg_rev = {
                    .input = flipped_input,
                    .weights = weights_buf,
                    .bias = NULL,
                    .output = xz_proj + chunk_start,
                    .l1_ping = l1_weights,
                    .l1_pong = l1_weights + l1_tile_bytes_rev,
                    .in_features = d_model,
                    .out_features = this_chunk_out,
                    .output_stride = total_in_proj_out,
                    .batch_tokens = B * L,
                    .tile_out_features = l1_tile_out_rev,
                    .scale_in = scale_in,
                    .scale_w = scale_in * 0.05f,
                    .scale_out = scale_in
                };
                linear_pipelined_execute(&pipe_cfg_rev);
            } else {
                linear_int8_args_t proj_args = {
                    .input = flipped_input,
                    .weights = weights_buf,
                    .bias = NULL,
                    .output = xz_proj + chunk_start,
                    .in_features = d_model,
                    .out_features = this_chunk_out,
                    .batch_tokens = B * L,
                    .output_stride = total_in_proj_out,
                    .seq_len = 0,
                    .output_layout = 0,
                    .scale_in = scale_in,
                    .scale_w = scale_in * 0.05f,
                    .scale_out = scale_in
                };
                pi_cl_team_fork(NUM_CORES, linear_int8_worker, &proj_args);
            }
        }
    }
% endif
% else:
    // NON-CHUNKED MODE: Small model, load all in_proj weights at once
#ifndef MINIMAL_OUTPUT
    printf("CL: [REV] in_proj NON-CHUNKED: loading all %d bytes\n", total_in_proj_out * d_model);
#endif
    {
        int8_t *in_proj_buf = a->mamba_cur_in_proj_weight;
        cl_ram_read(in_proj_buf, rev_in_proj_l3, total_in_proj_out * d_model);

        // Execute with L2â†’L1 pipelining if available
        const int l1_tile_out_rev = 32;
        const int l1_tile_bytes_rev = l1_tile_out_rev * d_model;

        if (l1_weights && a->l1_buffer_size >= (size_t)(2 * l1_tile_bytes_rev)) {
            linear_pipelined_cfg_t pipe_cfg_rev = {
                .input = flipped_input,
                .weights = in_proj_buf,
                .bias = NULL,
                .output = xz_proj,
                .l1_ping = l1_weights,
                .l1_pong = l1_weights + l1_tile_bytes_rev,
                .in_features = d_model,
                .out_features = total_in_proj_out,
                .output_stride = total_in_proj_out,
                .batch_tokens = B * L,
                .tile_out_features = l1_tile_out_rev,
                .scale_in = scale_in,
                .scale_w = scale_in * 0.05f,
                .scale_out = scale_in
            };
            linear_pipelined_execute(&pipe_cfg_rev);
        } else {
            linear_int8_args_t proj_args = {
                .input = flipped_input,
                .weights = in_proj_buf,
                .bias = NULL,
                .output = xz_proj,
                .in_features = d_model,
                .out_features = total_in_proj_out,
                .batch_tokens = B * L,
                .output_stride = total_in_proj_out,
                .seq_len = 0,
                .output_layout = 0,
                .scale_in = scale_in,
                .scale_w = scale_in * 0.05f,
                .scale_out = scale_in
            };
            pi_cl_team_fork(NUM_CORES, linear_int8_worker, &proj_args);
        }
    }
% endif

    // Transpose
    {
        transpose_x_args_t trans_args = {
            .input = xz_proj,
            .output = x_transposed,
            .batch = B,
            .seq_len = L,
            .d_inner = d_inner
        };
        pi_cl_team_fork(NUM_CORES, transpose_x_worker, &trans_args);
    }

    // Conv1D + SiLU
    {
        conv1d_silu_transpose_args_t cst_args = {
            .input = x_transposed,
            .weights = a->mamba_cur_conv1d_weight,
            .bias = a->mamba_cur_conv1d_bias,
            .silu_lut = a->mamba_cur_silu_lut,
            .output = ssm_in,
            .batch = B,
            .channels = d_inner,
            .length = L,
            .kernel_size = kernel_size,
            .causal = 1,
            .scale_input = scale_in,
            .scale_weight = scale_in * 0.1f,
            .scale_output = scale_in
        };
        pi_cl_team_fork(NUM_CORES, conv1d_silu_transpose_worker, &cst_args);
    }

    // SSM with gating
    {
        // OPTIMIZATION: Reuse xz_proj's x-part for small SSM buffers
        // After transpose, x-part is free (x values copied to x_transposed, z remains for gating)
        int16_t *h_state_q15 = (int16_t *)xz_proj;  // Reuses x-part
        int32_t *proj_all = (int32_t *)(h_state_q15 + B * d_inner * d_state);
        int16_t *B_all = (int16_t *)(proj_all + L * proj_size);
        int16_t *C_all = B_all + L * d_state;
        // dt_all uses dedicated scratch space
        void *ssm_scratch = (void *)(ssm_in + ssm_inout_size);
        int16_t *dt_all = (int16_t *)ssm_scratch;

        memset(h_state_q15, 0, B * d_inner * d_state * sizeof(int16_t));

        ssm_gated_args_t ssm_args = {
            .x_int8 = ssm_in,
            .output_int8 = ssm_out,
            .xz_proj = xz_proj,  // Full in_proj output [B*L, 2*d_inner] for z access
            .x_proj_weight = a->mamba_cur_x_proj_weight,
            .dt_proj_weight = a->mamba_cur_dt_proj_weight,
            .dt_proj_bias_q16_16 = a->mamba_cur_dt_proj_bias_q16_16,
            .A_q15 = a->mamba_cur_A_q15,
            .D_q15 = a->mamba_cur_D_q15,
            .softplus_lut = a->mamba_cur_softplus_lut,
            .exp_lut = a->mamba_cur_exp_lut,
            .silu_gate_lut_q13 = a->mamba_cur_silu_gate_lut_q13,
            .h_state_q15 = h_state_q15,
            .proj_all = proj_all,
            .dt_all = dt_all,
            .B_all = B_all,
            .C_all = C_all,
            .batch = B, .seq_len = L, .d_inner = d_inner, .d_state = d_state, .dt_rank = dt_rank,
            .dt_scale_q = 1, .bc_scale_factor = 1, .output_scale_q = 1, .dt_scale_shift = 0,
            .core0_instr_out = NULL  // IPC measurement disabled
        };
        pi_cl_team_fork(NUM_CORES, ssm_gated_worker, &ssm_args);
    }

    // Output projection -> rev_out_raw (strided to avoid overwriting still-needed data)
% if mamba_needs_chunking:
    // CHUNKED MODE: Large model, use chunked streaming
    {
        const int out_proj_chunk_size = (d_model + 7) / 8;
        const int out_proj_num_chunks = (d_model + out_proj_chunk_size - 1) / out_proj_chunk_size;
        int8_t *cur_out_weights = a->mamba_cur_out_proj_weight;

        mamba_stream_out_proj_chunk_to(cur_out_weights, rev_out_proj_l3,
                                        0, out_proj_chunk_size, d_model, d_inner);

        for (int chunk = 0; chunk < out_proj_num_chunks; chunk++) {
            int chunk_start = chunk * out_proj_chunk_size;
            int chunk_end = chunk_start + out_proj_chunk_size;
            if (chunk_end > d_model) chunk_end = d_model;
            int this_chunk_out = chunk_end - chunk_start;

            // Execute with L2â†’L1 pipelining if available
            const int l1_tile_out_proj_rev = 32;
            const int l1_tile_bytes_out_rev = l1_tile_out_proj_rev * d_inner;

            if (l1_weights && a->l1_buffer_size >= (size_t)(2 * l1_tile_bytes_out_rev)) {
                linear_pipelined_cfg_t out_pipe_cfg_rev = {
                    .input = ssm_out,
                    .weights = cur_out_weights,
                    .bias = NULL,
                    .output = rev_out_raw + chunk_start,
                    .l1_ping = l1_weights,
                    .l1_pong = l1_weights + l1_tile_bytes_out_rev,
                    .in_features = d_inner,
                    .out_features = this_chunk_out,
                    .output_stride = d_model,
                    .batch_tokens = B * L,
                    .tile_out_features = l1_tile_out_proj_rev,
                    .scale_in = scale_in,
                    .scale_w = scale_in * 0.02f,
                    .scale_out = scale_out
                };
                linear_pipelined_execute(&out_pipe_cfg_rev);
            } else {
                linear_int8_args_t out_args = {
                    .input = ssm_out,
                    .weights = cur_out_weights,
                    .bias = NULL,
                    .output = rev_out_raw + chunk_start,
                    .in_features = d_inner,
                    .out_features = this_chunk_out,
                    .batch_tokens = B * L,
                    .output_stride = d_model,
                    .seq_len = 0,
                    .output_layout = 0,
                    .scale_in = scale_in,
                    .scale_w = scale_in * 0.02f,
                    .scale_out = scale_out
                };
                pi_cl_team_fork(NUM_CORES, linear_int8_worker, &out_args);
            }

            if (chunk + 1 < out_proj_num_chunks) {
                mamba_stream_out_proj_chunk_to(cur_out_weights, rev_out_proj_l3,
                                                chunk + 1, out_proj_chunk_size, d_model, d_inner);
            }
        }
    }
% else:
    // NON-CHUNKED MODE: Small model, load all out_proj weights at once
    {
        int8_t *out_proj_buf = a->mamba_cur_out_proj_weight;
        cl_ram_read(out_proj_buf, rev_out_proj_l3, d_model * d_inner);

        // Execute with L2â†’L1 pipelining if available
        const int l1_tile_out_proj_rev = 32;
        const int l1_tile_bytes_out_rev = l1_tile_out_proj_rev * d_inner;

        if (l1_weights && a->l1_buffer_size >= (size_t)(2 * l1_tile_bytes_out_rev)) {
            linear_pipelined_cfg_t out_pipe_cfg_rev = {
                .input = ssm_out,
                .weights = out_proj_buf,
                .bias = NULL,
                .output = rev_out_raw,
                .l1_ping = l1_weights,
                .l1_pong = l1_weights + l1_tile_bytes_out_rev,
                .in_features = d_inner,
                .out_features = d_model,
                .output_stride = d_model,
                .batch_tokens = B * L,
                .tile_out_features = l1_tile_out_proj_rev,
                .scale_in = scale_in,
                .scale_w = scale_in * 0.02f,
                .scale_out = scale_out
            };
            linear_pipelined_execute(&out_pipe_cfg_rev);
        } else {
            linear_int8_args_t out_args = {
                .input = ssm_out,
                .weights = out_proj_buf,
                .bias = NULL,
                .output = rev_out_raw,
                .in_features = d_inner,
                .out_features = d_model,
                .batch_tokens = B * L,
                .output_stride = d_model,
                .seq_len = 0,
                .output_layout = 0,
                .scale_in = scale_in,
                .scale_w = scale_in * 0.02f,
                .scale_out = scale_out
            };
            pi_cl_team_fork(NUM_CORES, linear_int8_worker, &out_args);
        }
    }
% endif

#ifndef MINIMAL_OUTPUT
    printf("CL: [REV] Complete, raw output first 8: [%d,%d,%d,%d,%d,%d,%d,%d]\n",
           rev_out_raw[0], rev_out_raw[1], rev_out_raw[2], rev_out_raw[3],
           rev_out_raw[4], rev_out_raw[5], rev_out_raw[6], rev_out_raw[7]);
#endif

    // ========== COMBINE: flip reverse + add to forward + input residual ==========
    // flip_add_worker adds rev_output[L-1-t] + input[t] to output[t] in place
    // FEMBA architecture: output = input + fwd_mamba(input) + rev_mamba(flip(input))
    {
        flip_add_args_t combine_args = {
            .output = fwd_out,  // In-place add: fwd_out += flip(rev_out) + residual
            .rev_out = rev_out_raw,
            .residual = ctx->input_buffer_l2,  // Add input as residual (FEMBA)
            .batch = B,
            .seq_len = L,
            .d_model = d_model
        };
        pi_cl_team_fork(NUM_CORES, flip_add_worker, &combine_args);
    }
    // fwd_out is the same as ctx->output_buffer_l2, so result is already in place

#ifndef MINIMAL_OUTPUT
    printf("CL: [BIDIR] Final output first 8: [%d,%d,%d,%d,%d,%d,%d,%d]\n",
           ctx->output_buffer_l2[0], ctx->output_buffer_l2[1],
           ctx->output_buffer_l2[2], ctx->output_buffer_l2[3],
           ctx->output_buffer_l2[4], ctx->output_buffer_l2[5],
           ctx->output_buffer_l2[6], ctx->output_buffer_l2[7]);
#endif

    // CROSS-BLOCK PREFETCH: Start prefetching next block's FWD small weights
    // This overlaps DMA with any layers between this block and the next Mamba block
% if num_mamba_blocks > 1:
% for idx, spec in enumerate([s for s in layer_specs if s.get('op') == 'mamba_wrapper']):
% if idx < num_mamba_blocks - 1:
<%
    next_spec = [s for s in layer_specs if s.get('op') == 'mamba_wrapper'][idx + 1]
%>
    if (block_idx == ${idx}) {
        // Prefetch block ${idx + 1}'s FWD small weights
        mamba_stream_small_weights_async_start(&cross_block_prefetch, a,
            a->${next_spec['c_name']}_fwd_conv1d_weight_l3, d_inner * kernel_size,
            a->${next_spec['c_name']}_fwd_conv1d_bias_l3, d_inner * sizeof(int32_t),
            a->${next_spec['c_name']}_fwd_silu_lut_l3, 256,
            a->${next_spec['c_name']}_fwd_silu_gate_lut_q13_l3, 256 * sizeof(int16_t),
            a->${next_spec['c_name']}_fwd_softplus_lut_l3, 256 * sizeof(int16_t),
            a->${next_spec['c_name']}_fwd_exp_lut_l3, 256 * sizeof(int16_t),
            a->${next_spec['c_name']}_fwd_x_proj_weight_l3, proj_size * d_inner,
            a->${next_spec['c_name']}_fwd_dt_proj_weight_l3, dt_rank * d_inner,
            a->${next_spec['c_name']}_fwd_dt_proj_bias_q16_16_l3, d_inner * sizeof(int32_t),
            a->${next_spec['c_name']}_fwd_A_q15_l3, d_inner * d_state * sizeof(int16_t),
            a->${next_spec['c_name']}_fwd_D_q15_l3, d_inner * sizeof(int16_t)
        );
        cross_block_prefetch_valid = 1;
#ifndef MINIMAL_OUTPUT
        printf("CL: [XBLK] Started async prefetch of ${next_spec['c_name']} FWD weights\n");
#endif
    }
% endif
% endfor
% endif
}
% else:
// No mamba_wrapper layers - stub function to satisfy linker
void _execute_mamba_wrapper_impl(int layer_idx, const LayerSpec *layer,
                                         network_cl_args_t *a, layer_runtime_ctx_t *ctx) {
    printf("CL: ERROR - _execute_mamba_wrapper_impl called but no Mamba layers in network\n");
}
% endif

// ---
// CrossAttentionWithSelfRefine template-generated implementation
// ---
<%
    casr_specs = [s for s in layer_specs if s.get('op') == 'cross_attn_self_refine']
%>
% if casr_specs:
% for spec in casr_specs:
void _execute_cross_attn_self_refine_impl_${loop.index}(
    int layer_idx, const LayerSpec *layer,
    network_cl_args_t *a, layer_runtime_ctx_t *ctx)
{
    const int B = layer->params.cross_attn_self_refine.batch;
    const int kv_len = layer->params.cross_attn_self_refine.kv_len;
    const int Q = layer->params.cross_attn_self_refine.num_queries;
    const int D = layer->params.cross_attn_self_refine.embed_dim;
    const int H = layer->params.cross_attn_self_refine.num_heads;
    const int Dh = layer->params.cross_attn_self_refine.head_dim;
    const int ff_dim = layer->params.cross_attn_self_refine.ff_dim;
    const int num_sa = layer->params.cross_attn_self_refine.num_sa_blocks;
    const float softmax_scale = layer->params.cross_attn_self_refine.softmax_scale;
    (void)num_sa;

<%
    _D = spec['embed_dim']
    _ff = spec['ff_dim']
    _Q = spec['num_queries']
    _norm_w_bytes = _D * 4   # sizeof(float)
    _norm_b_bytes = _D * 4
    _proj_w_bytes = _D * _D  # int8
    _proj_b_bytes = _D * 4   # sizeof(int32_t)
    _ffn1_w_bytes = _D * _ff
    _ffn1_b_bytes = _ff * 4
    _ffn2_w_bytes = _ff * _D
    _ffn2_b_bytes = _D * 4
    _qembed_bytes = _Q * _D
%>
    // Slab-streaming: reuse shared weight/bias slabs for each sub-operation
    int8_t *wslab = (int8_t *)a->block_weight_slab;
    int8_t *bslab = (int8_t *)a->block_bias_slab;

    // Input/output buffers
    int8_t *input = ctx->input_buffer_l2;
    int8_t *output = ctx->output_buffer_l2;

    // Scratch partitioning
    int8_t *scratch = a->${spec['scratch_buffer']};
    const int kv_buf_bytes = B * kv_len * D;
    const int q_buf_bytes = B * Q * D;
    const int ffn_buf_bytes = B * Q * ff_dim;

    int8_t *normed_kv = scratch;
    int8_t *k_proj = scratch + kv_buf_bytes;
    int8_t *v_proj = scratch + 2 * kv_buf_bytes;
    int8_t *context_buf = scratch + 3 * kv_buf_bytes;
    int8_t *ffn_hidden = context_buf + q_buf_bytes;
    int8_t *normed_q = ffn_hidden + ffn_buf_bytes;

    // Requantization parameters for softmax
    const int32_t requant_shift = 24;
    const int32_t requant_mul = qround(softmax_scale * (float)(1 << requant_shift));

% if spec.get('needs_patch_reshape'):
    // Per-patch transpose: input [${spec['total_input_tokens']}, D] = [kv_len, num_patches, D]
    // â†’ [num_patches, kv_len, D] where num_patches = ${spec['num_patches']}, kv_len = ${spec['kv_len']}
    {
        const int P = ${spec['num_patches']};
        const int N = kv_len;
        int8_t *transposed = normed_kv;
        for (int ch = 0; ch < N; ch++) {
            for (int p = 0; p < P; p++) {
                const int8_t *src = input + (ch * P + p) * D;
                int8_t *dst = transposed + (p * N + ch) * D;
                for (int d = 0; d < D; d++) dst[d] = src[d];
            }
        }
        for (int i = 0; i < B * kv_len * D; i++) input[i] = transposed[i];
    }
% endif

    // ---- Stage 1: LayerNorm on queries, keys, values ----
    // Load query_embed from L3, copy to normed_q
    cl_ram_read(wslab, a->${spec['query_embed_param']}_weight_l3, ${_qembed_bytes});
    for (int i = 0; i < Q * D; i++) normed_q[i] = wslab[i];

    // Norm queries
    cl_ram_read(wslab, a->${spec['queries_norm_param']}_weight_l3, ${_norm_w_bytes});
    cl_ram_read(bslab, a->${spec['queries_norm_param']}_bias_l3, ${_norm_b_bytes});
    network_layernorm_int8_integer_parallel(
        normed_q, normed_q,
        (const float *)wslab, (const float *)bslab,
        Q * D, D,
        ${spec['query_scale']}f, ${spec['queries_norm_scale_output']}f
    );

    // Norm keys
    cl_ram_read(wslab, a->${spec['keys_norm_param']}_weight_l3, ${_norm_w_bytes});
    cl_ram_read(bslab, a->${spec['keys_norm_param']}_bias_l3, ${_norm_b_bytes});
    network_layernorm_int8_integer_parallel(
        input, normed_kv,
        (const float *)wslab, (const float *)bslab,
        B * kv_len * D, D,
        ${spec['scale_input']}f, ${spec['keys_norm_scale_output']}f
    );
    // K projection
    cl_ram_read(wslab, a->${spec['k_param']}_weight_l3, ${_proj_w_bytes});
    cl_ram_read(bslab, a->${spec['k_param']}_bias_l3, ${_proj_b_bytes});
    network_linear_int8_parallel_tokens(
        normed_kv, (const int8_t *)wslab, (const int32_t *)bslab,
        k_proj, B * kv_len, D, D,
        ${spec['keys_norm_scale_output']}f, ${spec['k_scale_weight']}f, ${spec['k_scale_output']}f
    );

    // Norm values (reuse normed_kv buffer)
    cl_ram_read(wslab, a->${spec['values_norm_param']}_weight_l3, ${_norm_w_bytes});
    cl_ram_read(bslab, a->${spec['values_norm_param']}_bias_l3, ${_norm_b_bytes});
    network_layernorm_int8_integer_parallel(
        input, normed_kv,
        (const float *)wslab, (const float *)bslab,
        B * kv_len * D, D,
        ${spec['scale_input']}f, ${spec['values_norm_scale_output']}f
    );
    // V projection
    cl_ram_read(wslab, a->${spec['v_param']}_weight_l3, ${_proj_w_bytes});
    cl_ram_read(bslab, a->${spec['v_param']}_bias_l3, ${_proj_b_bytes});
    network_linear_int8_parallel_tokens(
        normed_kv, (const int8_t *)wslab, (const int32_t *)bslab,
        v_proj, B * kv_len, D, D,
        ${spec['values_norm_scale_output']}f, ${spec['v_scale_weight']}f, ${spec['v_scale_output']}f
    );

    // ---- Stage 2: Q projection (constant across batch) ----
    cl_ram_read(wslab, a->${spec['q_param']}_weight_l3, ${_proj_w_bytes});
    cl_ram_read(bslab, a->${spec['q_param']}_bias_l3, ${_proj_b_bytes});
    int8_t *q_proj = normed_q;
    network_linear_int8_parallel_tokens(
        normed_q, (const int8_t *)wslab, (const int32_t *)bslab,
        q_proj, Q, D, D,
        ${spec['queries_norm_scale_output']}f, ${spec['q_scale_weight']}f, ${spec['q_scale_output']}f
    );

    // ---- Stage 3: Attention core (softmax + AV + out projection) ----
    cl_ram_read(wslab, a->${spec['out_param']}_weight_l3, ${_proj_w_bytes});
    cl_ram_read(bslab, a->${spec['out_param']}_bias_l3, ${_proj_b_bytes});
    network_attention_core_int8(
        q_proj, k_proj, v_proj,
        (const int8_t *)wslab, (const int32_t *)bslab,
        output, context_buf,
        (uint32_t)B, (uint32_t)Q, (uint32_t)kv_len,
        (uint32_t)D, (uint32_t)H,
        ${spec['q_scale_output']}f, ${spec['k_scale_output']}f, ${spec['v_scale_output']}f,
        ${spec['out_scale_weight']}f, ${spec['out_scale_output']}f,
        requant_mul, requant_shift,
        1,  // q_shared_across_batch
        ctx->l1_buffer, ctx->l1_buffer_size
    );

    // ---- Stage 4: FFN + residual ----
    cl_ram_read(wslab, a->${spec['ffn_fc1_param']}_weight_l3, ${_ffn1_w_bytes});
    cl_ram_read(bslab, a->${spec['ffn_fc1_param']}_bias_l3, ${_ffn1_b_bytes});
    network_linear_int8_parallel_tokens(
        output, (const int8_t *)wslab, (const int32_t *)bslab,
        ffn_hidden, B * Q, D, ff_dim,
        ${spec['out_scale_output']}f, ${spec['ffn_fc1_scale_weight']}f, ${spec['ffn_gelu_scale']}f
    );
    network_gelu_int8_lut_inplace_parallel(ffn_hidden, B * Q * ff_dim,
                                            ${spec['ffn_gelu_scale']}f, ${spec['ffn_gelu_scale']}f);
    cl_ram_read(wslab, a->${spec['ffn_fc2_param']}_weight_l3, ${_ffn2_w_bytes});
    cl_ram_read(bslab, a->${spec['ffn_fc2_param']}_bias_l3, ${_ffn2_b_bytes});
    network_linear_int8_parallel_tokens(
        ffn_hidden, (const int8_t *)wslab, (const int32_t *)bslab,
        context_buf, B * Q, ff_dim, D,
        ${spec['ffn_gelu_scale']}f, ${spec['ffn_fc2_scale_weight']}f, ${spec['ffn_add_scale']}f
    );
    // Residual add: context_buf (ffn_out) + output (cross_out) â†’ output
    network_add_int8(context_buf, output, output, B * Q * D,
                     ${spec['ffn_add_scale']}f, ${spec['out_scale_output']}f, ${spec['ffn_add_scale']}f);

    // ---- Stage 5: Self-attention refinement blocks ----
    int8_t *current = output;
    int8_t *residual_buf = normed_kv;
    int8_t *sa_normed = k_proj;
    int8_t *sa_q = v_proj;
    int8_t *sa_k = normed_kv + q_buf_bytes;
    int8_t *sa_v = sa_k + q_buf_bytes;
    int8_t *sa_ctx = context_buf;

% for sa_idx, sa in enumerate(spec.get('sa_blocks', [])):
    // --- Self-attention block ${sa_idx} ---
    {
        for (int i = 0; i < B * Q * D; i++) residual_buf[i] = current[i];

        // Norm1
        cl_ram_read(wslab, a->${sa['norm1_param']}_weight_l3, ${_norm_w_bytes});
        cl_ram_read(bslab, a->${sa['norm1_param']}_bias_l3, ${_norm_b_bytes});
        network_layernorm_int8_integer_parallel(
            current, sa_normed,
            (const float *)wslab, (const float *)bslab,
            B * Q * D, D,
  % if sa_idx == 0:
            ${spec['ffn_add_scale']}f, ${sa['norm1_scale_output']}f
  % else:
            ${spec['sa_blocks'][sa_idx - 1]['add2_scale']}f, ${sa['norm1_scale_output']}f
  % endif
        );

        // Q/K/V projections
        cl_ram_read(wslab, a->${sa['q_param']}_weight_l3, ${_proj_w_bytes});
        cl_ram_read(bslab, a->${sa['q_param']}_bias_l3, ${_proj_b_bytes});
        network_linear_int8_parallel_tokens(
            sa_normed, (const int8_t *)wslab, (const int32_t *)bslab,
            sa_q, B * Q, D, D,
            ${sa['norm1_scale_output']}f, ${sa['q_scale_weight']}f, ${sa['q_scale_output']}f
        );
        cl_ram_read(wslab, a->${sa['k_param']}_weight_l3, ${_proj_w_bytes});
        cl_ram_read(bslab, a->${sa['k_param']}_bias_l3, ${_proj_b_bytes});
        network_linear_int8_parallel_tokens(
            sa_normed, (const int8_t *)wslab, (const int32_t *)bslab,
            sa_k, B * Q, D, D,
            ${sa['norm1_scale_output']}f, ${sa['k_scale_weight']}f, ${sa['k_scale_output']}f
        );
        cl_ram_read(wslab, a->${sa['v_param']}_weight_l3, ${_proj_w_bytes});
        cl_ram_read(bslab, a->${sa['v_param']}_bias_l3, ${_proj_b_bytes});
        network_linear_int8_parallel_tokens(
            sa_normed, (const int8_t *)wslab, (const int32_t *)bslab,
            sa_v, B * Q, D, D,
            ${sa['norm1_scale_output']}f, ${sa['v_scale_weight']}f, ${sa['v_scale_output']}f
        );

        // Self-attention core
        cl_ram_read(wslab, a->${sa['out_param']}_weight_l3, ${_proj_w_bytes});
        cl_ram_read(bslab, a->${sa['out_param']}_bias_l3, ${_proj_b_bytes});
        network_attention_core_int8(
            sa_q, sa_k, sa_v,
            (const int8_t *)wslab, (const int32_t *)bslab,
            current, sa_ctx,
            (uint32_t)B, (uint32_t)Q, (uint32_t)Q,
            (uint32_t)D, (uint32_t)H,
            ${sa['q_scale_output']}f, ${sa['k_scale_output']}f, ${sa['v_scale_output']}f,
            ${sa['out_scale_weight']}f, ${sa['out_scale_output']}f,
            requant_mul, requant_shift,
            0,  // not shared across batch
            ctx->l1_buffer, ctx->l1_buffer_size
        );

        // Residual add 1
        network_add_int8(current, residual_buf, current, B * Q * D,
                         ${sa['out_scale_output']}f,
  % if sa_idx == 0:
                         ${spec['ffn_add_scale']}f,
  % else:
                         ${spec['sa_blocks'][sa_idx - 1]['add2_scale']}f,
  % endif
                         ${sa['add1_scale']}f);

        for (int i = 0; i < B * Q * D; i++) residual_buf[i] = current[i];

        // Norm2
        cl_ram_read(wslab, a->${sa['norm2_param']}_weight_l3, ${_norm_w_bytes});
        cl_ram_read(bslab, a->${sa['norm2_param']}_bias_l3, ${_norm_b_bytes});
        network_layernorm_int8_integer_parallel(
            current, sa_normed,
            (const float *)wslab, (const float *)bslab,
            B * Q * D, D,
            ${sa['add1_scale']}f, ${sa['norm2_scale_output']}f
        );

        // MLP: FC1 â†’ GELU â†’ FC2
        cl_ram_read(wslab, a->${sa['mlp_fc1_param']}_weight_l3, ${_ffn1_w_bytes});
        cl_ram_read(bslab, a->${sa['mlp_fc1_param']}_bias_l3, ${_ffn1_b_bytes});
        network_linear_int8_parallel_tokens(
            sa_normed, (const int8_t *)wslab, (const int32_t *)bslab,
            ffn_hidden, B * Q, D, ff_dim,
            ${sa['norm2_scale_output']}f, ${sa['mlp_fc1_scale_weight']}f, ${sa['mlp_gelu_scale']}f
        );
        network_gelu_int8_lut_inplace_parallel(ffn_hidden, B * Q * ff_dim,
                                                ${sa['mlp_gelu_scale']}f, ${sa['mlp_gelu_scale']}f);
        cl_ram_read(wslab, a->${sa['mlp_fc2_param']}_weight_l3, ${_ffn2_w_bytes});
        cl_ram_read(bslab, a->${sa['mlp_fc2_param']}_bias_l3, ${_ffn2_b_bytes});
        network_linear_int8_parallel_tokens(
            ffn_hidden, (const int8_t *)wslab, (const int32_t *)bslab,
            sa_ctx, B * Q, ff_dim, D,
            ${sa['mlp_gelu_scale']}f, ${sa['mlp_fc2_scale_weight']}f, ${sa['add2_scale']}f
        );

        // Residual add 2
        network_add_int8(sa_ctx, residual_buf, current, B * Q * D,
                         ${sa['add2_scale']}f, ${sa['add1_scale']}f, ${sa['add2_scale']}f);
    }
% endfor
}
% endfor
% endif

// ---
// ClassificationHeadWithMLP template-generated implementation
// ---
<%
    cls_specs = [s for s in layer_specs if s.get('op') == 'classification_head_mlp']
%>
% if cls_specs:
% for spec in cls_specs:
void _execute_classification_head_impl_${loop.index}(
    int layer_idx, const LayerSpec *layer,
    network_cl_args_t *a, layer_runtime_ctx_t *ctx)
{
    const int B = layer->params.classification_head_mlp.batch;
    const int S = layer->params.classification_head_mlp.seq_len;
    const int D = layer->params.classification_head_mlp.hidden_dim;
    const int H = layer->params.classification_head_mlp.num_heads;
    const int mlp_dim = layer->params.classification_head_mlp.mlp_hidden_dim;
    const int C = layer->params.classification_head_mlp.num_classes;
    const float softmax_scale = layer->params.classification_head_mlp.softmax_scale;

<%
    _cD = spec['hidden_dim']
    _cmlp = spec['mlp_hidden_dim']
    _cC = spec['num_classes']
    _c_proj_w = _cD * _cD
    _c_proj_b = _cD * 4
    _c_fc1_w = _cD * _cmlp
    _c_fc1_b = _cmlp * 4
    _c_fc2_w = _cmlp * _cC
    _c_fc2_b = _cC * 4
    _c_agg = _cD
%>
    // Slab-streaming: reuse shared weight/bias slabs for each sub-operation
    int8_t *wslab = (int8_t *)a->block_weight_slab;
    int8_t *bslab = (int8_t *)a->block_bias_slab;

    int8_t *input = ctx->input_buffer_l2;
    int8_t *output = ctx->output_buffer_l2;

    // Scratch partitioning
    int8_t *scratch = a->${spec['scratch_buffer']};
    int8_t *q_proj = scratch;                              // [B, 1, D]
    int8_t *k_proj = q_proj + B * D;                       // [B, S, D]
    int8_t *v_proj = k_proj + B * S * D;                   // [B, S, D]
    int8_t *context_buf = v_proj + B * S * D;              // [B, 1, D]
    int8_t *mlp_hidden = context_buf + B * D;              // [B, 1, mlp_dim]

    // Requantization parameters for softmax
    const int32_t requant_shift = 24;
    const int32_t requant_mul = qround(softmax_scale * (float)(1 << requant_shift));

    // ---- Stage 1: Cross-attention pooling ----
    // Load learned_agg from L3 into context_buf (temporary, ${_c_agg} bytes)
    cl_ram_read(context_buf, a->${spec['learned_agg_param']}_weight_l3, ${_c_agg});

    // Q projection: learned_agg â†’ q_proj
    cl_ram_read(wslab, a->${spec['q_param']}_weight_l3, ${_c_proj_w});
    cl_ram_read(bslab, a->${spec['q_param']}_bias_l3, ${_c_proj_b});
    network_linear_int8_parallel_tokens(
        context_buf, (const int8_t *)wslab, (const int32_t *)bslab,
        q_proj, 1, D, D,
        ${spec['agg_scale']}f, ${spec['q_scale_weight']}f, ${spec['q_scale_output']}f
    );

    // K from input sequence [B*S, D]
    cl_ram_read(wslab, a->${spec['k_param']}_weight_l3, ${_c_proj_w});
    cl_ram_read(bslab, a->${spec['k_param']}_bias_l3, ${_c_proj_b});
    network_linear_int8_parallel_tokens(
        input, (const int8_t *)wslab, (const int32_t *)bslab,
        k_proj, B * S, D, D,
        ${spec['scale_input']}f, ${spec['k_scale_weight']}f, ${spec['k_scale_output']}f
    );
    // V from input sequence
    cl_ram_read(wslab, a->${spec['v_param']}_weight_l3, ${_c_proj_w});
    cl_ram_read(bslab, a->${spec['v_param']}_bias_l3, ${_c_proj_b});
    network_linear_int8_parallel_tokens(
        input, (const int8_t *)wslab, (const int32_t *)bslab,
        v_proj, B * S, D, D,
        ${spec['scale_input']}f, ${spec['v_scale_weight']}f, ${spec['v_scale_output']}f
    );

    // Attention core: 1 query attends to S keys/values per batch
    cl_ram_read(wslab, a->${spec['out_param']}_weight_l3, ${_c_proj_w});
    cl_ram_read(bslab, a->${spec['out_param']}_bias_l3, ${_c_proj_b});
    int8_t *attn_out = q_proj;  // Reuse q_proj buffer for output [B, 1, D]
    network_attention_core_int8(
        q_proj, k_proj, v_proj,
        (const int8_t *)wslab, (const int32_t *)bslab,
        attn_out, context_buf,
        (uint32_t)B, 1, (uint32_t)S,
        (uint32_t)D, (uint32_t)H,
        ${spec['q_scale_output']}f, ${spec['k_scale_output']}f, ${spec['v_scale_output']}f,
        ${spec['out_scale_weight']}f, ${spec['out_scale_output']}f,
        requant_mul, requant_shift,
        1,  // q_shared_across_batch
        ctx->l1_buffer, ctx->l1_buffer_size
    );

    // ---- Stage 2: MLP classifier ----
    // FC1: [B, D] â†’ [B, mlp_dim]
    cl_ram_read(wslab, a->${spec['mlp_fc1_param']}_weight_l3, ${_c_fc1_w});
    cl_ram_read(bslab, a->${spec['mlp_fc1_param']}_bias_l3, ${_c_fc1_b});
    network_linear_int8_parallel_tokens(
        attn_out, (const int8_t *)wslab, (const int32_t *)bslab,
        mlp_hidden, B, D, mlp_dim,
        ${spec['out_scale_output']}f, ${spec['mlp_fc1_scale_weight']}f, ${spec['mlp_gelu_scale']}f
    );
    // GELU in-place
    network_gelu_int8_lut_inplace_parallel(mlp_hidden, B * mlp_dim,
                                            ${spec['mlp_gelu_scale']}f, ${spec['mlp_gelu_scale']}f);
    // FC2: [B, mlp_dim] â†’ [B, num_classes]
    cl_ram_read(wslab, a->${spec['mlp_fc2_param']}_weight_l3, ${_c_fc2_w});
    cl_ram_read(bslab, a->${spec['mlp_fc2_param']}_bias_l3, ${_c_fc2_b});
    network_linear_int8_parallel_tokens(
        mlp_hidden, (const int8_t *)wslab, (const int32_t *)bslab,
        output, B, mlp_dim, C,
        ${spec['mlp_gelu_scale']}f, ${spec['mlp_fc2_scale_weight']}f, ${spec['scale_output']}f
    );
}
% endfor
% endif

/* End Worker Functions */

// Forward declaration for mamba wrapper (defined in network_workers.c)
extern void _execute_mamba_wrapper_impl(int layer_idx, const LayerSpec *layer,
                                        network_cl_args_t *a, layer_runtime_ctx_t *ctx);

// Forward declarations for LUNA composite block wrappers (defined in network_workers.c)
<%
    _casr_fwd = [s for s in layer_specs if s.get('op') == 'cross_attn_self_refine']
    _cls_fwd = [s for s in layer_specs if s.get('op') == 'classification_head_mlp']
%>
% for idx, _ in enumerate(_casr_fwd):
extern void _execute_cross_attn_self_refine_impl_${idx}(int layer_idx, const LayerSpec *layer,
                                        network_cl_args_t *a, layer_runtime_ctx_t *ctx);
% endfor
% for idx, _ in enumerate(_cls_fwd):
extern void _execute_classification_head_impl_${idx}(int layer_idx, const LayerSpec *layer,
                                        network_cl_args_t *a, layer_runtime_ctx_t *ctx);
% endfor

/* ---
 * Cluster Entry Point
 * Called by the Fabric Controller to run network inference on the cluster.
 * --- */

static void network_cl_entry(void *arg)
{
    network_cl_args_t *a = (network_cl_args_t *)arg;
#ifndef MINIMAL_OUTPUT
    printf("CL: Running ${network_name} INT8 inference\n");
#endif

    if (pi_core_id() == CL_ORCHESTRATOR_CORE_ID) {
        #if NUM_CORES != 8 && !defined(MINIMAL_OUTPUT)
        printf("WARNING: NUM_CORES=%d, expected 8 for GAP9 cluster\n", NUM_CORES);
        #endif
    }

    // Always initialize the cycle counter for total execution timing
    pi_perf_conf((1 << PI_PERF_CYCLES));
    pi_perf_reset();
    pi_perf_start();

#ifdef ENABLE_PERF_COUNTERS
    // Initialize performance counters for detailed cycle breakdown
    perf_counter_init();
#ifndef MINIMAL_OUTPUT
    printf("CL: Performance counters ENABLED\n");
#endif
#endif

    uint32_t cycles_start = pi_perf_read(PI_PERF_CYCLES);
    int alloc_fail = 0;
    float *golden_fp32 = NULL;

    // ---
    // Memory Arena Allocation
    // ---
% if l2_arena_size > 0:
#ifndef MINIMAL_OUTPUT
    printf("CL: Allocating L2 Arena: %d bytes\n", ${l2_arena_size});
#endif
    a->l2_arena = pi_l2_malloc(${l2_arena_size});
    if (!a->l2_arena) {
        printf("CL ERR: Failed to allocate L2 Arena\n");
        a->status = -1;
        return;
    }
% else:
    a->l2_arena = NULL;
% endif

<%
    # Count L3 fallback buffers and calculate max size
    has_l3_fallback = any(buf.get('use_l3_fallback', False) for buf in activation_buffers) or \
                      any(buf.get('use_l3_fallback', False) for buf in shared_activation_pool)

    # Calculate max staging buffer size needed (largest L3 fallback buffer)
    l3_fallback_sizes = []
    for buf in activation_buffers:
        if buf.get('use_l3_fallback', False):
            elem_size = 1 if buf['ctype'] in ['int8_t', 'uint8_t'] else (4 if buf['ctype'] in ['int32_t', 'float'] else 1)
            l3_fallback_sizes.append(buf['numel'] * elem_size)
    for pool_buf in shared_activation_pool:
        if pool_buf.get('use_l3_fallback', False):
            elem_size = 1 if pool_buf['ctype'] in ['int8_t', 'uint8_t'] else (4 if pool_buf['ctype'] in ['int32_t', 'float'] else 1)
            l3_fallback_sizes.append(pool_buf['numel'] * elem_size)

    # Calculate max but cap at 160KB to leave room for shared weight slab
    # For large activations (>160KB), operations must use tiled/chunked streaming
    raw_max_l3_staging_size = max(l3_fallback_sizes) if l3_fallback_sizes else 65536
    MAX_STAGING_CAP = 160 * 1024  # 160KB cap - must accommodate LayerNorm (153600 bytes)
    max_l3_staging_size = min(raw_max_l3_staging_size, MAX_STAGING_CAP)
    staging_capped = raw_max_l3_staging_size > MAX_STAGING_CAP
%>
% if has_l3_fallback:
% if staging_capped:
    // WARNING: Staging buffer capped at ${max_l3_staging_size} bytes (max needed: ${raw_max_l3_staging_size})
    // Operations with larger outputs MUST use tiled streaming to L3
% endif
    // Shared L3 staging buffer - ${max_l3_staging_size} bytes
    a->l3_staging_buffer = (int8_t *)pi_l2_malloc(${max_l3_staging_size});
    if (!a->l3_staging_buffer) {
        printf("CL ERR: Failed to allocate L3 staging buffer (${max_l3_staging_size} bytes)\n");
        alloc_fail = 1;
    }
% endif

    // Map Activation Buffers (Arena or Fallback)
% for buf in activation_buffers:
%   if buf.get('use_l3_fallback', False):
    // L3 Fallback: Buffer is in L3 (handled by FC), use shared staging buffer
    a->${buf['c_name']}_staging = a->l3_staging_buffer;
    a->${buf['c_name']} = a->${buf['c_name']}_staging; // Point to staging buffer
%   elif 'offset' in buf:
    // Mapped to Arena: Pointer arithmetic
    a->${buf['c_name']} = (${buf['ctype']} *)((uintptr_t)a->l2_arena + ${buf['offset']});
%   else:
    // Fallback: Buffer wasn't in plan (shouldn't happen usually), malloc it
    a->${buf['c_name']} = (${buf['ctype']} *)pi_l2_malloc(${buf['numel']} * sizeof(${buf['ctype']}));
    if (!a->${buf['c_name']}) alloc_fail = 1;
%   endif
% endfor

    // Map Shared Pools (Arena or Fallback)
% for pool_buf in shared_activation_pool:
%   if pool_buf.get('use_l3_fallback', False):
    // L3 Fallback: Use shared staging buffer
    a->${pool_buf['c_name']}_staging = a->l3_staging_buffer;
    a->${pool_buf['c_name']} = NULL;
%   elif 'offset' in pool_buf:
    a->${pool_buf['c_name']} = (${pool_buf['ctype']} *)((uintptr_t)a->l2_arena + ${pool_buf['offset']});
%   else:
    a->${pool_buf['c_name']} = (${pool_buf['ctype']} *)pi_l2_malloc(${pool_buf['numel']} * sizeof(${pool_buf['ctype']}));
    if (!a->${pool_buf['c_name']}) alloc_fail = 1;
%   endif
% endfor

    // Setup Aliases
% for block_id, block_bufs in sorted(block_activation_buffers.items()):
% for buf in block_bufs:
    a->${buf['c_name']} = a->${buf['pool_c_name']};
% endfor
% endfor

    if (alloc_fail) {
        printf("CL ERR: L2 activation allocation failed\n");
        a->status = -1;
        goto cleanup;
    }

    // Allocate Parameters (Weights/Biases) - These remain individual mallocs
<%
    non_block_params = []
    shared_slab_params = []  # Large non-block layers that use shared slab
    block_params_by_id = {}
    for layer in param_layers:
        if layer.get('block_id') is not None:
            if layer['block_id'] not in block_params_by_id: block_params_by_id[layer['block_id']] = []
            block_params_by_id[layer['block_id']].append(layer)
        elif layer.get('uses_shared_slab', False):
            # Large non-block layers that use the shared slab
            shared_slab_params.append(layer)
        else:
            non_block_params.append(layer)

    # Calculate max weight/bias sizes for shared slab
    # MHSA needs 4 projection weights at once (Q/K/V/Out), so we need to account for that
    max_block_weight_bytes = 0
    max_block_bias_bytes = 0
    for block_id, block_layers in block_params_by_id.items():
        # Calculate max for MLP layers (fc1, fc2)
        for layer in block_layers:
            weight_elem_size = (
                4 if layer.get('weight_type') == 'fp32'
                else (2 if layer.get('weight_type') == 'int16' else 1)
            )
            weight_bytes = layer['weight_elements'] * weight_elem_size
            if weight_bytes > max_block_weight_bytes:
                max_block_weight_bytes = weight_bytes
            bias_elem_size = 4  # Both int32 and float are 4 bytes
            bias_bytes = layer['bias_elements'] * bias_elem_size
            if bias_bytes > max_block_bias_bytes:
                max_block_bias_bytes = bias_bytes

        # MHSA needs 4 projection weights/biases at once
        # Detect MHSA by looking for attn_q, attn_k, attn_v, attn_out layers
        attn_layers = [l for l in block_layers if 'attn_q' in l['c_name'] or 'attn_k' in l['c_name']
                       or 'attn_v' in l['c_name'] or 'attn_out' in l['c_name']]
        if len(attn_layers) >= 4:
            # Sum the 4 MHSA projection weights
            mhsa_weight_bytes = sum(l['weight_elements'] * (
                4 if l.get('weight_type') == 'fp32'
                else (2 if l.get('weight_type') == 'int16' else 1)
            )
                                   for l in attn_layers[:4])
            mhsa_bias_bytes = sum(l['bias_elements'] * 4 for l in attn_layers[:4])
            if mhsa_weight_bytes > max_block_weight_bytes:
                max_block_weight_bytes = mhsa_weight_bytes
            if mhsa_bias_bytes > max_block_bias_bytes:
                max_block_bias_bytes = mhsa_bias_bytes

    # Include shared_slab_params in max calculation (large non-block layers)
    # For NE16 layers with streaming, use the PACKED weight size
    for layer in shared_slab_params:
        if layer.get('ne16_with_streaming', False):
            # NE16 packed weight size (padded to 16-byte Ki groups)
            weight_bytes = layer.get('ne16_packed_weight_elements', layer['weight_elements'])
        else:
            weight_elem_size = (
                4 if layer.get('weight_type') == 'fp32'
                else (2 if layer.get('weight_type') == 'int16' else 1)
            )
            weight_bytes = layer['weight_elements'] * weight_elem_size
        if weight_bytes > max_block_weight_bytes:
            max_block_weight_bytes = weight_bytes
        bias_elem_size = 4  # Both int32 and float are 4 bytes
        bias_bytes = layer.get('bias_elements', 0) * bias_elem_size
        if bias_bytes > max_block_bias_bytes:
            max_block_bias_bytes = bias_bytes

    # Also check block layers for NE16 streaming
    for block_id, block_layers in block_params_by_id.items():
        for layer in block_layers:
            if layer.get('ne16_with_streaming', False):
                weight_bytes = layer.get('ne16_packed_weight_elements', layer['weight_elements'])
                if weight_bytes > max_block_weight_bytes:
                    max_block_weight_bytes = weight_bytes

    # Also check non-block layers for NE16 streaming (e.g., NE16 3x3 conv with L3 weights)
    ne16_streaming_non_block = []
    for layer in non_block_params:
        if layer.get('ne16_with_streaming', False):
            ne16_streaming_non_block.append(layer)
            weight_bytes = layer.get('ne16_packed_weight_elements', layer['weight_elements'])
            if weight_bytes > max_block_weight_bytes:
                max_block_weight_bytes = weight_bytes
            # NE16 bias uses int32, so 4 bytes per element
            bias_bytes = layer.get('ne16_bias_corr_elements', layer.get('bias_elements', 0)) * 4
            if bias_bytes > max_block_bias_bytes:
                max_block_bias_bytes = bias_bytes

    # Use shared slab if there are transformer blocks OR large non-block layers OR NE16 streaming layers
    use_block_weight_slab = len(block_params_by_id) > 0 or len(shared_slab_params) > 0 or len(ne16_streaming_non_block) > 0
%>
% if use_block_weight_slab:
    // Allocate shared weight slab FIRST (largest allocation, avoids fragmentation)
    // This slab is used for transformer block streaming
    a->block_weight_slab_size = ${max_block_weight_bytes};
    a->block_bias_slab_size = ${max_block_bias_bytes};
#ifndef MINIMAL_OUTPUT
    printf("CL: Allocating shared weight slab: %zu bytes (max weight=%zu, max bias=%zu)\n",
           a->block_weight_slab_size + a->block_bias_slab_size,
           a->block_weight_slab_size, a->block_bias_slab_size);
#endif
    a->block_weight_slab = (int8_t *)pi_l2_malloc(a->block_weight_slab_size);
    if (!a->block_weight_slab) {
        printf("CL ERR: Failed to allocate shared weight slab (%zu bytes)\n", a->block_weight_slab_size);
        alloc_fail = 1;
    }
    a->block_bias_slab = (int8_t *)pi_l2_malloc(a->block_bias_slab_size);
    if (!a->block_bias_slab) {
        printf("CL ERR: Failed to allocate shared bias slab (%zu bytes)\n", a->block_bias_slab_size);
        alloc_fail = 1;
    }
    // Initialize block weight pointers to NULL (will be set to slab during streaming)
% for block_id, block_layers in sorted(block_params_by_id.items()):
% for layer in block_layers:
    a->${layer['c_name']}_weight = NULL;
% if layer.get('bias_elements', 0) > 0:
    a->${layer['c_name']}_bias = NULL;
% endif
    % if layer.get('ne16_eligible', False) and not layer.get('ne16_with_streaming', False):
        // NE16 packed weights for block layer (non-streaming: dedicated L2 buffers)
        a->${layer['c_name']}_ne16_packed = (int8_t *)pi_l2_malloc(${layer.get('ne16_packed_weight_elements', layer['weight_elements'])} * sizeof(int8_t));
        if (!a->${layer['c_name']}_ne16_packed) alloc_fail = 1;
        a->${layer['c_name']}_ne16_bias_corr = (int32_t *)pi_l2_malloc(${layer.get('ne16_bias_corr_elements', layer.get('bias_elements', 0))} * sizeof(int32_t));
        if (!a->${layer['c_name']}_ne16_bias_corr) alloc_fail = 1;
        % if layer.get('ne16_use_hw_requant', False):
<%
    # Get output channels: out_features for linear, out_ch for conv2d, bias_elements as fallback
    hw_scale_ch = layer.get('out_features', layer.get('out_ch', layer.get('bias_elements', 0)))
%>
        // NE16 HW outquant scale arrays (per-output-channel)
        a->${layer['c_name']}_ne16_hw_scale = (uint8_t *)pi_l2_malloc(${hw_scale_ch} * sizeof(uint8_t));
        if (!a->${layer['c_name']}_ne16_hw_scale) alloc_fail = 1;
        a->${layer['c_name']}_ne16_hw_scale_shift = (uint8_t *)pi_l2_malloc(${hw_scale_ch} * sizeof(uint8_t));
        if (!a->${layer['c_name']}_ne16_hw_scale_shift) alloc_fail = 1;
        % endif
    % elif layer.get('ne16_with_streaming', False):
        // NE16 with L3 streaming for block layer: Use shared slab
        a->${layer['c_name']}_ne16_packed = NULL;
        a->${layer['c_name']}_ne16_bias_corr = NULL;
    % endif
% endfor
% endfor
    // Initialize shared slab non-block layer pointers to NULL
% for layer in shared_slab_params:
    a->${layer['c_name']}_weight = NULL;
% if layer.get('bias_elements', 0) > 0:
    a->${layer['c_name']}_bias = NULL;
% endif
% endfor
% else:
    a->block_weight_slab = NULL;
    a->block_bias_slab = NULL;
    a->block_weight_slab_size = 0;
    a->block_bias_slab_size = 0;
% endif

% for layer in non_block_params:
%   if layer.get('deferred', False):
    // ${layer['c_name']} deferred: allocated on-demand by composite layer
    a->${layer['c_name']}_weight = NULL;
    % if layer.get('bias_elements', 0) > 0:
    a->${layer['c_name']}_bias = NULL;
    % endif
%   elif layer.get('uses_mamba_scratch', False):
    // ${layer['c_name']} uses mamba_shared_scratch (L3 streaming, saves L2)
    // Weight/bias will be streamed from L3 into scratch at execution time
    a->${layer['c_name']}_weight = NULL;  // Will point to mamba_shared_scratch during execution
    % if layer.get('bias_elements', 0) > 0:
    a->${layer['c_name']}_bias = NULL;    // Will point to mamba_shared_scratch + weight_offset
    % endif
%   elif not layer.get('is_streamed', False):
        // Standard Allocation
    % if layer.get('weight_type') == 'fp32':
        a->${layer['c_name']}_weight = (float *)pi_l2_malloc(${layer['weight_elements']} * sizeof(float));
    % elif layer.get('weight_type') == 'int16':
        a->${layer['c_name']}_weight = (int16_t *)pi_l2_malloc(${layer['weight_elements']} * sizeof(int16_t));
    % else:
        a->${layer['c_name']}_weight = (int8_t *)pi_l2_malloc(${layer['weight_elements']} * sizeof(int8_t));
    % endif
        if (!a->${layer['c_name']}_weight) alloc_fail = 1;

    % if layer.get('bias_elements', 0) > 0 and layer.get('bias_index') is not None:
    % if layer.get('bias_type') == 'int32':
        a->${layer['c_name']}_bias = (int32_t *)pi_l2_malloc(${layer['bias_elements']} * sizeof(int32_t));
    % else:
        a->${layer['c_name']}_bias = (float *)pi_l2_malloc(${layer['bias_elements']} * sizeof(float));
    % endif
        if (!a->${layer['c_name']}_bias) alloc_fail = 1;
    % elif layer.get('bias_elements', 0) > 0:
        a->${layer['c_name']}_bias = NULL;  // No bias for this layer (bias_index not set)
    % endif
    % if layer.get('ne16_eligible', False) and not layer.get('ne16_with_streaming', False):
        // NE16 packed weights allocation (non-streaming: use dedicated L2 buffers)
        a->${layer['c_name']}_ne16_packed = (int8_t *)pi_l2_malloc(${layer.get('ne16_packed_weight_elements', layer['weight_elements'])} * sizeof(int8_t));
        if (!a->${layer['c_name']}_ne16_packed) alloc_fail = 1;
        a->${layer['c_name']}_ne16_bias_corr = (int32_t *)pi_l2_malloc(${layer.get('ne16_bias_corr_elements', layer.get('bias_elements', 0))} * sizeof(int32_t));
        if (!a->${layer['c_name']}_ne16_bias_corr) alloc_fail = 1;
        % if layer.get('ne16_use_hw_requant', False):
<%
    # Get output channels: out_features for linear, out_ch for conv2d, bias_elements as fallback
    hw_scale_ch_nb = layer.get('out_features', layer.get('out_ch', layer.get('bias_elements', 0)))
%>
        // NE16 HW outquant scale arrays (per-output-channel)
        a->${layer['c_name']}_ne16_hw_scale = (uint8_t *)pi_l2_malloc(${hw_scale_ch_nb} * sizeof(uint8_t));
        if (!a->${layer['c_name']}_ne16_hw_scale) alloc_fail = 1;
        a->${layer['c_name']}_ne16_hw_scale_shift = (uint8_t *)pi_l2_malloc(${hw_scale_ch_nb} * sizeof(uint8_t));
        if (!a->${layer['c_name']}_ne16_hw_scale_shift) alloc_fail = 1;
        % endif
    % elif layer.get('ne16_with_streaming', False):
        // NE16 with L3 streaming: Use shared slab, not dedicated L2 buffers
        a->${layer['c_name']}_ne16_packed = NULL;   // Will use block_weight_slab
        a->${layer['c_name']}_ne16_bias_corr = NULL; // Will use block_bias_slab
    % endif
%   else:
    // Streamed: Pointers will be set to Slabs during execution
    a->${layer['c_name']}_weight = NULL;
    % if layer.get('bias_elements', 0) > 0:
    a->${layer['c_name']}_bias = NULL;
    % endif
    // The actual storage is in the 'weight_slab' buffer registered in the Arena
%   endif
% endfor

% if ssm_entries:
    // Allocate SSM Parameters
% for entry in ssm_entries:
    a->${entry['c_name']}_x_proj_weight = (int8_t *)pi_l2_malloc(${entry['x_proj_weight_elements']} * sizeof(int8_t));
    if (!a->${entry['c_name']}_x_proj_weight) alloc_fail = 1;
    a->${entry['c_name']}_dt_proj_weight = (int8_t *)pi_l2_malloc(${entry['dt_proj_weight_elements']} * sizeof(int8_t));
    if (!a->${entry['c_name']}_dt_proj_weight) alloc_fail = 1;
    a->${entry['c_name']}_dt_proj_bias_q16_16 = (int32_t *)pi_l2_malloc(${entry['dt_proj_bias_elements']} * sizeof(int32_t));  // Q16.16
    if (!a->${entry['c_name']}_dt_proj_bias_q16_16) alloc_fail = 1;
    a->${entry['c_name']}_A_q15 = (int16_t *)pi_l2_malloc(${entry['A_elements']} * sizeof(int16_t));  // Q15 A
    if (!a->${entry['c_name']}_A_q15) alloc_fail = 1;
    a->${entry['c_name']}_D_q15 = (int16_t *)pi_l2_malloc(${entry['D_elements']} * sizeof(int16_t));  // Q15 D
    if (!a->${entry['c_name']}_D_q15) alloc_fail = 1;
    a->${entry['c_name']}_softplus_lut = (int16_t *)pi_l2_malloc(256 * sizeof(int16_t));  // Q8.8 softplus
    if (!a->${entry['c_name']}_softplus_lut) alloc_fail = 1;
    a->${entry['c_name']}_exp_lut = (int16_t *)pi_l2_malloc(256 * sizeof(int16_t));  // Q15 exp
    if (!a->${entry['c_name']}_exp_lut) alloc_fail = 1;
% endfor
% endif


<%
    # Collect alternating attention layers with NE16 support
    alt_attn_ne16_specs = [s for s in layer_specs if s.get('op') == 'alternating_attention' and (s.get('use_ne16_qkv', False) or s.get('use_ne16_out', False))]
%>
% if alt_attn_ne16_specs:
    // Allocate Alternating Attention NE16 Packed Weight Buffers
% for spec in alt_attn_ne16_specs:
<%
    # Calculate packed weight sizes (16-byte Ki padding for NE16)
    embed_dim = spec.get('embed_dim', 180)
    nb_ki_qkv = (embed_dim + 15) // 16
    nb_ki_out = (embed_dim + 15) // 16
    qkv_packed_size = 3 * embed_dim * nb_ki_qkv * 16
    out_packed_size = embed_dim * nb_ki_out * 16
%>
% if spec.get('use_ne16_qkv', False):
    // NE16 QKV projection for ${spec['c_name']}
    a->${spec['c_name']}_qkv_ne16_packed = (int8_t *)pi_l2_malloc(${qkv_packed_size});
    if (!a->${spec['c_name']}_qkv_ne16_packed) { printf("WARN: ${spec['c_name']} QKV NE16 alloc failed\\n"); }
    a->${spec['c_name']}_qkv_ne16_bias = (int32_t *)pi_l2_malloc(${3 * embed_dim} * sizeof(int32_t));
    if (!a->${spec['c_name']}_qkv_ne16_bias) { printf("WARN: ${spec['c_name']} QKV bias alloc failed\\n"); }
% else:
    a->${spec['c_name']}_qkv_ne16_packed = NULL;
    a->${spec['c_name']}_qkv_ne16_bias = NULL;
% endif
% if spec.get('use_ne16_out', False):
    // NE16 output projection for ${spec['c_name']}
    a->${spec['c_name']}_out_ne16_packed = (int8_t *)pi_l2_malloc(${out_packed_size});
    if (!a->${spec['c_name']}_out_ne16_packed) { printf("WARN: ${spec['c_name']} out NE16 alloc failed\\n"); }
    a->${spec['c_name']}_out_ne16_bias = (int32_t *)pi_l2_malloc(${embed_dim} * sizeof(int32_t));
    if (!a->${spec['c_name']}_out_ne16_bias) { printf("WARN: ${spec['c_name']} out bias alloc failed\\n"); }
% else:
    a->${spec['c_name']}_out_ne16_packed = NULL;
    a->${spec['c_name']}_out_ne16_bias = NULL;
% endif
% endfor
% endif

% if mamba_block_entries and mamba_slab_sizes:
    // Allocate MambaBlock L3 Streaming resources
    // Single shared slab for weights - load one direction at a time from L3
<%
    slab_in_proj = mamba_slab_sizes.get('in_proj_weight', 0)
    slab_conv1d_weight = mamba_slab_sizes.get('conv1d_weight', 0)
    slab_conv1d_bias = mamba_slab_sizes.get('conv1d_bias', 0)
    slab_x_proj = mamba_slab_sizes.get('x_proj_weight', 0)
    slab_dt_proj = mamba_slab_sizes.get('dt_proj_weight', 0)
    slab_dt_proj_bias = mamba_slab_sizes.get('dt_proj_bias', 0)
    slab_A = mamba_slab_sizes.get('A', 0)
    slab_D = mamba_slab_sizes.get('D', 0)
    slab_out_proj = mamba_slab_sizes.get('out_proj_weight', 0)
    slab_lut = mamba_slab_sizes.get('lut', 256)
    slab_total = mamba_slab_sizes.get('total', 0)
    # Double-buffering for async prefetch overlap
    enable_double_buffer = mamba_slab_sizes.get('enable_double_buffer', False)
    proj_chunk_bytes = mamba_slab_sizes.get('proj_chunk_bytes', slab_in_proj)
%>
    a->mamba_weight_slab_size = ${slab_total};
    a->mamba_weight_slab = (int8_t *)pi_l2_malloc(a->mamba_weight_slab_size);
    if (!a->mamba_weight_slab) alloc_fail = 1;

#ifndef MINIMAL_OUTPUT
    printf("CL: Allocated Mamba weight slab: %d bytes (%.1f KB)\n",
           (int)a->mamba_weight_slab_size, (float)a->mamba_weight_slab_size / 1024.0f);
#endif

    // Setup fixed offsets into the shared slab (pointers assigned during streaming)
    if (a->mamba_weight_slab) {
        int8_t *slab = a->mamba_weight_slab;
        size_t offset = 0;
        // Ping buffer for in_proj chunked streaming
        a->mamba_cur_in_proj_weight = (int8_t *)(slab + offset);
        offset += ${slab_in_proj} * sizeof(int8_t);
% if enable_double_buffer:
        // Pong buffer for double-buffered async prefetch (enables compute/DMA overlap)
        a->mamba_cur_in_proj_weight_alt = (int8_t *)(slab + offset);
        offset += ${proj_chunk_bytes} * sizeof(int8_t);
% else:
        a->mamba_cur_in_proj_weight_alt = NULL;  // No double-buffering
% endif
        a->mamba_cur_conv1d_weight = (int8_t *)(slab + offset);
        offset += ${slab_conv1d_weight} * sizeof(int8_t);
        a->mamba_cur_conv1d_bias = (int32_t *)(slab + offset);
        offset += ${slab_conv1d_bias} * sizeof(int32_t);
        a->mamba_cur_silu_lut = (int8_t *)(slab + offset);
        offset += ${slab_lut} * sizeof(int8_t);
        a->mamba_cur_silu_gate_lut_q13 = (int16_t *)(slab + offset);
        offset += ${slab_lut} * sizeof(int16_t);
        a->mamba_cur_softplus_lut = (int16_t *)(slab + offset);
        offset += ${slab_lut} * sizeof(int16_t);
        a->mamba_cur_exp_lut = (int16_t *)(slab + offset);
        offset += ${slab_lut} * sizeof(int16_t);
        a->mamba_cur_x_proj_weight = (int8_t *)(slab + offset);
        offset += ${slab_x_proj} * sizeof(int8_t);
        a->mamba_cur_dt_proj_weight = (int8_t *)(slab + offset);
        offset += ${slab_dt_proj} * sizeof(int8_t);
        a->mamba_cur_dt_proj_bias_q16_16 = (int32_t *)(slab + offset);
        offset += ${slab_dt_proj_bias} * sizeof(int32_t);
        a->mamba_cur_A_q15 = (int16_t *)(slab + offset);
        offset += ${slab_A} * sizeof(int16_t);
        a->mamba_cur_D_q15 = (int16_t *)(slab + offset);
        offset += ${slab_D} * sizeof(int16_t);
        a->mamba_cur_out_proj_weight = (int8_t *)(slab + offset);
    }

    // Shared scratch buffer for all Mamba directions (run sequentially)
    a->mamba_shared_scratch_size = ${mamba_slab_sizes['scratch']};
    a->mamba_shared_scratch = (int8_t *)pi_l2_malloc(a->mamba_shared_scratch_size);
    if (!a->mamba_shared_scratch) alloc_fail = 1;
#ifndef MINIMAL_OUTPUT
    printf("  Mamba shared scratch: %zu bytes\n", a->mamba_shared_scratch_size);
#endif
% endif

    if (alloc_fail) {
        printf("CL ERR: L2 parameter allocation failed\n");
        a->status = -1;
        goto cleanup;
    }

#ifndef MINIMAL_OUTPUT
    printf("  Arena: %p - %p (%d bytes)\n",
           (void *)a->l2_arena,
           (void *)((char*)a->l2_arena + ${l2_arena_size}),
           ${l2_arena_size});
    printf("  input_quant: %p\n", (void *)a->input_quant);
% if any(spec['op'] == 'conv2d' for spec in layer_specs):
    <%
        conv_layer = next(spec for spec in layer_specs if spec['op'] == 'conv2d')
    %>
    printf("  %s_out: %p\n", "${conv_layer['name']}", (void *)a->${conv_layer['output_buffer']});
% elif any(spec['op'] in ('linear_int8', 'linear_fp32') for spec in layer_specs):
    <%
        linear_layer = next(spec for spec in layer_specs if spec['op'] in ('linear_int8', 'linear_fp32'))
    %>
    printf("  %s_out: %p\n", "${linear_layer['name']}", (void *)a->${linear_layer['output_buffer']});
% endif
% for layer in param_layers:
%   if not layer.get('is_streamed', False) and not layer.get('deferred', False):
    printf("  %s_weight: %p\n", "${layer['c_name']}", (void *)a->${layer['c_name']}_weight);
%     if layer.get('bias_elements', 0) > 0:
    printf("  %s_bias: %p\n", "${layer['c_name']}", (void *)a->${layer['c_name']}_bias);
%     endif
%   endif
% endfor
#endif // MINIMAL_OUTPUT

    // --- Prefetch L2 Resident Data ---
<%
    l2_resident_layers = [l for l in param_layers if l.get('weight_residency', 'L2') == 'L2' and l.get('block_id') is None and not l.get('deferred', False)]
    l3_staged_layers = [l for l in param_layers if l.get('weight_residency', 'L2') == 'L3_STAGED' and l.get('block_id') is None and not l.get('deferred', False)]
    # Count entries: 1 for primary input + N for additional inputs + 1 per weight + 1 per bias (only if bias_elements > 0)
    num_additional_inputs = len(additional_input_entries) if additional_input_entries else 0
    num_l2_prefetch = 1 + num_additional_inputs + len(l2_resident_layers) + sum(1 for l in l2_resident_layers if l.get('bias_elements', 0) > 0 and l.get('bias_index') is not None)
%>
    prefetch_descriptor_t network_prefetch[${num_l2_prefetch}] = {
        { .name = "input_quant", .l3_src = a->input_l3, .l2_dst = a->input_quant, .bytes = ${input_numel} * sizeof(int8_t), .priority = 0 },
% if additional_input_entries:
        // Additional inputs for multi-input model
% for i, entry in enumerate(additional_input_entries):
        { .name = "${entry['quant_layer']}", .l3_src = a->additional_inputs_l3[${i}], .l2_dst = a->${entry['buffer_c_name']}, .bytes = ${entry['numel']} * sizeof(int8_t), .priority = 0 },
% endfor
% endif
% for layer in l2_resident_layers:
        { .name = "${layer['c_name']}_weight", .l3_src = a->${layer['c_name']}_weight_l3, .l2_dst = a->${layer['c_name']}_weight,
          .bytes = ${layer['weight_elements']} * sizeof(${'float' if layer.get('weight_type')=='fp32' else ('int16_t' if layer.get('weight_type')=='int16' else 'int8_t')}), .priority = 0 },
% if layer.get('bias_elements', 0) > 0 and layer.get('bias_index') is not None:
        { .name = "${layer['c_name']}_bias", .l3_src = a->${layer['c_name']}_bias_l3, .l2_dst = a->${layer['c_name']}_bias,
          .bytes = ${layer['bias_elements']} * sizeof(${'int32_t' if layer.get('bias_type')=='int32' else 'float'}), .priority = 0 },
% endif
% endfor
    };
    if (prefetch_network_data(network_prefetch, ${num_l2_prefetch}) != 0) {
        printf("CL ERR: Prefetch failed\n"); a->status = -1; goto cleanup;
    }

<%
    # NE16 layers can be in L2 or L3 resident, use param_layers to find all NE16-eligible layers
    # Exclude ne16_with_streaming layers - they load weights directly from L3 during execution
    ne16_layers_prefetch = [l for l in param_layers if l.get('ne16_eligible', False) and not l.get('ne16_with_streaming', False)]
    # Count HW requant layers for additional prefetch entries (scale + scale_shift)
    ne16_hw_requant_layers = [l for l in ne16_layers_prefetch if l.get('ne16_use_hw_requant', False)]
%>
% if ne16_layers_prefetch:
    // --- NE16 Packed Weights Prefetch (non-streaming layers only) ---
    // Also prefetch original weights and bias for runtime packing (works around gvsoc issue)
<%
    # Base: 3 entries per layer (packed weights + bias_corr + original weights)
    # +1 entry for layers with bias (original bias)
    # +2 entries for HW requant layers: hw_scale + hw_scale_shift
    ne16_layers_with_bias = [l for l in ne16_layers_prefetch if l.get('bias_elements', 0) > 0]
    num_ne16_prefetch = len(ne16_layers_prefetch) * 3 + len(ne16_layers_with_bias) + len(ne16_hw_requant_layers) * 2
%>
    prefetch_descriptor_t ne16_prefetch[${num_ne16_prefetch}] = {
% for layer in ne16_layers_prefetch:
        { .name = "${layer['c_name']}_ne16_packed", .l3_src = a->${layer['c_name']}_ne16_packed_l3, .l2_dst = a->${layer['c_name']}_ne16_packed,
          .bytes = ${layer.get('ne16_packed_weight_elements', layer['weight_elements'])} * sizeof(int8_t), .priority = 0 },
        { .name = "${layer['c_name']}_ne16_bias_corr", .l3_src = a->${layer['c_name']}_ne16_bias_corr_l3, .l2_dst = a->${layer['c_name']}_ne16_bias_corr,
          .bytes = ${layer.get('ne16_bias_corr_elements', layer.get('out_features', 0))} * sizeof(int32_t), .priority = 0 },
        { .name = "${layer['c_name']}_weight", .l3_src = a->${layer['c_name']}_weight_l3, .l2_dst = a->${layer['c_name']}_weight,
          .bytes = ${layer['weight_elements']} * sizeof(int8_t), .priority = 0 },
    % if layer.get('bias_elements', 0) > 0:
        { .name = "${layer['c_name']}_bias", .l3_src = a->${layer['c_name']}_bias_l3, .l2_dst = a->${layer['c_name']}_bias,
          .bytes = ${layer.get('bias_elements', 0)} * sizeof(int32_t), .priority = 0 },
    % endif
    % if layer.get('ne16_use_hw_requant', False):
<%
        # Get output channels for prefetch descriptor
        prefetch_hw_scale_ch = layer.get('out_features', layer.get('out_ch', layer.get('bias_elements', 0)))
%>
        { .name = "${layer['c_name']}_ne16_hw_scale", .l3_src = a->${layer['c_name']}_ne16_hw_scale_l3, .l2_dst = a->${layer['c_name']}_ne16_hw_scale,
          .bytes = ${prefetch_hw_scale_ch} * sizeof(uint8_t), .priority = 0 },
        { .name = "${layer['c_name']}_ne16_hw_scale_shift", .l3_src = a->${layer['c_name']}_ne16_hw_scale_shift_l3, .l2_dst = a->${layer['c_name']}_ne16_hw_scale_shift,
          .bytes = ${prefetch_hw_scale_ch} * sizeof(uint8_t), .priority = 0 },
    % endif
% endfor
    };
    if (prefetch_network_data(ne16_prefetch, ${num_ne16_prefetch}) != 0) {
        printf("CL ERR: NE16 prefetch failed\n"); a->status = -1; goto cleanup;
    }
% endif

% if ssm_entries:
    // --- SSM Parameter Prefetch ---
<%
    num_ssm_prefetch = len(ssm_entries) * 7  # 7 params per SSM layer (integer-only)
%>
    prefetch_descriptor_t ssm_prefetch[${num_ssm_prefetch}] = {
% for i, entry in enumerate(ssm_entries):
        { .name = "${entry['c_name']}_x_proj_weight", .l3_src = a->${entry['c_name']}_x_proj_weight_l3, .l2_dst = a->${entry['c_name']}_x_proj_weight, .bytes = ${entry['x_proj_weight_elements']} * sizeof(int8_t), .priority = 0 },
        { .name = "${entry['c_name']}_dt_proj_weight", .l3_src = a->${entry['c_name']}_dt_proj_weight_l3, .l2_dst = a->${entry['c_name']}_dt_proj_weight, .bytes = ${entry['dt_proj_weight_elements']} * sizeof(int8_t), .priority = 0 },
        { .name = "${entry['c_name']}_dt_proj_bias_q16_16", .l3_src = a->${entry['c_name']}_dt_proj_bias_q16_16_l3, .l2_dst = a->${entry['c_name']}_dt_proj_bias_q16_16, .bytes = ${entry['dt_proj_bias_elements']} * sizeof(int32_t), .priority = 0 },
        { .name = "${entry['c_name']}_A_q15", .l3_src = a->${entry['c_name']}_A_q15_l3, .l2_dst = a->${entry['c_name']}_A_q15, .bytes = ${entry['A_elements']} * sizeof(int16_t), .priority = 0 },
        { .name = "${entry['c_name']}_D_q15", .l3_src = a->${entry['c_name']}_D_q15_l3, .l2_dst = a->${entry['c_name']}_D_q15, .bytes = ${entry['D_elements']} * sizeof(int16_t), .priority = 0 },
        { .name = "${entry['c_name']}_softplus_lut", .l3_src = a->${entry['c_name']}_softplus_lut_l3, .l2_dst = a->${entry['c_name']}_softplus_lut, .bytes = 256 * sizeof(int16_t), .priority = 0 },
        { .name = "${entry['c_name']}_exp_lut", .l3_src = a->${entry['c_name']}_exp_lut_l3, .l2_dst = a->${entry['c_name']}_exp_lut, .bytes = 256 * sizeof(int16_t), .priority = 0 },
% endfor
    };
    if (prefetch_network_data(ssm_prefetch, ${num_ssm_prefetch}) != 0) {
        printf("CL ERR: SSM prefetch failed\n"); a->status = -1; goto cleanup;
    }
% endif


% if mamba_block_entries:
    // MambaBlock weights use L3 streaming - loaded on-demand per direction
    // (No upfront prefetch - see mamba_stream_direction_weights() calls during execution)
#ifndef MINIMAL_OUTPUT
    printf("CL: MambaBlock L3 streaming enabled (%d directions, load on-demand)\n", ${len(mamba_block_entries)});
#endif
% endif

    // --- Golden Output Prefetch ---
#if !defined(DISABLE_GOLDEN_VALIDATION) && !defined(DISABLE_INTERMEDIATE_GOLDEN) && !defined(MINIMAL_OUTPUT) && !defined(MINIMAL_OUTPUT)
<%
    non_block_golden = [s for s in layer_specs if s.get('golden_slot') is not None and s.get('block_id') is None]
%>
% if use_streamed_golden:
% if mamba_block_entries:
    // L3 Streamed Golden Mode: Reuse mamba_shared_scratch for golden staging
    // Golden validation happens AFTER Mamba execution, so scratch is free for reuse
    // This saves ${golden_chunk_size} bytes of L2 memory
    a->golden_staging = a->mamba_shared_scratch;
#ifndef MINIMAL_OUTPUT
    printf("CL: L3 streamed golden validation (reusing Mamba scratch, chunk: %d bytes)\n",
           ${golden_chunk_size});
#endif
% else:
    // L3 Streamed Golden Mode: Allocate single staging buffer for chunked comparison
    // Goldens are streamed on-demand in ${golden_chunk_size}-byte chunks before each comparison
    a->golden_staging = (int8_t *)pi_l2_malloc(${golden_chunk_size});
    if (!a->golden_staging) {
        printf("CL ERR: Failed to allocate golden staging buffer (${golden_chunk_size} bytes)\n");
        a->status = -1;
        goto cleanup;
    }
#ifndef MINIMAL_OUTPUT
    printf("CL: L3 streamed golden validation enabled (chunk: %d bytes, max golden: %d bytes)\n",
           ${golden_chunk_size}, ${max_golden_size});
#endif
% endif
% elif non_block_golden:
    golden_prefetch_descriptor_t golden_prefetch[${len(non_block_golden)}] = {
% for spec in non_block_golden:
<%
    golden_var_name = spec.get('golden_c_name', spec['c_name'])
%>\
        { .name = "${spec['golden_buffer']}", .l3_src = a->${golden_var_name}_golden_l3, .l2_dst_ptr = (void **)&a->${golden_var_name}_golden, .bytes = ${spec['golden_size']}, .priority = 1 },
% endfor
    };
    if (prefetch_golden_outputs(golden_prefetch, ${len(non_block_golden)}) != 0) { printf("CL ERR: Golden prefetch failed\n"); a->status = -1; goto cleanup; }

% for spec in non_block_golden:
<%
    golden_var_name = spec.get('golden_c_name', spec['c_name'])
%>\
    printf("  %s_golden: %p\n", "${golden_var_name}", (void *)a->${golden_var_name}_golden);
% endfor
% endif
#endif

#if !defined(DISABLE_GOLDEN_VALIDATION) && !defined(MINIMAL_OUTPUT)
    // Allocate golden buffer early, but prefetch AFTER execution (to avoid corruption during Mamba scratch reuse)
    golden_fp32 = (float *)pi_l2_malloc(NUM_CLASSES * sizeof(float));
#endif

    // --- EXECUTION LOOP ---
    prefetch_async_handle_t async_prefetch_weight = {0}, async_prefetch_bias = {0};
    prefetch_async_handle_t async_prefetch_multi[8] = {0}; // For MHSA (Q/K/V/Out weights + biases)
    int prefetch_pending = 0;
    float current_scale = ${input_scale}f;

<%
    # Check if first layer needs prefetch
    # Find first L3_STAGED layer that we can prefetch at startup
    l3_staged_indices = [i for i, s in enumerate(layer_specs) if s.get('weight_residency') == 'L3_STAGED' and s['op'] in ['conv2d', 'linear_int8', 'linear_fp32', 'mhsa']]

    # Also find L3_TILED layers (they stream weights during execution and may conflict with startup prefetch)
    l3_tiled_indices = [i for i, s in enumerate(layer_specs) if s.get('weight_residency') == 'L3_TILED' or s.get('memory_tier') == 'L3_TILED']

    # Only prefetch at startup if the first staged layer comes BEFORE any tiled layers
    # Otherwise, the tiled layer's L3 streaming may conflict with the prefetch
    first_staged = None
    if l3_staged_indices:
        first_staged_idx = l3_staged_indices[0]
        # Check if any L3_TILED layer comes before the first staged layer
        l3_tiled_before_staged = [i for i in l3_tiled_indices if i < first_staged_idx]
        if not l3_tiled_before_staged:
            first_staged = layer_specs[first_staged_idx]
        # else: Skip startup prefetch - tiled layers will use L3 first
%>
% if first_staged:
    // Prefetch first staged layer
% if first_staged['op'] == 'mhsa':
    {
        const int mhsa_weight_bytes = ${first_staged['q_weight_elements']} * sizeof(int8_t);
        const int mhsa_bias_bytes   = ${first_staged['bias_elements']} * sizeof(int32_t);

        prefetch_async_start(&async_prefetch_multi[0], "${first_staged['c_name']}_q_weight", a->${first_staged['q_param']}_weight_l3, a->${first_staged['q_param']}_weight, mhsa_weight_bytes);
        prefetch_async_start(&async_prefetch_multi[1], "${first_staged['c_name']}_q_bias",   a->${first_staged['q_param']}_bias_l3,   a->${first_staged['q_param']}_bias,   mhsa_bias_bytes);
        prefetch_async_start(&async_prefetch_multi[2], "${first_staged['c_name']}_k_weight", a->${first_staged['k_param']}_weight_l3, a->${first_staged['k_param']}_weight, mhsa_weight_bytes);
        prefetch_async_start(&async_prefetch_multi[3], "${first_staged['c_name']}_k_bias",   a->${first_staged['k_param']}_bias_l3,   a->${first_staged['k_param']}_bias,   mhsa_bias_bytes);
        prefetch_async_start(&async_prefetch_multi[4], "${first_staged['c_name']}_v_weight", a->${first_staged['v_param']}_weight_l3, a->${first_staged['v_param']}_weight, mhsa_weight_bytes);
        prefetch_async_start(&async_prefetch_multi[5], "${first_staged['c_name']}_v_bias",   a->${first_staged['v_param']}_bias_l3,   a->${first_staged['v_param']}_bias,   mhsa_bias_bytes);
        prefetch_async_start(&async_prefetch_multi[6], "${first_staged['c_name']}_out_weight", a->${first_staged['out_param']}_weight_l3, a->${first_staged['out_param']}_weight, mhsa_weight_bytes);
        prefetch_async_start(&async_prefetch_multi[7], "${first_staged['c_name']}_out_bias",   a->${first_staged['out_param']}_bias_l3,   a->${first_staged['out_param']}_bias,   mhsa_bias_bytes);
        prefetch_pending = 1;
    }
% else:
    prefetch_async_start(&async_prefetch_weight, "${first_staged['c_name']}_weight", a->${first_staged['c_name']}_weight_l3, a->${first_staged['c_name']}_weight, ${first_staged['weight_elements']} * sizeof(${'float' if first_staged.get('weight_type')=='fp32' else ('int16_t' if first_staged.get('weight_type')=='int16' else 'int8_t')}));
% if first_staged.get('bias_elements', 0) > 0:
    prefetch_async_start(&async_prefetch_bias, "${first_staged['c_name']}_bias", a->${first_staged['c_name']}_bias_l3, a->${first_staged['c_name']}_bias, ${first_staged['bias_elements']} * sizeof(${'int32_t' if first_staged.get('bias_type')=='int32' else 'float'}));
% endif
    prefetch_pending = 1;
% endif
% endif

% if len(block_params_by_id) > 0:
    int current_loaded_block = -1;
% endif

<%
    # Check if cross-block mamba prefetching is needed
    mamba_blocks = [s for s in layer_specs if s.get('op') == 'mamba_wrapper']
    has_multiple_mamba_blocks = len(mamba_blocks) > 1
    # mamba_needs_chunking is a global template variable set earlier in the template
    enable_cross_block_prefetch = has_multiple_mamba_blocks and mamba_needs_chunking and enable_double_buffer
%>
% if enable_cross_block_prefetch:
    // CROSS-BLOCK MAMBA PREFETCHING: Prefetch next block's FWD small weights during current block's REV
    // This hides ~417KB of DMA per block transition
    // Define the prefetch handle struct (same as in network_helpers.c)
    #define NUM_SMALL_WEIGHT_BUFFERS_CL 11
    typedef struct {
        pi_cl_ram_req_t reqs[NUM_SMALL_WEIGHT_BUFFERS_CL];
        int bytes[NUM_SMALL_WEIGHT_BUFFERS_CL];
        int pending;
    } small_weights_prefetch_t;
% endif

    /* ---
     * RUNTIME EXECUTOR - Data-driven dispatch over network_layers[] array
     * --- */

    // [EXECUTOR FIX] Synchronously prefetch ALL L3-staged weights before the loop
    // The template-unrolled path does this with overlapped async prefetch, but
    // the executor loop cannot easily track prefetch state per-layer.
    // For correctness, we prefetch all L3-staged weights up front.
<%
    # Exclude block layers (block_id is set) - they use the shared slab, not per-layer L2 pointers
    l3_staged_layers = [s for s in layer_specs if s.get('weight_residency') == 'L3_STAGED' and s['op'] in ['conv2d', 'linear_int8', 'linear_fp32', 'mhsa'] and s.get('block_id') is None]
%>
% if l3_staged_layers:
#ifndef MINIMAL_OUTPUT
    printf("CL: EXECUTOR: Prefetching ${len(l3_staged_layers)} L3-staged layer weights\n");
#endif
% for staged_spec in l3_staged_layers:
% if staged_spec['op'] == 'mhsa':
    // MHSA layer ${staged_spec['c_name']}: prefetch Q/K/V/Out weights and biases
    {
        l3_prefetch_request_t req;
        req.l3_src = a->${staged_spec['q_param']}_weight_l3; req.l2_dst = a->${staged_spec['q_param']}_weight;
        req.bytes = ${staged_spec['q_weight_elements']} * sizeof(int8_t); req.priority = 0;
        l3_prefetch_sync(&req);
        req.l3_src = a->${staged_spec['q_param']}_bias_l3; req.l2_dst = a->${staged_spec['q_param']}_bias;
        req.bytes = ${staged_spec['bias_elements']} * sizeof(int32_t); req.priority = 0;
        l3_prefetch_sync(&req);
        req.l3_src = a->${staged_spec['k_param']}_weight_l3; req.l2_dst = a->${staged_spec['k_param']}_weight;
        req.bytes = ${staged_spec['k_weight_elements']} * sizeof(int8_t); req.priority = 0;
        l3_prefetch_sync(&req);
        req.l3_src = a->${staged_spec['k_param']}_bias_l3; req.l2_dst = a->${staged_spec['k_param']}_bias;
        req.bytes = ${staged_spec['bias_elements']} * sizeof(int32_t); req.priority = 0;
        l3_prefetch_sync(&req);
        req.l3_src = a->${staged_spec['v_param']}_weight_l3; req.l2_dst = a->${staged_spec['v_param']}_weight;
        req.bytes = ${staged_spec['v_weight_elements']} * sizeof(int8_t); req.priority = 0;
        l3_prefetch_sync(&req);
        req.l3_src = a->${staged_spec['v_param']}_bias_l3; req.l2_dst = a->${staged_spec['v_param']}_bias;
        req.bytes = ${staged_spec['bias_elements']} * sizeof(int32_t); req.priority = 0;
        l3_prefetch_sync(&req);
        req.l3_src = a->${staged_spec['out_param']}_weight_l3; req.l2_dst = a->${staged_spec['out_param']}_weight;
        req.bytes = ${staged_spec['out_weight_elements']} * sizeof(int8_t); req.priority = 0;
        l3_prefetch_sync(&req);
        req.l3_src = a->${staged_spec['out_param']}_bias_l3; req.l2_dst = a->${staged_spec['out_param']}_bias;
        req.bytes = ${staged_spec['bias_elements']} * sizeof(int32_t); req.priority = 0;
        l3_prefetch_sync(&req);
    }
% else:
    // Layer ${staged_spec['c_name']}: prefetch weight${ ' and bias' if staged_spec.get('bias_elements', 0) > 0 else '' }
    {
        l3_prefetch_request_t req;
        req.l3_src = a->${staged_spec['c_name']}_weight_l3; req.l2_dst = a->${staged_spec['c_name']}_weight;
        req.bytes = ${staged_spec['weight_elements']} * sizeof(${'float' if staged_spec.get('weight_type')=='fp32' else ('int16_t' if staged_spec.get('weight_type')=='int16' else 'int8_t')}); req.priority = 0;
        l3_prefetch_sync(&req);
% if staged_spec.get('bias_elements', 0) > 0:
        req.l3_src = a->${staged_spec['c_name']}_bias_l3; req.l2_dst = a->${staged_spec['c_name']}_bias;
        req.bytes = ${staged_spec['bias_elements']} * sizeof(${'int32_t' if staged_spec.get('bias_type')=='int32' else 'float'}); req.priority = 0;
        l3_prefetch_sync(&req);
% endif
    }
% endif
% endfor
    // Clear prefetch_pending since we've done all prefetches synchronously
    prefetch_pending = 0;
% endif

    // Execute all layers via runtime dispatcher
#ifdef ENABLE_PERF_COUNTERS
    layer_perf_t layer_perf;  // Per-layer performance counter
#endif

    for (int layer_idx = 0; layer_idx < ${len(layer_specs)}; layer_idx++) {
        const LayerSpec *layer = &network_layers[layer_idx];
        layer_runtime_ctx_t ctx = {0};

        // Setup runtime context from args struct
        ctx.l1_buffer = a->l1_buffer;
        ctx.l1_buffer_size = a->l1_buffer_size;
        ctx.cluster_dev = a->cluster_dev;
        ctx.ram_dev = a->ram_dev;
        ctx.network_args = a;  // For complex ops (Mamba) needing full network context

        // NE16 persistent L1 buffers (pre-allocated to avoid per-layer alloc overhead)
        ctx.ne16_weight_l1 = a->ne16_weight_l1;
        ctx.ne16_weight_l1_size = a->ne16_weight_l1_size;
        ctx.ne16_bias_l1 = a->ne16_bias_l1;
        ctx.ne16_bias_l1_size = a->ne16_bias_l1_size;

#ifdef ENABLE_PERF_COUNTERS
        ctx.perf_counter = &layer_perf;
#endif

        // Map buffer pointers based on layer index
        // (Generated switch to resolve buffer names at compile time)
        switch (layer_idx) {
% for loop_idx, spec in enumerate(layer_specs):
            case ${loop_idx}: // ${spec['c_name']} (${spec['op']})
<%
    # Determine buffer mappings for this layer
    input_buf = spec.get('input_buffer')
    output_buf = spec.get('output_buffer')

    # In-place operations use 'buffer' field for both input and output
    if spec['op'] in ['requantize', 'gelu', 'silu', 'relu', 'pos_embed'] and spec.get('buffer'):
        input_buf = spec['buffer']
        output_buf = spec['buffer']  # Same buffer (in-place)

    # Add ops use input1_buffer and input2_buffer
    if spec['op'] == 'add':
        input_buf = spec.get('input1_buffer')
        input_b_buf = spec.get('input2_buffer')
    else:
        input_b_buf = spec.get('input_b_buffer')
%>\
% if input_buf:
                ctx.input_buffer_l2 = a->${input_buf};
% endif
% if output_buf and output_buf != input_buf:
  % if spec['op'] == 'linear_fp32':
                ctx.output_buffer_l2 = (int8_t *)a->${output_buf};  // FP32 output
  % else:
                ctx.output_buffer_l2 = a->${output_buf};
  % endif
% elif output_buf:
                ctx.output_buffer_l2 = a->${output_buf};  // In-place
% endif
% if spec.get('block_id') is not None and spec['op'] in ('linear_int8', 'linear_fp32'):
<%
    linear_weight_size = spec.get('in_features', 0) * spec.get('out_features', 0)
    linear_bias_size = spec.get('out_features', 0) * 4
    has_bias = spec.get('bias_elements', 0) > 0
    is_fp32 = spec['op'] == 'linear_fp32'
%>
                // Block layer: Load weights from L3 to shared slab
                cl_ram_read(a->block_weight_slab, a->${spec['c_name']}_weight_l3, ${linear_weight_size});
% if has_bias:
                cl_ram_read(a->block_bias_slab, a->${spec['c_name']}_bias_l3, ${linear_bias_size});
% endif
                ctx.weight_l2 = a->block_weight_slab;
                ctx.bias_l2 = ${'(float *)a->block_bias_slab' if is_fp32 and has_bias else '(int32_t *)a->block_bias_slab' if has_bias else 'NULL'};
% elif spec.get('uses_shared_slab', False) and spec['op'] == 'linear_int8':
<%
    linear_weight_size = spec.get('in_features', 0) * spec.get('out_features', 0)
    linear_bias_size = spec.get('out_features', 0) * 4
    has_bias = spec.get('bias_elements', 0) > 0
%>
                // Large non-block layer: Load weights from L3 to shared slab
                cl_ram_read(a->block_weight_slab, a->${spec['c_name']}_weight_l3, ${linear_weight_size});
% if has_bias:
                cl_ram_read(a->block_bias_slab, a->${spec['c_name']}_bias_l3, ${linear_bias_size});
% endif
                ctx.weight_l2 = a->block_weight_slab;
                ctx.bias_l2 = ${'(int32_t *)a->block_bias_slab' if has_bias else 'NULL'};
% elif spec.get('memory_tier') == 'L3_TILED' and spec['op'] in ['conv2d', 'linear_int8', 'linear_fp32']:
                // L3_TILED: Dynamic streaming from L3 during execution
                ctx.weight_l2 = a->${spec['c_name']}_weight_slab;
                ctx.bias_l2 = a->${spec['c_name']}_bias_slab;
                ctx.l3_weight_addr = a->${spec['c_name']}_weight_l3;
                ctx.l3_bias_addr = a->${spec['c_name']}_bias_l3;
                ctx.l3_output_addr = a->${spec.get('compare_buffer', spec['output_buffer'])};
                ctx.l3_tiling_enabled = 1;
% elif spec.get('is_streamed', False) and spec['op'] in ['conv2d', 'linear_int8', 'linear_fp32']:
                // Streamed weights (L3_STAGED): Use weight_slab (prefetched from L3)
                ctx.weight_l2 = a->${spec['c_name']}_weight_slab;
                ctx.bias_l2 = a->${spec['c_name']}_bias_slab;
% elif spec.get('ne16_with_streaming', False) and spec['op'] == 'linear_ne16':
<%
    # NE16 packed weight size (padded to 16-byte Ki groups)
    in_features = spec.get('in_features', 0)
    out_features = spec.get('out_features', 0)
    nb_ki = (in_features + 15) // 16
    ne16_packed_weight_size = out_features * nb_ki * 16
    ne16_bias_size = out_features * 4
%>
                // NE16 Linear with L3 Streaming: Load packed weights from L3 to shared slab
                cl_ram_read(a->block_weight_slab, a->${spec['c_name']}_ne16_packed_l3, ${ne16_packed_weight_size});
                cl_ram_read(a->block_bias_slab, a->${spec['c_name']}_ne16_bias_corr_l3, ${ne16_bias_size});
                ctx.ne16_weights_packed = a->block_weight_slab;
                ctx.ne16_bias_corrected = (int32_t *)a->block_bias_slab;
            % if spec.get('ne16_use_hw_requant', False):
                // HW outquant: per-channel scale arrays (reference-compatible) - streamed from L3
                // L3 streaming path currently uses SW requant; HW scales are not streamed here.
                ctx.ne16_hw_scale = NULL;
                ctx.ne16_hw_scale_shift = NULL;
                ctx.ne16_use_hw_requant = 0;
            % else:
                ctx.ne16_hw_scale = NULL;
                ctx.ne16_hw_scale_shift = NULL;
                ctx.ne16_use_hw_requant = 0;
            % endif
                // Original weights as fallback (SLOW PATH - only used if pre-packed unavailable)
                ctx.weight_l2 = NULL;
                ctx.bias_l2 = NULL;
% elif spec.get('ne16_eligible', False) and spec['op'] == 'linear_ne16':
                // NE16 Linear: Use pre-packed weights (FAST PATH - no runtime packing)
                ctx.ne16_weights_packed = a->${spec['c_name']}_ne16_packed;
                ctx.ne16_bias_corrected = a->${spec['c_name']}_ne16_bias_corr;
            % if spec.get('ne16_use_hw_requant', False):
                // HW outquant: per-channel scale arrays (reference-compatible)
                ctx.ne16_hw_scale = a->${spec['c_name']}_ne16_hw_scale;
                ctx.ne16_hw_scale_shift = a->${spec['c_name']}_ne16_hw_scale_shift;
                ctx.ne16_use_hw_requant = 1;
            % else:
                ctx.ne16_hw_scale = NULL;
                ctx.ne16_hw_scale_shift = NULL;
                ctx.ne16_use_hw_requant = 0;
            % endif
                // Original weights as fallback (SLOW PATH - only used if pre-packed unavailable)
                ctx.weight_l2 = a->${spec['c_name']}_weight;
                ctx.bias_l2 = a->${spec['c_name']}_bias;
% elif spec.get('ne16_eligible', False) and spec['op'] == 'conv2d_1x1_ne16':
                // NE16 Conv2D 1x1: Use pre-packed weights (FAST PATH - no runtime packing)
                ctx.ne16_weights_packed = a->${spec['c_name']}_ne16_packed;
                ctx.ne16_bias_corrected = a->${spec['c_name']}_ne16_bias_corr;
            % if spec.get('ne16_use_hw_requant', False):
                // HW outquant: per-channel scale arrays (reference-compatible)
                ctx.ne16_hw_scale = a->${spec['c_name']}_ne16_hw_scale;
                ctx.ne16_hw_scale_shift = a->${spec['c_name']}_ne16_hw_scale_shift;
                ctx.ne16_use_hw_requant = 1;
            % else:
                ctx.ne16_hw_scale = NULL;
                ctx.ne16_hw_scale_shift = NULL;
                ctx.ne16_use_hw_requant = 0;
            % endif
                // Original weights as fallback (SLOW PATH - only used if pre-packed unavailable)
                ctx.weight_l2 = a->${spec['c_name']}_weight;
                ctx.bias_l2 = a->${spec['c_name']}_bias;
% elif spec.get('ne16_with_streaming', False) and spec['op'] == 'conv2d_3x3_ne16':
<%
    # NE16 3x3 packed weight size: Ko * ceil(Ki/16) * 16 * 9 bytes
    conv_in_ch = spec.get('in_ch', 0)
    conv_out_ch = spec.get('out_ch', 0)
    conv_nb_ki = (conv_in_ch + 15) // 16
    ne16_3x3_packed_size = conv_out_ch * conv_nb_ki * 16 * 9
    ne16_3x3_bias_size = conv_out_ch * 4
%>
                // NE16 Conv2D 3x3 with L3 Streaming: Load packed weights from L3 to shared slab
                cl_ram_read(a->block_weight_slab, a->${spec['c_name']}_ne16_packed_l3, ${ne16_3x3_packed_size});
                cl_ram_read(a->block_bias_slab, a->${spec['c_name']}_ne16_bias_corr_l3, ${ne16_3x3_bias_size});
                ctx.ne16_weights_packed = a->block_weight_slab;
                ctx.ne16_bias_corrected = (int32_t *)a->block_bias_slab;
                ctx.ne16_hw_scale = NULL;
                ctx.ne16_hw_scale_shift = NULL;
                ctx.ne16_use_hw_requant = 0;
                // Original weights as fallback (not used with L3 streaming)
                ctx.weight_l2 = NULL;
                ctx.bias_l2 = NULL;
% elif spec.get('ne16_eligible', False) and spec['op'] == 'conv2d_3x3_ne16':
                // NE16 Conv2D 3x3: Use pre-packed weights (FAST PATH - no runtime packing)
                ctx.ne16_weights_packed = a->${spec['c_name']}_ne16_packed;
                ctx.ne16_bias_corrected = a->${spec['c_name']}_ne16_bias_corr;
            % if spec.get('ne16_use_hw_requant', False):
                // HW outquant: per-channel scale arrays (reference-compatible)
                ctx.ne16_hw_scale = a->${spec['c_name']}_ne16_hw_scale;
                ctx.ne16_hw_scale_shift = a->${spec['c_name']}_ne16_hw_scale_shift;
                ctx.ne16_use_hw_requant = 1;
            % else:
                ctx.ne16_hw_scale = NULL;
                ctx.ne16_hw_scale_shift = NULL;
                ctx.ne16_use_hw_requant = 0;
            % endif
                // Original weights as fallback (SLOW PATH - only used if pre-packed unavailable)
                ctx.weight_l2 = a->${spec['c_name']}_weight;
                ctx.bias_l2 = a->${spec['c_name']}_bias;
% elif spec.get('ne16_eligible', False) and spec['op'] == 'conv2d_3x3_dw_ne16':
                // NE16 Depthwise Conv2D 3x3: Use pre-packed weights (FAST PATH)
                ctx.ne16_weights_packed = a->${spec['c_name']}_ne16_packed;
                ctx.ne16_bias_corrected = a->${spec['c_name']}_ne16_bias_corr;
            % if spec.get('ne16_use_hw_requant', False):
                // HW outquant: per-channel scale arrays (depthwise)
                ctx.ne16_hw_scale = a->${spec['c_name']}_ne16_hw_scale;
                ctx.ne16_hw_scale_shift = a->${spec['c_name']}_ne16_hw_scale_shift;
                ctx.ne16_use_hw_requant = 1;
            % else:
                ctx.ne16_hw_scale = NULL;
                ctx.ne16_hw_scale_shift = NULL;
                ctx.ne16_use_hw_requant = 0;
            % endif
                // Original weights as fallback (SLOW PATH - only used if pre-packed unavailable)
                ctx.weight_l2 = a->${spec['c_name']}_weight;
                ctx.bias_l2 = a->${spec['c_name']}_bias;
% elif spec.get('c_name') and spec['op'] in ['conv2d', 'linear_int8', 'linear_fp32']:
                ctx.weight_l2 = a->${spec['c_name']}_weight;
% if spec.get('bias_elements', 0) > 0:
                ctx.bias_l2 = a->${spec['c_name']}_bias;
% else:
                ctx.bias_l2 = NULL;  // No bias for this layer
% endif
% elif spec['op'] == 'patch_embed' and spec.get('c_name'):
                // PatchEmbed: Conv2D-style weight [embed_dim, in_ch, patch_h, patch_w] and bias [embed_dim]
                ctx.weight_l2 = a->${spec['c_name']}_weight;
                ctx.bias_l2 = a->${spec['c_name']}_bias;
% if spec.get('uses_mamba_scratch', False):
                ctx.mamba_scratch = a->mamba_shared_scratch;
                ctx.mamba_scratch_size = a->mamba_shared_scratch_size;
% endif
% elif spec['op'] == 'pos_embed' and spec.get('c_name'):
                // PosEmbed: Weights streamed from L3 into scratch buffer
                ctx.pos_embed_weight_l3 = a->${spec['c_name']}_weight_l3;
% if spec.get('uses_mamba_scratch', False):
                ctx.mamba_scratch = a->mamba_shared_scratch;
                ctx.mamba_scratch_size = a->mamba_shared_scratch_size;
% endif
% elif spec['op'] == 'conv1d_depthwise' and spec.get('c_name'):
                // Conv1D Depthwise: weight [C, K] and bias [C]
                ctx.weight_l2 = a->${spec['c_name']}_weight;
                ctx.bias_l2 = a->${spec['c_name']}_bias;
% elif spec['op'] == 'silu' and spec.get('c_name'):
                // SiLU: LUT is stored as "weight" (256 entries, INT8)
                ctx.weight_l2 = a->${spec['c_name']}_weight;
% elif input_b_buf:
                ctx.input_b_buffer_l2 = a->${input_b_buf};
% endif
% if spec['op'] == 'concat':
                // Concat: populate input arrays for variable number of inputs
                ctx.concat_num_inputs = ${spec['num_inputs']};
  % for i, buf in enumerate(spec.get('input_buffers', [])):
                ctx.concat_inputs[${i}] = a->${buf};
                ctx.concat_scales[${i}] = ${spec['input_scales'][i]}f;
                ctx.concat_channels[${i}] = ${spec['channels_per_input'][i]};
  % endfor
% endif
% if spec['op'] == 'layernorm':
                // LayerNorm: set weight/bias pointers
  % if spec.get('block_id') is not None and spec.get('weight_param'):
<%
    ln_weight_c_name = spec.get('weight_param', '').replace('_weight', '')
    ln_normalized_dim = spec.get('normalized_dim', 192)
%>
                // Block LayerNorm: Stream weights from L3 to shared slab
                {
                    const int weight_bytes = ${ln_normalized_dim} * sizeof(float);
                    cl_ram_read(a->block_weight_slab, a->${ln_weight_c_name}_weight_l3, weight_bytes);
                    cl_ram_read(a->block_bias_slab, a->${ln_weight_c_name}_bias_l3, weight_bytes);
                    ctx.layernorm_weight = (float *)a->block_weight_slab;
                    ctx.layernorm_bias = (float *)a->block_bias_slab;
                }
  % elif spec.get('weight_param'):
                ctx.layernorm_weight = a->${spec['weight_param']};
                ctx.layernorm_bias = a->${spec['bias_param']};
  % else:
                ctx.layernorm_weight = NULL;
                ctx.layernorm_bias = NULL;
  % endif
% endif
% if spec['op'] == 'rmsnorm':
                // RMSNorm: set weight pointer (no bias for RMSNorm)
  % if spec.get('block_id') is not None and spec.get('weight_param'):
<%
    rms_weight_c_name = spec.get('weight_param', '').replace('_weight', '')
    rms_normalized_dim = spec.get('normalized_dim', 192)
%>
                // Block RMSNorm: Stream weights from L3 to shared slab
                {
                    const int weight_bytes = ${rms_normalized_dim} * sizeof(float);
                    cl_ram_read(a->block_weight_slab, a->${rms_weight_c_name}_weight_l3, weight_bytes);
                    ctx.rmsnorm_weight = (float *)a->block_weight_slab;
                }
  % elif spec.get('weight_param'):
                ctx.rmsnorm_weight = a->${spec['weight_param']};
  % else:
                ctx.rmsnorm_weight = NULL;
  % endif
% endif
% if spec['op'] == 'mhsa':
  % if spec.get('block_id') is not None:
<%
    mhsa_weight_size = spec['embed_dim'] * spec['embed_dim']
    mhsa_bias_size = spec['embed_dim']
%>
                // MHSA Block layer: Stream weights from L3 to shared slab
                {
                    const int weight_size = ${mhsa_weight_size};
                    const int bias_size = ${mhsa_bias_size} * sizeof(int32_t);
                    int8_t *weight_ptr = a->block_weight_slab;
                    int8_t *bias_ptr = a->block_bias_slab;

                    // Q
                    cl_ram_read(weight_ptr, a->${spec['q_param']}_weight_l3, weight_size);
                    cl_ram_read(bias_ptr, a->${spec['q_param']}_bias_l3, bias_size);
                    ctx.mhsa_q_weight_l2 = weight_ptr; ctx.mhsa_q_bias_l2 = (int32_t *)bias_ptr;
                    weight_ptr += weight_size; bias_ptr += bias_size;

                    // K
                    cl_ram_read(weight_ptr, a->${spec['k_param']}_weight_l3, weight_size);
                    cl_ram_read(bias_ptr, a->${spec['k_param']}_bias_l3, bias_size);
                    ctx.mhsa_k_weight_l2 = weight_ptr; ctx.mhsa_k_bias_l2 = (int32_t *)bias_ptr;
                    weight_ptr += weight_size; bias_ptr += bias_size;

                    // V
                    cl_ram_read(weight_ptr, a->${spec['v_param']}_weight_l3, weight_size);
                    cl_ram_read(bias_ptr, a->${spec['v_param']}_bias_l3, bias_size);
                    ctx.mhsa_v_weight_l2 = weight_ptr; ctx.mhsa_v_bias_l2 = (int32_t *)bias_ptr;
                    weight_ptr += weight_size; bias_ptr += bias_size;

                    // Out
                    cl_ram_read(weight_ptr, a->${spec['out_param']}_weight_l3, weight_size);
                    cl_ram_read(bias_ptr, a->${spec['out_param']}_bias_l3, bias_size);
                    ctx.mhsa_out_weight_l2 = weight_ptr; ctx.mhsa_out_bias_l2 = (int32_t *)bias_ptr;
                }
  % else:
                // L2-resident MHSA: set Q/K/V/Out weight and bias pointers
                ctx.mhsa_q_weight_l2 = a->${spec['q_param']}_weight;
                ctx.mhsa_q_bias_l2 = a->${spec['q_param']}_bias;
                ctx.mhsa_k_weight_l2 = a->${spec['k_param']}_weight;
                ctx.mhsa_k_bias_l2 = a->${spec['k_param']}_bias;
                ctx.mhsa_v_weight_l2 = a->${spec['v_param']}_weight;
                ctx.mhsa_v_bias_l2 = a->${spec['v_param']}_bias;
                ctx.mhsa_out_weight_l2 = a->${spec['out_param']}_weight;
                ctx.mhsa_out_bias_l2 = a->${spec['out_param']}_bias;
  % endif
                // MHSA: set Q/K/V intermediate projection buffers
                ctx.mhsa_q_buffer_l2 = a->${spec['q_buffer']};
                ctx.mhsa_k_buffer_l2 = a->${spec['k_buffer']};
                ctx.mhsa_v_buffer_l2 = a->${spec['v_buffer']};
% endif
% if spec['op'] == 'cross_attention':
                // Cross-Attention: set Q/K/V/Out weight and bias pointers (reuses MHSA fields)
                ctx.mhsa_q_weight_l2 = a->${spec['q_param']}_weight;
                ctx.mhsa_q_bias_l2 = a->${spec['q_param']}_bias;
                ctx.mhsa_k_weight_l2 = a->${spec['k_param']}_weight;
                ctx.mhsa_k_bias_l2 = a->${spec['k_param']}_bias;
                ctx.mhsa_v_weight_l2 = a->${spec['v_param']}_weight;
                ctx.mhsa_v_bias_l2 = a->${spec['v_param']}_bias;
                ctx.mhsa_out_weight_l2 = a->${spec['out_param']}_weight;
                ctx.mhsa_out_bias_l2 = a->${spec['out_param']}_bias;
                // Cross-Attention specific: query embedding and scratch buffers
                ctx.cross_attn_query_embed = a->${spec['query_param']}_weight;
                ctx.cross_attn_q_proj_out = a->${spec['q_buffer']};
                ctx.cross_attn_k_proj_out = a->${spec['k_buffer']};
                ctx.cross_attn_v_proj_out = a->${spec['v_buffer']};
                ctx.cross_attn_context_out = a->${spec['ctx_buffer']};
% endif
% if spec['op'] == 'alternating_attention':
                // Alternating Attention (Cerebro): set QKV and Output projection weights
                ctx.alt_attn_qkv_weight_l2 = a->${spec['qkv_param']}_weight;
                ctx.alt_attn_qkv_bias_l2 = a->${spec['qkv_param']}_bias;
                ctx.alt_attn_out_weight_l2 = a->${spec['out_param']}_weight;
                ctx.alt_attn_out_bias_l2 = a->${spec['out_param']}_bias;
<%
    embed_dim = spec.get('embed_dim', 180)
    nb_ki = (embed_dim + 15) // 16
    qkv_packed_bytes = 3 * embed_dim * nb_ki * 16
    qkv_bias_bytes = 3 * embed_dim * 4
    out_packed_bytes = embed_dim * nb_ki * 16
    out_bias_bytes = embed_dim * 4
%>
% if spec.get('use_ne16_qkv', False):
                // Load NE16 packed weights for QKV projection from L3
                if (a->${spec['c_name']}_qkv_ne16_packed && a->${spec['c_name']}_qkv_ne16_packed_l3) {
                    cl_ram_read(a->${spec['c_name']}_qkv_ne16_packed, a->${spec['c_name']}_qkv_ne16_packed_l3, ${qkv_packed_bytes});
                    cl_ram_read(a->${spec['c_name']}_qkv_ne16_bias, a->${spec['c_name']}_qkv_ne16_bias_l3, ${qkv_bias_bytes});
                    ctx.alt_attn_qkv_ne16_packed_l2 = a->${spec['c_name']}_qkv_ne16_packed;
                    ctx.alt_attn_qkv_ne16_bias_l2 = a->${spec['c_name']}_qkv_ne16_bias;
                } else {
                    ctx.alt_attn_qkv_ne16_packed_l2 = NULL;
                    ctx.alt_attn_qkv_ne16_bias_l2 = NULL;
                }
% else:
                ctx.alt_attn_qkv_ne16_packed_l2 = NULL;
                ctx.alt_attn_qkv_ne16_bias_l2 = NULL;
% endif
% if spec.get('use_ne16_out', False):
                // Load NE16 packed weights for output projection from L3
                if (a->${spec['c_name']}_out_ne16_packed && a->${spec['c_name']}_out_ne16_packed_l3) {
                    cl_ram_read(a->${spec['c_name']}_out_ne16_packed, a->${spec['c_name']}_out_ne16_packed_l3, ${out_packed_bytes});
                    cl_ram_read(a->${spec['c_name']}_out_ne16_bias, a->${spec['c_name']}_out_ne16_bias_l3, ${out_bias_bytes});
                    ctx.alt_attn_out_ne16_packed_l2 = a->${spec['c_name']}_out_ne16_packed;
                    ctx.alt_attn_out_ne16_bias_l2 = a->${spec['c_name']}_out_ne16_bias;
                } else {
                    ctx.alt_attn_out_ne16_packed_l2 = NULL;
                    ctx.alt_attn_out_ne16_bias_l2 = NULL;
                }
% else:
                ctx.alt_attn_out_ne16_packed_l2 = NULL;
                ctx.alt_attn_out_ne16_bias_l2 = NULL;
% endif
% endif
% if spec.get('fusion_maxpool') and spec.get('fused_output_buffer'):
                // Conv+MaxPool fusion: pooled output goes to fused_output_buffer_l2
                ctx.fused_output_buffer_l2 = a->${spec['fused_output_buffer']};
% endif
                break;
% endfor
            default:
                break;
        }

        // Debug: Show buffer pointers and values
#ifndef MINIMAL_OUTPUT
        printf("CL: DEBUG %s buffers: in=%p out=%p wt=%p bias=%p\n", layer->name,
               (void *)ctx.input_buffer_l2, (void *)ctx.output_buffer_l2, (void *)ctx.weight_l2, (void *)ctx.bias_l2);
        if (ctx.input_buffer_l2) {
            printf("CL: DEBUG %s input: %d %d %d %d %d\n", layer->name,
                   ctx.input_buffer_l2[0], ctx.input_buffer_l2[1],
                   ctx.input_buffer_l2[2], ctx.input_buffer_l2[3],
                   ctx.input_buffer_l2[4]);
        }
        if (ctx.weight_l2) {
            printf("CL: DEBUG %s weight: %d %d %d %d %d\n", layer->name,
                   ctx.weight_l2[0], ctx.weight_l2[1], ctx.weight_l2[2],
                   ctx.weight_l2[3], ctx.weight_l2[4]);
        }
#endif

        // Wait for any pending async prefetch before executing layers with L3-staged weights
        // This is critical for linear_fp32 (classifier) which typically uses L3-staged weights
        if (prefetch_pending) {
            prefetch_async_wait(&async_prefetch_weight);
            prefetch_async_wait(&async_prefetch_bias);
            prefetch_pending = 0;
        }

        // Execute the layer
        if (pi_core_id() == 0) printf("CL: Exec [%d] %s type=%d\n", layer_idx, layer->name, layer->type);
#ifdef LAYER_PROFILING
        uint32_t layer_start = pi_perf_read(PI_PERF_CYCLES);
#endif
        // Composite blocks bypass execute_layer() - call generated helpers directly
        if (layer->type == OP_MAMBA_WRAPPER
% if any(s.get('op') == 'cross_attn_self_refine' for s in layer_specs):
            || layer->type == OP_CROSS_ATTN_SELF_REFINE
% endif
% if any(s.get('op') == 'classification_head_mlp' for s in layer_specs):
            || layer->type == OP_CLASSIFICATION_HEAD_MLP
% endif
        ) {
#ifdef ENABLE_PERF_COUNTERS
            if (ctx.perf_counter) {
                memset(ctx.perf_counter, 0, sizeof(layer_perf_t));
                perf_layer_start(layer->name);
            }
#endif
            if (layer->type == OP_MAMBA_WRAPPER) {
                _execute_mamba_wrapper_impl(layer_idx, layer, a, &ctx);
            }
<%
    casr_dispatch_specs = [s for s in layer_specs if s.get('op') == 'cross_attn_self_refine']
    cls_dispatch_specs = [s for s in layer_specs if s.get('op') == 'classification_head_mlp']
%>
% for idx, _ in enumerate(casr_dispatch_specs):
            if (layer->type == OP_CROSS_ATTN_SELF_REFINE) {
                _execute_cross_attn_self_refine_impl_${idx}(layer_idx, layer, a, &ctx);
            }
% endfor
% for idx, _ in enumerate(cls_dispatch_specs):
            if (layer->type == OP_CLASSIFICATION_HEAD_MLP) {
                _execute_classification_head_impl_${idx}(layer_idx, layer, a, &ctx);
            }
% endfor
#ifdef ENABLE_PERF_COUNTERS
            if (ctx.perf_counter) {
                perf_layer_end(layer->name, ctx.perf_counter);
            }
#endif
        } else {
            execute_layer(layer, &ctx);
        }
#ifdef LAYER_PROFILING
        uint32_t layer_end = pi_perf_read(PI_PERF_CYCLES);
        printf("PROF %s: %u cycles\n", layer->name, layer_end - layer_start);
#endif

        // Memory fence: Ensure all L2 writes are visible before next layer's DMA reads.
        // This is critical because L2 fallback paths (maxpool, relu, etc.) write directly
        // to L2 via CPU, while L1 tiled paths read via DMA. Without the fence, DMA might
        // see stale data from the write buffer.
        asm volatile("fence" ::: "memory");

        // Debug: Show first 5 values of output buffer after each layer
#ifndef MINIMAL_OUTPUT
        if (ctx.output_buffer_l2) {
            printf("CL: DEBUG %s output: %d %d %d %d %d\n", layer->name,
                   ctx.output_buffer_l2[0], ctx.output_buffer_l2[1],
                   ctx.output_buffer_l2[2], ctx.output_buffer_l2[3],
                   ctx.output_buffer_l2[4]);
        }
#endif
    }

    // --- Final Validation & Cleanup ---
#ifndef MINIMAL_OUTPUT
    printf("CL: Layer-by-layer execution complete\n");
#endif

#ifdef ENABLE_PERF_COUNTERS
    // Print performance summary
    perf_summary_print();
#endif

<%
final_buffer = None
int8_classifier = False
classifier_scale = 1.0
for spec in reversed(layer_specs):
    if spec.get('is_classifier'):
        final_buffer = spec['output_buffer']
        int8_classifier = spec.get('int8_classifier_output', False)
        classifier_scale = spec.get('scale_output', 1.0) if int8_classifier else 1.0
        break
    # Fallback when classifier metadata is absent.
    if spec.get('op') == 'linear_fp32':
        final_buffer = spec['output_buffer']
        break
%>
#ifndef MINIMAL_OUTPUT
% if int8_classifier:
    printf("CL: Network output (INT8 classifier):\n");
    for (int i = 0; i < NUM_CLASSES; i++) {
        printf("  Class %d: %d (scale=%.4f)\n", i, (int)a->${final_buffer or 'output_int8'}[i], (float)${classifier_scale});
    }
% else:
    printf("CL: Network output (FP32 logits):\n");
    for (int i = 0; i < NUM_CLASSES; i++) {
        printf("  Class %d: %.4f\n", i, a->${final_buffer or 'output_fp32'}[i]);
    }
% endif
#endif

#if !defined(DISABLE_GOLDEN_VALIDATION) && !defined(MINIMAL_OUTPUT)
    // Prefetch golden AFTER execution to avoid corruption during Mamba scratch reuse
    if (golden_fp32) {
        l3_prefetch_request_t req = { .l3_src = a->golden_l3, .l2_dst = golden_fp32, .bytes = NUM_CLASSES * sizeof(float), .priority = 1 };
        l3_prefetch_sync(&req);
    }
    printf("CL: Golden output:\n");
    for (int i = 0; i < NUM_CLASSES; i++) {
        printf("  Class %d: %.4f\n", i, golden_fp32[i]);
    }

    float max_error = 0.0f;
    float sum_error = 0.0f;
% if int8_classifier:
    // INT8 classifier: convert output to float for comparison
    float output_scale = ${classifier_scale}f;
    for (int i = 0; i < NUM_CLASSES; i++) {
        float output_fp = (float)a->${final_buffer or 'output_int8'}[i] * output_scale;
        float err = fabsf(output_fp - golden_fp32[i]);
        if (err > max_error) max_error = err;
        sum_error += err;
    }
% else:
    for (int i = 0; i < NUM_CLASSES; i++) {
        float err = fabsf(a->${final_buffer or 'output_fp32'}[i] - golden_fp32[i]);
        if (err > max_error) max_error = err;
        sum_error += err;
    }
% endif
    float mean_error = sum_error / NUM_CLASSES;

    printf("CL: Error analysis:\n");
    printf("  Max error:  %.6f\n", max_error);
    printf("  Mean error: %.6f\n", mean_error);

    // Adaptive tolerance based on network architecture
    // Transformers/SSM: Use absolute error (relative error explodes near zero, FP approximations)
    // L3 tiled: Use absolute error (tiled accumulation introduces small numerical noise)
    // CNNs/MLPs: Use relative error with 1% tolerance (bit-exact reproducibility achievable)
<%
    _transformer_op_set = {'mhsa', 'ssm', 'gelu', 'layernorm', 'mamba_block', 'mamba_wrapper', 'silu'}
    has_transformer_ops = any(spec.get('op') in _transformer_op_set for spec in layer_specs)
    def check_l3_tiling(spec):
        tc = spec.get('tile_config')
        if tc is None:
            return False
        # Handle both dict and TileConfig object forms
        if isinstance(tc, dict):
            return tc.get('l3_tiling_enabled', False)
        return getattr(tc, 'l3_tiling_enabled', False)
    has_l3_tiling = any(check_l3_tiling(spec) for spec in layer_specs)
%>
% if has_transformer_ops:
    // Transformer/SSM/GELU: use absolute error threshold since logits can be near zero
    // and fast_exp/fast_softplus/erf approximations introduce small errors
    float abs_tolerance = 3.0f;
% elif any(spec.get('op') == 'ssm' for spec in layer_specs):
    printf("CL: Using SSM tolerance (abs error < 3.0 per logit, accounts for fast_exp approx)\n");
% elif any(spec.get('op') == 'gelu' for spec in layer_specs):
    printf("CL: Using GELU model tolerance (abs error < 3.0 per logit)\n");
% else:
    printf("CL: Using transformer tolerance (abs error < 3.0 per logit)\n");
% endif
    int passed = (max_error < abs_tolerance);
% elif has_l3_tiling:
    // L3 tiled linear: Use absolute error (tiled accumulation introduces numerical noise)
    // Tolerance: 0.5 per logit is reasonable for INT8 with multi-tile accumulation
    float abs_tolerance = 0.5f;
    printf("CL: Using L3-tiled tolerance (abs error < 0.5 per logit)\n");
    int passed = (max_error < abs_tolerance);
% elif additional_input_entries:
    // Multi-input network: Use relaxed relative error (concat + avgpool cause rounding differences)
    // AvgPool2d division rounding differs slightly between Python and C implementations
    float tolerance = 0.05f;  // 5% relative error tolerance for multi-input networks
    int passed = 1;
    for (int i = 0; i < NUM_CLASSES; i++) {
        float denom = fabsf(golden_fp32[i]) + 1e-8f;
        float rel_error = fabsf(a->${final_buffer or 'output_fp32'}[i] - golden_fp32[i]) / denom;
        if (rel_error > tolerance) {
            passed = 0;
            break;
        }
    }
% elif len(layer_specs) >= 50:
    // Deep network (e.g., ResNet-18): Use absolute error tolerance
    // With 50+ layers of INT8 quantization, errors compound but argmax should still match
    float abs_tolerance = 5.0f;
    printf("CL: Using deep-network tolerance (abs error < 5.0 per logit)\n");
    int passed = (max_error < abs_tolerance);
% elif int8_classifier:
    // INT8 classifier: use absolute error tolerance (quantization introduces ~scale-sized errors)
    float abs_tolerance = ${classifier_scale * 2}f;  // 2x scale as tolerance
    printf("CL: Using INT8 classifier tolerance (abs error < %.2f)\n", abs_tolerance);
    int passed = (max_error < abs_tolerance);
% else:
    float tolerance = 0.01f;  // CNN/MLP model
    int passed = 1;
    for (int i = 0; i < NUM_CLASSES; i++) {
        float denom = fabsf(golden_fp32[i]) + 1e-8f;
        float rel_error = fabsf(a->${final_buffer or 'output_fp32'}[i] - golden_fp32[i]) / denom;
        if (rel_error > tolerance) {
            passed = 0;
            break;
        }
    }
% endif

    if (passed) {
% if has_transformer_ops:
        printf("CL: âœ“ Test PASSED (max abs error < 3.0)\n");
% elif has_l3_tiling:
        printf("CL: âœ“ Test PASSED (max abs error < 0.5)\n");
% elif additional_input_entries:
        printf("CL: âœ“ Test PASSED (multi-input, error < %.1f%%)\n", tolerance * 100);
% elif len(layer_specs) >= 50:
        printf("CL: âœ“ Test PASSED (max abs error < 5.0)\n");
% elif int8_classifier:
        printf("CL: âœ“ Test PASSED (INT8 classifier, max abs error < %.2f)\n", abs_tolerance);
% else:
        printf("CL: âœ“ Test PASSED (error < %.1f%%)\n", tolerance * 100);
% endif
        a->status = 0;
    } else {
% if has_transformer_ops:
        printf("CL: âœ— Test FAILED (max abs error >= 3.0)\n");
% elif has_l3_tiling:
        printf("CL: âœ— Test FAILED (max abs error >= 0.5)\n");
% elif additional_input_entries:
        printf("CL: âœ— Test FAILED (multi-input, error > %.1f%%)\n", tolerance * 100);
% elif len(layer_specs) >= 50:
        printf("CL: âœ— Test FAILED (max abs error >= 5.0)\n");
% elif int8_classifier:
        printf("CL: âœ— Test FAILED (INT8 classifier, max abs error >= %.2f)\n", abs_tolerance);
% else:
        printf("CL: âœ— Test FAILED (error > %.1f%%)\n", tolerance * 100);
% endif
        a->status = 1;
    }
#elif defined(MINIMAL_OUTPUT)
    // Minimal output mode - just report cycles, skip validation
    a->status = 0;
#else
    // Golden validation disabled - mark as passed (network executed without crashes)
    printf("CL: Golden validation DISABLED - marking test as PASSED (execution completed)\n");
    a->status = 0;
#endif

    uint32_t cycles_end = pi_perf_read(PI_PERF_CYCLES);
    a->cluster_cycles = cycles_end - cycles_start;
#ifndef MINIMAL_OUTPUT
    printf("CL: Inference complete\n");
#endif
    printf("CL: Total cluster cycles: %u\n", a->cluster_cycles);


cleanup:
    // Free Parameters (non-block layers only - block layers use shared slab)
% for layer in reversed(param_layers):
%   if not layer.get('is_streamed', False) and layer.get('block_id') is None and not layer.get('deferred', False):
%     if layer.get('bias_elements', 0) > 0:
    if (a->${layer['c_name']}_bias) pi_l2_free(a->${layer['c_name']}_bias, ${layer['bias_elements']} * sizeof(${'int32_t' if layer.get('bias_type')=='int32' else 'float'}));
%     endif
    if (a->${layer['c_name']}_weight) pi_l2_free(a->${layer['c_name']}_weight, ${layer['weight_elements']} * sizeof(${'float' if layer.get('weight_type')=='fp32' else ('int16_t' if layer.get('weight_type')=='int16' else 'int8_t')}));
%   endif
% endfor

    // Free Shared Weight Slab
    if (a->block_weight_slab) pi_l2_free(a->block_weight_slab, a->block_weight_slab_size);
    if (a->block_bias_slab) pi_l2_free(a->block_bias_slab, a->block_bias_slab_size);

% if ssm_entries:
    // Free SSM Parameters (integer-only)
% for entry in reversed(ssm_entries):
    if (a->${entry['c_name']}_exp_lut) pi_l2_free(a->${entry['c_name']}_exp_lut, 256 * sizeof(int16_t));
    if (a->${entry['c_name']}_softplus_lut) pi_l2_free(a->${entry['c_name']}_softplus_lut, 256 * sizeof(int16_t));
    if (a->${entry['c_name']}_D_q15) pi_l2_free(a->${entry['c_name']}_D_q15, ${entry['D_elements']} * sizeof(int16_t));
    if (a->${entry['c_name']}_A_q15) pi_l2_free(a->${entry['c_name']}_A_q15, ${entry['A_elements']} * sizeof(int16_t));
    if (a->${entry['c_name']}_dt_proj_bias_q16_16) pi_l2_free(a->${entry['c_name']}_dt_proj_bias_q16_16, ${entry['dt_proj_bias_elements']} * sizeof(int32_t));
    if (a->${entry['c_name']}_dt_proj_weight) pi_l2_free(a->${entry['c_name']}_dt_proj_weight, ${entry['dt_proj_weight_elements']} * sizeof(int8_t));
    if (a->${entry['c_name']}_x_proj_weight) pi_l2_free(a->${entry['c_name']}_x_proj_weight, ${entry['x_proj_weight_elements']} * sizeof(int8_t));
% endfor
% endif


% if mamba_block_entries and mamba_slab_sizes:
    // Free MambaBlock L3 Streaming Resources
    // Free shared scratch buffer (one buffer for all directions)
    if (a->mamba_shared_scratch) pi_l2_free(a->mamba_shared_scratch, a->mamba_shared_scratch_size);
    // Free shared weight slab
    if (a->mamba_weight_slab) pi_l2_free(a->mamba_weight_slab, a->mamba_weight_slab_size);
% endif

    // Free Activation Arena
    if (a->l2_arena) {
#ifndef MINIMAL_OUTPUT
        printf("CL: Freeing L2 Arena (%d bytes)\n", ${l2_arena_size});
#endif
        pi_l2_free(a->l2_arena, ${l2_arena_size});
    }

    // Free Shared L3 Staging Buffer (if any L3 fallback buffers exist)
<%
    # Recalculate max_l3_staging_size for cleanup (same logic as allocation)
    l3_cleanup_sizes = []
    for buf in activation_buffers:
        if buf.get('use_l3_fallback', False):
            elem_size = 1 if buf['ctype'] in ['int8_t', 'uint8_t'] else (4 if buf['ctype'] in ['int32_t', 'float'] else 1)
            l3_cleanup_sizes.append(buf['numel'] * elem_size)
    for pool_buf in shared_activation_pool:
        if pool_buf.get('use_l3_fallback', False):
            elem_size = 1 if pool_buf['ctype'] in ['int8_t', 'uint8_t'] else (4 if pool_buf['ctype'] in ['int32_t', 'float'] else 1)
            l3_cleanup_sizes.append(pool_buf['numel'] * elem_size)
    cleanup_raw_max = max(l3_cleanup_sizes) if l3_cleanup_sizes else 65536
    CLEANUP_MAX_STAGING_CAP = 160 * 1024  # Must match allocation cap
    cleanup_max_l3_staging_size = min(cleanup_raw_max, CLEANUP_MAX_STAGING_CAP)
%>
% if has_l3_fallback:
    if (a->l3_staging_buffer) pi_l2_free(a->l3_staging_buffer, ${cleanup_max_l3_staging_size});
% endif

    // Free Non-Arena Activation Buffers (rare fallback case)
% for buf in activation_buffers:
%   if not buf.get('use_l3_fallback', False) and 'offset' not in buf:
      if (a->${buf['c_name']}) pi_l2_free(a->${buf['c_name']}, ${buf['numel']} * sizeof(${buf['ctype']}));
%   endif
% endfor
% for pool_buf in shared_activation_pool:
%   if not pool_buf.get('use_l3_fallback', False) and 'offset' not in pool_buf:
      if (a->${pool_buf['c_name']}) pi_l2_free(a->${pool_buf['c_name']}, ${pool_buf['numel']} * sizeof(${pool_buf['ctype']}));
%   endif
% endfor

    // Free Golden (final output)
#if !defined(DISABLE_GOLDEN_VALIDATION)
    if (golden_fp32) pi_l2_free(golden_fp32, NUM_CLASSES * sizeof(float));
#endif
}

// --- FC Entry Point ---
<%
    ne16_param_layers_sig = [l for l in param_layers if l.get('ne16_eligible', False)]
%>
int network_run_test_from_l3(void *input_l3, ${'void *additional_inputs_l3[], size_t num_additional_inputs, ' if additional_input_entries else ''}void *weights_l3[], size_t num_weights, void *biases_l3[], size_t num_biases, void *golden_l3, void *intermediate_golden_l3[], size_t num_intermediate, void *l3_activation_buffers[], size_t num_l3_activations${''.join(', void *' + entry['c_name'] + '_ssm_params_l3[]' for entry in ssm_entries)}${''.join(', void *' + entry['c_name'] + '_mamba_params_l3[]' for entry in mamba_block_entries)}\
${', void *ne16_packed_l3[], void *ne16_bias_corr_l3[], void *ne16_hw_scale_l3[], void *ne16_hw_scale_shift_l3[], size_t num_ne16' if ne16_param_layers_sig else ''}${''.join(', void *' + entry['c_name'] + '_alt_attn_ne16_l3[]' for entry in alt_attn_ne16_entries)}, struct pi_device *cluster_dev)
{
#ifndef MINIMAL_OUTPUT
    printf("FC: Preparing network test\n");
#endif
    // Default to failure so we don't report a false PASS if the cluster task never runs.
    g_network_cl_args.status = -1;
    g_network_cl_args.input_l3 = input_l3;
% if additional_input_entries:
    // Map additional inputs for multi-input model
    for (size_t i = 0; i < num_additional_inputs && i < ${len(additional_input_entries)}; i++) {
        g_network_cl_args.additional_inputs_l3[i] = additional_inputs_l3[i];
    }
    g_network_cl_args.num_additional_inputs = num_additional_inputs;
% endif
    g_network_cl_args.golden_l3 = golden_l3;
    g_network_cl_args.cluster_dev = cluster_dev;
    g_network_cl_args.ram_dev = get_ram_ptr();

    // Map L3 Pointers
    size_t idx = 0;
% for layer in param_layers:
    g_network_cl_args.${layer['c_name']}_weight_l3 = weights_l3[idx++];
% endfor
    idx = 0;
% for layer in param_layers:
% if layer.get('bias_elements', 0) > 0:
    g_network_cl_args.${layer['c_name']}_bias_l3 = biases_l3[idx++];
% else:
    idx++;  // Skip NULL bias entry for ${layer.get('name', 'layer')}
% endif
% endfor

<%
    ne16_param_layers = [l for l in param_layers if l.get('ne16_eligible', False)]
%>
% if ne16_param_layers:
    // Map NE16 packed weights L3 pointers
    // after regular weights, indexed by ne16_packed_weight_index and ne16_bias_corr_index
% for layer in ne16_param_layers:
    g_network_cl_args.${layer['c_name']}_ne16_packed_l3 = ne16_packed_l3[${loop.index}];
    g_network_cl_args.${layer['c_name']}_ne16_bias_corr_l3 = ne16_bias_corr_l3[${loop.index}];
% if layer.get('ne16_use_hw_requant', False):
    g_network_cl_args.${layer['c_name']}_ne16_hw_scale_l3 = ne16_hw_scale_l3[${loop.index}];
    g_network_cl_args.${layer['c_name']}_ne16_hw_scale_shift_l3 = ne16_hw_scale_shift_l3[${loop.index}];
% endif
% endfor
% endif

% if ssm_entries:
    // Map SSM L3 Pointers (integer-only)
% for entry in ssm_entries:
    g_network_cl_args.${entry['c_name']}_x_proj_weight_l3 = ${entry['c_name']}_ssm_params_l3[0];
    g_network_cl_args.${entry['c_name']}_dt_proj_weight_l3 = ${entry['c_name']}_ssm_params_l3[1];
    g_network_cl_args.${entry['c_name']}_dt_proj_bias_q16_16_l3 = ${entry['c_name']}_ssm_params_l3[2];
    g_network_cl_args.${entry['c_name']}_A_q15_l3 = ${entry['c_name']}_ssm_params_l3[3];
    g_network_cl_args.${entry['c_name']}_D_q15_l3 = ${entry['c_name']}_ssm_params_l3[4];
    g_network_cl_args.${entry['c_name']}_softplus_lut_l3 = ${entry['c_name']}_ssm_params_l3[5];
    g_network_cl_args.${entry['c_name']}_exp_lut_l3 = ${entry['c_name']}_ssm_params_l3[6];
% endfor
% endif

% if mamba_block_entries:
    // Map MambaBlock L3 Pointers (integer-only)
% for entry in mamba_block_entries:
    g_network_cl_args.${entry['c_name']}_in_proj_weight_l3 = ${entry['c_name']}_mamba_params_l3[0];
    g_network_cl_args.${entry['c_name']}_conv1d_weight_l3 = ${entry['c_name']}_mamba_params_l3[1];
    g_network_cl_args.${entry['c_name']}_conv1d_bias_l3 = ${entry['c_name']}_mamba_params_l3[2];
    g_network_cl_args.${entry['c_name']}_silu_lut_l3 = ${entry['c_name']}_mamba_params_l3[3];
    g_network_cl_args.${entry['c_name']}_silu_gate_lut_q13_l3 = ${entry['c_name']}_mamba_params_l3[4];
    g_network_cl_args.${entry['c_name']}_softplus_lut_l3 = ${entry['c_name']}_mamba_params_l3[5];
    g_network_cl_args.${entry['c_name']}_exp_lut_l3 = ${entry['c_name']}_mamba_params_l3[6];
    g_network_cl_args.${entry['c_name']}_x_proj_weight_l3 = ${entry['c_name']}_mamba_params_l3[7];
    g_network_cl_args.${entry['c_name']}_dt_proj_weight_l3 = ${entry['c_name']}_mamba_params_l3[8];
    g_network_cl_args.${entry['c_name']}_dt_proj_bias_q16_16_l3 = ${entry['c_name']}_mamba_params_l3[9];
    g_network_cl_args.${entry['c_name']}_A_q15_l3 = ${entry['c_name']}_mamba_params_l3[10];
    g_network_cl_args.${entry['c_name']}_D_q15_l3 = ${entry['c_name']}_mamba_params_l3[11];
    g_network_cl_args.${entry['c_name']}_out_proj_weight_l3 = ${entry['c_name']}_mamba_params_l3[12];
% endfor
% endif


% if alt_attn_ne16_entries:
    // Map Alternating Attention NE16 L3 Pointers
    // Array layout: [0]=qkv_packed, [1]=qkv_bias, [2]=out_packed, [3]=out_bias
% for entry in alt_attn_ne16_entries:
    g_network_cl_args.${entry['c_name']}_qkv_ne16_packed_l3 = ${entry['c_name']}_alt_attn_ne16_l3[0];
    g_network_cl_args.${entry['c_name']}_qkv_ne16_bias_l3 = ${entry['c_name']}_alt_attn_ne16_l3[1];
    g_network_cl_args.${entry['c_name']}_out_ne16_packed_l3 = ${entry['c_name']}_alt_attn_ne16_l3[2];
    g_network_cl_args.${entry['c_name']}_out_ne16_bias_l3 = ${entry['c_name']}_alt_attn_ne16_l3[3];
% endfor
% endif

    // Map Intermediate Golden Pointers
#ifndef DISABLE_INTERMEDIATE_GOLDEN
    idx = 0;
% for spec in layer_specs:
% if spec.get('golden_slot') is not None:
<%
    golden_var_name = spec.get('golden_c_name', spec['c_name'])
%>\
    g_network_cl_args.${golden_var_name}_golden_l3 = intermediate_golden_l3[idx++];
% endif
% endfor
#endif

    // Map L3 Fallback Buffers
    idx = 0;
% for buf in l3_fallback_buffers:
    g_network_cl_args.${buf['c_name']}_l3 = l3_activation_buffers[idx++];
% endfor
% for pool_buf in l3_fallback_pools:
    g_network_cl_args.${pool_buf['c_name']}_l3 = l3_activation_buffers[idx++];
% endfor

    // Calculate L1 Size (Double Buffering)
<%
    max_l1_bytes = 0
    has_mhsa = False
    has_mamba = False
    mamba_l1_bytes = 0
    MAMBA_MAX_L1_BYTES = 100000  # keep headroom for SDK stack/internal use
    for spec in layer_specs:
        if spec.get('op') == 'mhsa':
            has_mhsa = True
        if spec.get('op') == 'mamba_block':
            has_mamba = True
            # MambaBlock needs L1 for weight prefetch (largest = in_proj)
            mamba_l1_bytes = max(mamba_l1_bytes, spec['d_model'] * 2 * spec['d_inner'])
        if spec.get('op') == 'mamba_wrapper':
            has_mamba = True
            # MambaWrapper (bidirectional) needs L1 for tiled Linear operations
            # out_proj is largest bottleneck: d_inner * TILE_OUT * 2 bytes for double buffer
            # With TILE_OUT=32 and d_inner=1540: 1540 * 32 * 2 = 98,560 bytes
            # Keep ~30 KB free for SDK stack and internal use (L1 total = ~131 KB)
            d_inner = spec.get('d_inner', 1540)
            tile_out = 32  # Output features per tile (conservative to leave L1 for SDK)
            l1_tile_bytes = d_inner * tile_out * 2  # Double buffer for out_proj
            mamba_l1_bytes = max(mamba_l1_bytes, l1_tile_bytes)

            # Also try to budget for SSM L2â†’L1 staging (weights/LUTs/bias + scratch) when it is reasonable.
            # This enables both SSM prefetch and L1-resident scratch in a single invocation.
            d_state = spec.get('d_state', 16)
            dt_rank = spec.get('dt_rank', 16)
            seq_len = spec.get('seq_len', 0)
            batch = spec.get('batch', 1)
            proj_size = dt_rank + 2 * d_state

            # Prefetch payload into L1
            x_proj_bytes = proj_size * d_inner
            dt_proj_bytes = d_inner * dt_rank
            dt_bias_bytes = d_inner * 4  # int32_t Q16.16
            A_q15_bytes = d_state * d_inner * 2
            D_q15_bytes = d_inner * 2
            lut_bytes = 256 * 2  # int16_t LUTs
            ssm_prefetch_bytes = x_proj_bytes + dt_proj_bytes + dt_bias_bytes + A_q15_bytes + D_q15_bytes + 3 * lut_bytes

            # SSM scratch in L1
            h_state_bytes = batch * d_inner * d_state * 2
            proj_all_bytes = seq_len * proj_size * 4  # int32
            dt_all_bytes = d_inner * seq_len * 2
            bc_bytes = seq_len * d_state * 2
            ssm_scratch_bytes = h_state_bytes + proj_all_bytes + dt_all_bytes + 2 * bc_bytes + 16  # +alignment slack

            # Optional: stage SSM inputs into L1 too (x and z) to reduce EXT loads during x_proj + scan.
            # For channel-major x/z this is 2 * (B*L*d_inner); for interleaved xz, z staging would be larger
            # but we keep the heuristic conservative (runtime checks will skip if it doesn't fit).
            # Add a small fixed overhead for L1 alignment/padding between staged buffers.
            ssm_input_bytes = 2 * batch * seq_len * d_inner + 128

            ssm_l1_bytes = ssm_prefetch_bytes + ssm_scratch_bytes + ssm_input_bytes
            if ssm_l1_bytes <= MAMBA_MAX_L1_BYTES:
                mamba_l1_bytes = max(mamba_l1_bytes, ssm_l1_bytes)
        if spec.get('tile_config'):
            tc = spec['tile_config']
            # Use hasattr() for class-based tile configs, or dict access for dict-based configs
            if hasattr(tc, 'persistent_bytes') and hasattr(tc, 'tile_bytes'):
                # MHSA: persistent = K+V, tile_bytes = per-tile buffers
                # Double buffer for Q tiles and M tiles
                l1 = tc.persistent_bytes + 2 * tc.tile_bytes
            elif hasattr(tc, 'l1_buffer_bytes'):
                # Element-wise ops (ReLU, GELU, Requantize, Add, Concat, LayerNorm, Transpose_2d)
                l1 = tc.l1_buffer_bytes
            elif isinstance(tc, dict):
                # Legacy dict-based tile configs
                l1_in = tc.get('l1_input_bytes', 0)
                if 'l1_partial_sum_bytes' in tc:
                    # GlobalAvgPool: double-buffer input + shared partial sums
                    l1 = 2 * l1_in + tc.get('l1_partial_sum_bytes', 0)
                else:
                    l1_out = tc.get('l1_output_bytes', 0)
                    l1_w = tc.get('l1_weight_bytes', 0)
                    if ('tile_out_ch' in tc) or ('num_out_ch_tiles' in tc):
                        # Conv2D: double-buffer input/output (and weights when enabled)
                        l1 = 2 * l1_in + 2 * l1_out + 2 * l1_w
                    else:
                        # Linear and other ops: input loaded once, output/weights double-buffered
                        l1 = l1_in + 2 * l1_out + 2 * l1_w
            else:
                # Class-based tile configs: compute L1 requirement based on buffer manager layout.
                # - Conv2D: double-buffer input, output (and weight tiles when enabled)
                # - Linear: input loaded once, output+weights double-buffered
                # - GlobalAvgPool: double-buffer input + shared partial sums
                if hasattr(tc, 'tile_out_features'):
                    # LinearTileConfig
                    l1 = tc.l1_input_bytes
                    if hasattr(tc, 'l1_output_bytes'): l1 += 2 * tc.l1_output_bytes
                    if hasattr(tc, 'l1_weight_bytes'): l1 += 2 * tc.l1_weight_bytes
                elif hasattr(tc, 'tile_out_ch') or hasattr(tc, 'num_out_ch_tiles'):
                    # Conv2DTileConfig (covers both weight-cached and non-weight-cached paths)
                    l1 = 2 * getattr(tc, 'l1_input_bytes', 0)
                    if hasattr(tc, 'l1_output_bytes'): l1 += 2 * tc.l1_output_bytes
                    if hasattr(tc, 'l1_weight_bytes'): l1 += 2 * tc.l1_weight_bytes
                elif hasattr(tc, 'l1_partial_sum_bytes'):
                    # GlobalAvgPoolTileConfig
                    l1 = 2 * getattr(tc, 'l1_input_bytes', 0) + tc.l1_partial_sum_bytes
                else:
                    l1 = getattr(tc, 'l1_input_bytes', 0)
                    if hasattr(tc, 'l1_output_bytes'): l1 += 2 * tc.l1_output_bytes # Double buffer out
                    if hasattr(tc, 'l1_weight_bytes'): l1 += 2 * tc.l1_weight_bytes # Double buffer weights
                    if hasattr(tc, 'l1_partial_sum_bytes'): l1 += tc.l1_partial_sum_bytes # Shared
            if l1 > max_l1_bytes: max_l1_bytes = l1

    # MambaBlock L1 requirement for weight prefetch
    if has_mamba and mamba_l1_bytes > max_l1_bytes:
        max_l1_bytes = mamba_l1_bytes

    # Minimum L1 allocation for MHSA 8-core hybrid mode
    # Formula: 8 cores x seq_len x 6 bytes (score buffers) + K buffer (seq_len x head_dim)
    # For typical transformers (200 seq_len, 24 head_dim): ~14KB minimum
    # For larger transformers (800 seq_len, 64 head_dim): ~90KB minimum
    # Use 110KB as safe minimum to enable 8-core mode for most transformers
    MHSA_MIN_L1_BYTES = 110000
    if has_mhsa and max_l1_bytes < MHSA_MIN_L1_BYTES:
        max_l1_bytes = MHSA_MIN_L1_BYTES

    # NE16 pipelined execution buffer requirements
    # For layers with double-buffered Ko-tiling (weights > 64KB):
    #   - 2 * tile_ko * nb_ki * 16 (double-buffered weights)
    #   - 2 * tile_ko * 4 (double-buffered bias)
    #   - tile_tokens * in_features (input scratch)
    #   - tile_tokens * tile_ko * 4 (output scratch)
    # For smaller layers (pipelined mode):
    #   - 2 * input_scratch + output_scratch
    ne16_pipelined_l1 = 0
    for spec in layer_specs:
        if spec.get('op') == 'linear_ne16':
            tile_tokens = spec.get('tile_tokens', spec.get('num_tokens', 49))
            in_features = spec.get('in_features', 192)
            out_features = spec.get('out_features', 64)

            # Calculate packed weight size
            nb_ki = (in_features + 15) // 16
            packed_size = out_features * nb_ki * 16
            MAX_L1_WEIGHTS = 65536

            if packed_size > MAX_L1_WEIGHTS:
                # Double-buffered Ko-tiling mode
                # Find tile_ko that fits in L1 (try 64, 32, 16)
                for tile_ko in [64, 32, 16]:
                    tile_weight_size = tile_ko * nb_ki * 16
                    double_buf_size = 2 * tile_weight_size + 2 * tile_ko * 4
                    if double_buf_size <= MAX_L1_WEIGHTS:
                        break
                # L1 needs: 2*tile_weights + 2*tile_bias + input_scratch + output_scratch
                input_scratch = tile_tokens * in_features
                output_scratch = tile_tokens * tile_ko * 4
                tiled_bytes = double_buf_size + input_scratch + output_scratch
                ne16_pipelined_l1 = max(ne16_pipelined_l1, tiled_bytes)
            else:
                # Pipelined mode: 2 * input_scratch + output_scratch
                pipelined_bytes = 2 * tile_tokens * in_features + tile_tokens * out_features * 4
                ne16_pipelined_l1 = max(ne16_pipelined_l1, pipelined_bytes)
    if ne16_pipelined_l1 > max_l1_bytes:
        max_l1_bytes = ne16_pipelined_l1
%>
% if max_l1_bytes > 0:
    // L1 (TCDM) allocation for tile buffers
    // We reserve space for cluster stacks and runtime overhead before allocating our tile buffer.
    size_t l1_requested = ${max_l1_bytes};
    const size_t l1_total_bytes = ${target_l1_total_bytes};   // Target L1 total
    const size_t l1_reserved_bytes = (size_t)8192 + (size_t)NUM_CORES * (size_t)PI_CL_SLAVE_STACK_SIZE + (size_t)4092;
    const size_t l1_cap_bytes = (l1_total_bytes > l1_reserved_bytes) ? (l1_total_bytes - l1_reserved_bytes) : 0;
    if (l1_requested > l1_cap_bytes) {
        l1_requested = l1_cap_bytes;
    }

    g_network_cl_args.l1_buffer_size = l1_requested;
    g_network_cl_args.l1_buffer = (int8_t *)pi_l1_malloc(cluster_dev, l1_requested);
    if (!g_network_cl_args.l1_buffer) printf("FC WARN: L1 alloc failed\n");
% else:
    g_network_cl_args.l1_buffer = NULL;
    g_network_cl_args.l1_buffer_size = 0;
% endif

% if ne16_eligible_layers and max_ne16_weight_l1_size > 0:
    // NE16 Persistent L1 Buffers - allocated once, reused across all NE16 layers
    // This eliminates per-layer L1 alloc/free overhead for weight transfers
    g_network_cl_args.ne16_weight_l1_size = ${max_ne16_weight_l1_size};
    g_network_cl_args.ne16_weight_l1 = (uint8_t *)pi_l1_malloc(cluster_dev, ${max_ne16_weight_l1_size});
    g_network_cl_args.ne16_bias_l1_size = ${max_ne16_bias_l1_size};
    g_network_cl_args.ne16_bias_l1 = (int32_t *)pi_l1_malloc(cluster_dev, ${max_ne16_bias_l1_size});
#ifndef MINIMAL_OUTPUT
    if (g_network_cl_args.ne16_weight_l1 && g_network_cl_args.ne16_bias_l1) {
        printf("FC: NE16 L1 buffers allocated: weights=%zu, bias=%zu\n",
               g_network_cl_args.ne16_weight_l1_size, g_network_cl_args.ne16_bias_l1_size);
    } else {
        printf("FC WARN: NE16 L1 alloc failed, will use per-layer allocation\n");
    }
#endif
% else:
    g_network_cl_args.ne16_weight_l1 = NULL;
    g_network_cl_args.ne16_weight_l1_size = 0;
    g_network_cl_args.ne16_bias_l1 = NULL;
    g_network_cl_args.ne16_bias_l1_size = 0;
% endif

    struct pi_cluster_task cl_task = {0};
    pi_cluster_task(&cl_task, network_cl_entry, &g_network_cl_args);
    pi_cluster_send_task_to_cl(cluster_dev, &cl_task);

% if ne16_eligible_layers and max_ne16_weight_l1_size > 0:
    // Free NE16 L1 buffers
    if (g_network_cl_args.ne16_weight_l1) pi_l1_free(cluster_dev, g_network_cl_args.ne16_weight_l1, g_network_cl_args.ne16_weight_l1_size);
    if (g_network_cl_args.ne16_bias_l1) pi_l1_free(cluster_dev, g_network_cl_args.ne16_bias_l1, g_network_cl_args.ne16_bias_l1_size);
% endif
    if (g_network_cl_args.l1_buffer) pi_l1_free(cluster_dev, g_network_cl_args.l1_buffer, g_network_cl_args.l1_buffer_size);
    return g_network_cl_args.status;
}
