{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARES Getting Started Tutorial\n",
    "\n",
    "This tutorial walks through the complete ARES pipeline: taking a PyTorch neural network and deploying it on the GAP9 RISC-V processor with **true INT8 inference**.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Understand** the three stages of quantization (FP32 → Fake INT8 → True INT8)\n",
    "2. **Define** a quantized model using Brevitas\n",
    "3. **Train** on MNIST dataset\n",
    "4. **Extract** INT8 weights and quantization scales\n",
    "5. **Generate** C code for GAP9\n",
    "6. **Run** on the GAP9 simulator (GVSOC)\n",
    "\n",
    "**Time estimate:** ~30 minutes (including training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Understanding Quantization Stages\n",
    "\n",
    "Before we start coding, it's crucial to understand the **three stages** of quantization in the ARES pipeline:\n",
    "\n",
    "### Stage 1: FP32 (Floating Point)\n",
    "\n",
    "This is your regular PyTorch model with 32-bit floating point weights and activations.\n",
    "\n",
    "```\n",
    "Weights:     float32 (e.g., 0.0234, -0.1567, ...)\n",
    "Activations: float32\n",
    "Compute:     FP32 multiplication and addition\n",
    "```\n",
    "\n",
    "**Problem:** FP32 is expensive on embedded hardware - requires FPU, high power consumption more storage.\n",
    "\n",
    "---\n",
    "\n",
    "### Stage 2: INT8 Fake Quantized (Brevitas Training)\n",
    "\n",
    "Brevitas **simulates** INT8 quantization during training, but computations still happen in FP32.\n",
    "\n",
    "```\n",
    "Weights:     Quantized to INT8 range [-128, 127], but STORED as float32\n",
    "Activations: Quantized to INT8 range, but STORED as float32\n",
    "Compute:     Still FP32 (for gradient computation during backprop)\n",
    "```\n",
    "\n",
    "**Why \"fake\"?** The values are constrained to INT8-representable values, but the underlying computation uses float32. This allows:\n",
    "- Gradient computation for backpropagation\n",
    "- The model to **learn** to work with quantized values\n",
    "- Simulation of quantization noise during training\n",
    "- A lot quicker to do than true quantization (we can still use GPUs)\n",
    "\n",
    "**Key insight:** A Brevitas model's accuracy during training tells you what accuracy to expect after true quantization.\n",
    "\n",
    "---\n",
    "\n",
    "### Stage 3: INT8 True Quantized (ARES on GAP9)\n",
    "\n",
    "ARES extracts the quantization parameters and generates C code that performs **actual INT8 arithmetic**.\n",
    "\n",
    "```\n",
    "Weights:     True INT8 values (1 byte each)\n",
    "Activations: True INT8 values (1 byte each)\n",
    "Compute:     INT8 x INT8 → INT32 accumulator → scale → INT8 output\n",
    "```\n",
    "\n",
    "**This is the real deal:** No floating point operations. Pure integer math on the GAP9 cluster.\n",
    "\n",
    "---\n",
    "\n",
    "### The Complete Pipeline\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│  STAGE 1: FP32                                                              │\n",
    "│  ─────────────────                                                          │\n",
    "│  Regular PyTorch model (nn.Conv2d, nn.Linear, etc.)                         │\n",
    "│  Weights: float32, Compute: float32                                         │\n",
    "│                                    ↓                                        │\n",
    "│                         [Replace with Brevitas layers]                      │\n",
    "│                                    ↓                                        │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│  STAGE 2: INT8 FAKE QUANTIZED (Training)                                    │\n",
    "│  ────────────────────────────────────────                                   │\n",
    "│  Brevitas model (QuantConv2d, QuantLinear, etc.)                            │\n",
    "│  Weights: int8 values stored as float32                                     │\n",
    "│  Compute: float32 (enables backpropagation)                                 │\n",
    "│  Scales: Learned during training                                            │\n",
    "│                                    ↓                                        │\n",
    "│                         [BrevitasExtractor]                                 │\n",
    "│                                    ↓                                        │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│  STAGE 3: INT8 TRUE QUANTIZED (Deployment)                                  │\n",
    "│  ──────────────────────────────────────────                                 │\n",
    "│  ARES-generated C code on GAP9                                              │\n",
    "│  Weights: Actual int8 arrays (1 byte per weight)                            │\n",
    "│  Compute: int8 x int8 → int32 accumulator → rescale → int8                  │\n",
    "│  No floating point operations!                                              │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "| Aspect | FP32 | Fake INT8 (Brevitas) | True INT8 (ARES) |\n",
    "|--------|------|----------------------|------------------|\n",
    "| Weight storage | 4 bytes | 4 bytes (float32) | **1 byte** |\n",
    "| Compute | FPU required | FPU required | **Integer ALU only** |\n",
    "| Power | High | High | **Low** |\n",
    "| GAP9 compatible | No | No | **Yes** |\n",
    "| Training | Yes | Yes | No |\n",
    "| Accuracy | Baseline | ~Same as True INT8 | Matches Fake INT8 |\n",
    "\n",
    "**The goal:** Train with Brevitas (Stage 2), then deploy with ARES (Stage 3) with **0.0% error** between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Setup\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Python 3.8+ with PyTorch and Brevitas\n",
    "- GAP9 SDK (optional, for GVSOC simulation)\n",
    "\n",
    "### Required Packages\n",
    "\n",
    "```bash\n",
    "pip install torch brevitas numpy torchvision\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ARES root to path\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "# Standard imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Brevitas (quantization) - Stage 2: Fake INT8\n",
    "from brevitas.nn import QuantConv2d, QuantLinear, QuantReLU, QuantIdentity\n",
    "\n",
    "# ARES tools - Stage 3: True INT8\n",
    "from tools.pytorch_extractor import BrevitasExtractor\n",
    "from tools.int8_inference import INT8InferenceEngine  # True INT8 inference engine\n",
    "from tools.generate_golden_outputs import GoldenOutputGenerator  # Golden output generation\n",
    "from codegen.generate_c_code import CCodeGenerator\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "OUTPUT_DIR = Path('outputs')\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: From FP32 to Brevitas - Step by Step\n",
    "\n",
    "Before jumping into the full Brevitas model, let's see what a **regular FP32 PyTorch model** looks like and how we transform it.\n",
    "\n",
    "### Stage 1: The FP32 Network (Regular PyTorch)\n",
    "\n",
    "Here's a simple CNN in standard PyTorch - no quantization at all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1: Regular FP32 PyTorch CNN (NO quantization)\n",
    "class FP32_CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard PyTorch CNN - this is what you'd write normally.\n",
    "    Uses float32 for all weights and computations.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Standard PyTorch layers - no quantization\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.classifier = nn.Linear(7 * 7 * 128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Show the FP32 model\n",
    "fp32_model = FP32_CNN()\n",
    "print(\"FP32 CNN (Stage 1 - Regular PyTorch):\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in fp32_model.parameters()):,}\")\n",
    "print(f\"  Weight dtype: {next(fp32_model.parameters()).dtype}\")\n",
    "print(f\"  Memory per weight: 4 bytes (float32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2: Transforming to Brevitas (Fake INT8)\n",
    "\n",
    "Now we transform this FP32 network into a **Brevitas quantized network**. The transformation is mechanical - replace each layer type with its Brevitas equivalent:\n",
    "\n",
    "| FP32 (Stage 1) | Brevitas (Stage 2) | Notes |\n",
    "|----------------|-------------------|-------|\n",
    "| `nn.Conv2d` | `QuantConv2d` | Add `weight_bit_width=8` |\n",
    "| `nn.Linear` | `QuantLinear` | Add `weight_bit_width=8` |\n",
    "| `nn.ReLU` | `QuantReLU` | Add `bit_width=8, return_quant_tensor=True` |\n",
    "| *(add new)* | `QuantIdentity` | Add at input and after pooling |\n",
    "\n",
    "**Key additions for Brevitas:**\n",
    "\n",
    "1. **`QuantIdentity` at input** - Quantizes the FP32 input to INT8 range\n",
    "2. **`QuantIdentity` after pooling** - Re-establishes scale after non-Brevitas ops\n",
    "3. **`return_quant_tensor=True`** - Propagates scale information through the network\n",
    "4. **`weight_bit_width=8`** - Specifies 8-bit quantization for weights\n",
    "\n",
    "### The Transformed Network (Brevitas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TutorialCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN for MNIST classification using Brevitas (Fake INT8).\n",
    "    \n",
    "    Compare to FP32_CNN above - the key changes are:\n",
    "    1. nn.Conv2d → QuantConv2d (with weight_bit_width=8)\n",
    "    2. nn.Linear → QuantLinear (with weight_bit_width=8)\n",
    "    3. nn.ReLU → QuantReLU (with bit_width=8, return_quant_tensor=True)\n",
    "    4. Added QuantIdentity at input and after pooling layers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        bit_width = 8\n",
    "        \n",
    "        # NEW: Input quantization (not in FP32 version)\n",
    "        self.input_quant = QuantIdentity(bit_width=bit_width, return_quant_tensor=True)\n",
    "        \n",
    "        # CHANGED: nn.Conv2d → QuantConv2d\n",
    "        self.conv1 = QuantConv2d(1, 32, kernel_size=3, padding=1, bias=True, weight_bit_width=bit_width)\n",
    "        # CHANGED: nn.ReLU → QuantReLU\n",
    "        self.relu1 = QuantReLU(bit_width=bit_width, return_quant_tensor=True)\n",
    "        # UNCHANGED: MaxPool2d stays the same (no Brevitas equivalent)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # NEW: QuantIdentity after pooling (re-establishes scale)\n",
    "        self.pool1_quant = QuantIdentity(bit_width=bit_width, return_quant_tensor=True)\n",
    "        \n",
    "        # Block 2: Same pattern\n",
    "        self.conv2 = QuantConv2d(32, 64, kernel_size=3, padding=1, bias=True, weight_bit_width=bit_width)\n",
    "        self.relu2 = QuantReLU(bit_width=bit_width, return_quant_tensor=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.pool2_quant = QuantIdentity(bit_width=bit_width, return_quant_tensor=True)\n",
    "        \n",
    "        # Block 3\n",
    "        self.conv3 = QuantConv2d(64, 128, kernel_size=3, padding=1, bias=True, weight_bit_width=bit_width)\n",
    "        self.relu3 = QuantReLU(bit_width=bit_width, return_quant_tensor=True)\n",
    "        \n",
    "        # NEW: QuantIdentity before linear (after flatten)\n",
    "        self.pre_linear_quant = QuantIdentity(bit_width=bit_width, return_quant_tensor=True)\n",
    "        \n",
    "        # CHANGED: nn.Linear → QuantLinear\n",
    "        self.classifier = QuantLinear(7 * 7 * 128, 10, bias=True, weight_bit_width=bit_width)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # NEW: Quantize input\n",
    "        x = self.input_quant(x)\n",
    "        \n",
    "        # Block 1 (note: pool1_quant is NEW)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.pool1_quant(x)  # NEW\n",
    "        \n",
    "        # Block 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.pool2_quant(x)  # NEW\n",
    "        \n",
    "        # Block 3\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        # Flatten and classify\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.pre_linear_quant(x)  # NEW\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model (this is Stage 2: Fake INT8)\n",
    "cnn_model = TutorialCNN().to(DEVICE)\n",
    "print(\"Created TutorialCNN (Brevitas model - Fake INT8 quantization)\")\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in cnn_model.parameters()):,}\")\n",
    "\n",
    "# Calculate MACs\n",
    "macs = (32*1*9*784) + (64*32*9*196) + (128*64*9*49) + (6272*10)\n",
    "print(f\"Total MACs: {macs:,} (~{macs/1e6:.1f}M)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST transforms: convert to tensor (values in [0, 1])\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.MNIST(\n",
    "    root='../data/mnist', \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    root='../data/mnist', \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Use subset for faster training (1000 samples)\n",
    "train_subset = torch.utils.data.Subset(train_dataset, range(1000))\n",
    "test_subset = torch.utils.data.Subset(test_dataset, range(200))\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_subset)}\")\n",
    "print(f\"Test samples: {len(test_subset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the CNN (Stage 2: Fake INT8)\n",
    "\n",
    "Training a Brevitas model is **identical** to training a regular PyTorch model. The fake quantization happens automatically during the forward pass.\n",
    "\n",
    "**What happens during training:**\n",
    "- Forward pass: Values are quantized to INT8 range (but stored as float32)\n",
    "- Backward pass: Gradients flow through using Straight-Through Estimator (STE)\n",
    "- Weights update: Model learns to work with quantized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, epochs=5, lr=0.001):\n",
    "    \"\"\"Train a model on the given data loader.\"\"\"\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)  # Fake INT8 forward pass\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()  # Gradients via STE\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        acc = 100. * correct / total\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(train_loader):.4f}, Acc: {acc:.2f}%\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"Evaluate model accuracy.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return 100. * correct / total\n",
    "\n",
    "# Train the CNN (Fake INT8 training)\n",
    "print(\"=\"*60)\n",
    "print(\"STAGE 2: Training with FAKE INT8 quantization (Brevitas)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n(This takes ~1-2 minutes)\\n\")\n",
    "cnn_model = train_model(cnn_model, train_loader, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate (still Fake INT8 - but this predicts True INT8 accuracy!)\n",
    "cnn_accuracy_fake = evaluate_model(cnn_model, test_loader)\n",
    "print(f\"\\nTutorialCNN Test Accuracy (Fake INT8): {cnn_accuracy_fake:.2f}%\")\n",
    "print(\"\\n→ This accuracy should match True INT8 on GAP9!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract INT8 Weights (Transition to Stage 3)\n",
    "\n",
    "Now we use `BrevitasExtractor` to extract the **actual INT8 values** from the Brevitas model. This is the bridge between Stage 2 (fake) and Stage 3 (true).\n",
    "\n",
    "**What gets extracted:**\n",
    "- **INT8 weights** - The actual quantized values (not float32 anymore!)\n",
    "- **Scales** - How to convert between INT8 and real values\n",
    "- **Layer metadata** - Shapes, types, connections\n",
    "\n",
    "The formula for dequantization is:\n",
    "```\n",
    "real_value = int8_value x scale\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory for CNN\n",
    "cnn_output_dir = OUTPUT_DIR / 'tutorial_cnn'\n",
    "cnn_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Get a sample input for extraction (needed to trace activation scales)\n",
    "sample_input, _ = next(iter(test_loader))\n",
    "sample_input = sample_input[:1].to(DEVICE)  # Single image\n",
    "\n",
    "print(f\"Sample input shape: {sample_input.shape}\")\n",
    "print(f\"Output directory: {cnn_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Extracting TRUE INT8 weights from Brevitas model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extract weights and scales\n",
    "cnn_model.eval()\n",
    "extractor = BrevitasExtractor(cnn_model)\n",
    "network_info = extractor.extract_all(sample_input.cpu())  # Note: extract_all(), not extract()\n",
    "\n",
    "# Save to disk\n",
    "golden_dir = cnn_output_dir / 'golden_outputs'\n",
    "golden_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save network_info.json (convert numpy arrays to lists for JSON serialization)\n",
    "def numpy_to_list(obj):\n",
    "    \"\"\"Convert numpy arrays to lists for JSON serialization.\"\"\"\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: numpy_to_list(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [numpy_to_list(v) for v in obj]\n",
    "    return obj\n",
    "\n",
    "network_info_serializable = numpy_to_list(network_info)\n",
    "with open(golden_dir / 'network_info.json', 'w') as f:\n",
    "    json.dump(network_info_serializable, f, indent=2, default=str)\n",
    "\n",
    "# Save weights as actual INT8 arrays\n",
    "weights_dir = golden_dir / 'weights'\n",
    "weights_dir.mkdir(exist_ok=True)\n",
    "extractor.save_weights(str(weights_dir))\n",
    "\n",
    "print(f\"\\nExtracted {len(network_info)} layers\")\n",
    "print(f\"\\nLayers:\")\n",
    "for name, info in network_info.items():\n",
    "    if not name.startswith('__'):  # Skip metadata keys\n",
    "        layer_type = info.get('type', 'unknown')\n",
    "        print(f\"  {name}: {layer_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the quantization details for conv1\n",
    "print(\"=\"*60)\n",
    "print(\"Examining conv1 quantization (Fake → True INT8)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'conv1' in network_info:\n",
    "    conv1_info = network_info['conv1']\n",
    "    print(f\"\\nLayer type: {conv1_info.get('type')}\")\n",
    "    print(f\"Shape: {conv1_info.get('in_channels')}→{conv1_info.get('out_channels')} with {conv1_info.get('kernel_size')}x{conv1_info.get('kernel_size')} kernel\")\n",
    "    \n",
    "    # Scales are the key to INT8 conversion\n",
    "    weight_scale = conv1_info.get('weight_scale', 'N/A')\n",
    "    input_scale = conv1_info.get('input_scale', 'N/A')\n",
    "    output_scale = conv1_info.get('output_scale', 'N/A')\n",
    "    \n",
    "    print(f\"\\nQuantization scales:\")\n",
    "    print(f\"  Weight scale: {weight_scale}\")\n",
    "    print(f\"  Input scale:  {input_scale}\")\n",
    "    print(f\"  Output scale: {output_scale}\")\n",
    "    \n",
    "    print(f\"\\nHow to interpret:\")\n",
    "    print(f\"  real_weight = int8_weight x {weight_scale}\")\n",
    "    print(f\"  real_input  = int8_input  x {input_scale}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify weights are actually INT8\n",
    "print(\"=\"*60)\n",
    "print(\"Verifying extracted weights are TRUE INT8\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "weight_files = list(weights_dir.glob('*_weight*.npy'))\n",
    "if weight_files:\n",
    "    for wf in weight_files[:2]:  # Show first 2\n",
    "        w = np.load(wf)\n",
    "        print(f\"\\n{wf.name}:\")\n",
    "        print(f\"  Shape: {w.shape}\")\n",
    "        print(f\"  Dtype: {w.dtype}\")\n",
    "        print(f\"  Range: [{w.min()}, {w.max()}]\")\n",
    "        if w.dtype == np.int8:\n",
    "            print(f\"  ✓ TRUE INT8 (1 byte per weight)\")\n",
    "        else:\n",
    "            print(f\"  Note: Stored as {w.dtype} for compatibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Golden Outputs & Evaluate TRUE INT8 Accuracy\n",
    "\n",
    "Now we use `GoldenOutputGenerator` to:\n",
    "1. Run **actual INT8 arithmetic** through the network\n",
    "2. Save golden outputs for GAP9 verification\n",
    "3. Validate our quantization pipeline\n",
    "\n",
    "**What happens inside:**\n",
    "```python\n",
    "for each test input:\n",
    "    # TRUE INT8 computation (no floating point in the MAC operations!)\n",
    "    for each layer:\n",
    "        accumulator_int32 = sum(input_int8[i] * weight_int8[i])  # int8 x int8 → int32\n",
    "        output_int8 = clip((accumulator_int32 + bias_int32) * scale, -128, 127)\n",
    "    \n",
    "    # Save intermediate INT8 outputs for layer-by-layer verification\n",
    "    save(intermediate_int8/*.npy)\n",
    "```\n",
    "\n",
    "**Expected:** GAP9 output matches these golden outputs with **0.0% error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"STAGE 3: Generating golden outputs with TRUE INT8 inference\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create golden output generator (uses INT8InferenceEngine internally)\n",
    "golden_generator = GoldenOutputGenerator(network_info)\n",
    "\n",
    "# Get a real MNIST test image for the golden output\n",
    "test_image, test_label = test_dataset[3]  # Use test sample #3\n",
    "test_input_fp32 = test_image.unsqueeze(0).numpy()  # [1, 1, 28, 28]\n",
    "\n",
    "print(f\"\\nUsing MNIST test image (label={test_label})\")\n",
    "print(f\"Input shape: {test_input_fp32.shape}\")\n",
    "\n",
    "# Generate golden outputs using TRUE INT8 inference\n",
    "test_cases = [test_input_fp32]\n",
    "test_case_dir = golden_dir / 'test_cases'\n",
    "golden_generator.generate_golden_outputs(test_cases, output_dir=str(test_case_dir))\n",
    "\n",
    "print(f\"\\nGolden outputs saved to: {test_case_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate TRUE INT8 Accuracy\n",
    "\n",
    "Let's verify that TRUE INT8 inference produces the same accuracy as Fake INT8 training. This validates our quantization pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Evaluating TRUE INT8 accuracy\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n(Testing on a small subset - this is slow but validates the pipeline)\\n\")\n",
    "\n",
    "# Create INT8 inference engine\n",
    "engine = INT8InferenceEngine(network_info)\n",
    "\n",
    "# Evaluate on subset of test data\n",
    "num_test_samples = 20\n",
    "correct_true_int8 = 0\n",
    "\n",
    "for i in range(num_test_samples):\n",
    "    # Get test sample\n",
    "    img, label = test_dataset[i]\n",
    "    x_fp32 = img.unsqueeze(0).numpy()\n",
    "    \n",
    "    # Run TRUE INT8 inference\n",
    "    output_fp32, _, _ = engine.forward(x_fp32, verbose=False)\n",
    "    predicted = np.argmax(output_fp32[0])\n",
    "    \n",
    "    if predicted == label:\n",
    "        correct_true_int8 += 1\n",
    "    \n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"  Processed {i+1}/{num_test_samples} samples...\")\n",
    "\n",
    "cnn_accuracy_true_int8 = 100. * correct_true_int8 / num_test_samples\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"CNN Accuracy Comparison:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Fake INT8 (Brevitas): {cnn_accuracy_fake:.2f}%\")\n",
    "print(f\"  True INT8 (Python):   {cnn_accuracy_true_int8:.2f}%\")\n",
    "print(f\"\\n  Difference: {abs(cnn_accuracy_fake - cnn_accuracy_true_int8):.2f}%\")\n",
    "if abs(cnn_accuracy_fake - cnn_accuracy_true_int8) < 10:\n",
    "    print(\"  ✓ Pipeline validated - True INT8 matches Fake INT8!\")\n",
    "else:\n",
    "    print(\"  ! Large difference - check quantization parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate C Code (Stage 3: True INT8 on GAP9)\n",
    "\n",
    "The `CCodeGenerator` creates C code that performs **actual INT8 arithmetic** on GAP9.\n",
    "\n",
    "**What's generated:**\n",
    "- INT8 weight arrays (1 byte per weight)\n",
    "- INT8 x INT8 → INT32 MAC operations\n",
    "- Proper scaling and bias handling\n",
    "- Memory-optimized buffer allocation\n",
    "- DMA pipelining for efficient data movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"STAGE 3: Generating TRUE INT8 C code for GAP9\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Generate C code\n",
    "generated_dir = cnn_output_dir / 'generated'\n",
    "\n",
    "# GoldenOutputGenerator creates test_case_1, test_case_2, etc.\n",
    "actual_test_case_dir = test_case_dir / 'test_case_1'\n",
    "\n",
    "try:\n",
    "    generator = CCodeGenerator(\n",
    "        network_info_path=str(golden_dir / 'network_info.json'),\n",
    "        weights_dir=str(weights_dir),\n",
    "        test_case_dir=str(actual_test_case_dir),\n",
    "        output_dir=str(generated_dir)\n",
    "    )\n",
    "    generator.generate_all()\n",
    "    print(f\"\\n✓ C code generated at: {generated_dir}\")\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"\\nCode generation error: {e}\")\n",
    "    traceback.print_exc()\n",
    "    print(\"\\nThis may be expected if running without full ARES setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List generated files\n",
    "if generated_dir.exists():\n",
    "    print(\"Generated files:\")\n",
    "    for f in sorted(generated_dir.rglob('*')):\n",
    "        if f.is_file():\n",
    "            rel_path = f.relative_to(generated_dir)\n",
    "            size_kb = f.stat().st_size / 1024\n",
    "            print(f\"  {rel_path} ({size_kb:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running on GAP9 (Optional)\n",
    "\n",
    "If you have the GAP9 SDK installed, you can build and run on GVSOC:\n",
    "\n",
    "```bash\n",
    "cd outputs/tutorial_cnn/generated\n",
    "source ~/gap_sdk/configs/gap9_v2.sh\n",
    "make clean all run platform=gvsoc\n",
    "```\n",
    "\n",
    "**Expected output:** `Error: 0.0%`\n",
    "\n",
    "---\n",
    "\n",
    "### Makefile Options\n",
    "\n",
    "The generated Makefile supports several useful flags:\n",
    "\n",
    "#### Basic Build Options\n",
    "\n",
    "| Flag | Default | Description |\n",
    "|------|---------|-------------|\n",
    "| `CORE=N` | 8 | Number of cluster cores to use (1-8) |\n",
    "| `platform=gvsoc` | - | Run on GVSOC simulator |\n",
    "| `platform=board` | - | Run on real GAP9 hardware |\n",
    "\n",
    "#### Performance Profiling\n",
    "\n",
    "| Flag | Default | Description |\n",
    "|------|---------|-------------|\n",
    "| `ENABLE_PERF=1` | 0 | Enable per-layer cycle counters |\n",
    "| `MINIMAL_OUTPUT=1` | 0 | Reduce debug prints (keeps perf summary) |\n",
    "\n",
    "#### Validation Options\n",
    "\n",
    "| Flag | Default | Description |\n",
    "|------|---------|-------------|\n",
    "| `DISABLE_INTERMEDIATE_GOLDEN=1` | 0 | Skip per-layer validation (only check final output) |\n",
    "\n",
    "#### Memory Configuration\n",
    "\n",
    "| Flag | Default | Description |\n",
    "|------|---------|-------------|\n",
    "| `PI_CL_SLAVE_STACK_SIZE=0xNNN` | 0x400 | Stack size for worker cores (increase if stack overflow) |\n",
    "\n",
    "---\n",
    "\n",
    "### Example Commands\n",
    "\n",
    "```bash\n",
    "# Basic run with full validation\n",
    "make clean all run platform=gvsoc\n",
    "\n",
    "# Performance profiling (verbose - shows per-layer breakdown)\n",
    "make clean all run platform=gvsoc ENABLE_PERF=1\n",
    "\n",
    "# Clean benchmarking (minimal prints, but shows perf summary)\n",
    "make clean all run platform=gvsoc MINIMAL_OUTPUT=1 ENABLE_PERF=1\n",
    "\n",
    "# Fast run for large models (skip per-layer validation)\n",
    "make clean all run platform=gvsoc DISABLE_INTERMEDIATE_GOLDEN=1\n",
    "\n",
    "# Debug with single core\n",
    "make clean all run platform=gvsoc CORE=1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Understanding the Output\n",
    "\n",
    "**With `ENABLE_PERF=1`**, you'll see per-layer timing:\n",
    "```\n",
    "PERF conv1      : total=405248 compute=342017 dma_load=0 dma_store=0 idle=63231 overlap=84.4%\n",
    "PERF conv2      : total=375194 compute=302750 dma_load=0 dma_store=0 idle=72444 overlap=80.7%\n",
    "...\n",
    "\n",
    "PERFORMANCE SUMMARY\n",
    "Total layers:        7\n",
    "Total cycles:        898819\n",
    "  Compute cycles:    735044 (81.8%)\n",
    "```\n",
    "\n",
    "**Validation output:**\n",
    "```\n",
    "CL: Error analysis:\n",
    "  Max error:  0.000000    ← Maximum absolute error across all outputs\n",
    "  Mean error: 0.000000    ← Average absolute error\n",
    "CL: ✓ Test PASSED (error < 1.0%)\n",
    "```\n",
    "\n",
    "This confirms that:\n",
    "1. Stage 2 (Fake INT8, Brevitas) → Accuracy: ~95%\n",
    "2. Stage 3 (True INT8, GAP9) → **Bit-exact match** with Python INT8 reference\n",
    "3. The quantization pipeline is working correctly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The ARES Auto-Tuner & Knowledge Base\n",
    "\n",
    "One of the key features of ARES is its **auto-tuning system** that automatically discovers optimal configurations for each layer and stores them in a **knowledge base** for future use.\n",
    "\n",
    "### The Problem: Layer Configuration\n",
    "\n",
    "Each layer has many tunable parameters that affect performance:\n",
    "\n",
    "- **Tiling parameters**: `tile_m`, `tile_n`, `tile_k` for matrix operations\n",
    "- **Unrolling factors**: `outch_unroll` for convolutions\n",
    "- **Pipeline settings**: Double buffering, DMA overlap\n",
    "- **Memory placement**: L1 vs L2 vs L3\n",
    "\n",
    "Finding the optimal configuration manually is tedious and error-prone. Different layer shapes require different settings.\n",
    "\n",
    "### The Solution: Auto-Tuner + Knowledge Base\n",
    "\n",
    "ARES includes an **auto-tuning framework** that:\n",
    "\n",
    "1. **Profiles** each layer to identify bottlenecks\n",
    "2. **Searches** configuration space systematically\n",
    "3. **Measures** actual cycle counts on GVSOC\n",
    "4. **Records** successful configurations to a knowledge base\n",
    "5. **Reuses** configurations for similar layers in future runs\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                        AUTO-TUNER WORKFLOW                          │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                     │\n",
    "│   New Layer Shape          Knowledge Base                           │\n",
    "│        │                        │                                   │\n",
    "│        ▼                        ▼                                   │\n",
    "│   ┌─────────┐    lookup    ┌─────────────┐                         │\n",
    "│   │ linear  │ ───────────► │ Match found │ ──► Use cached config   │\n",
    "│   │ M=400   │              └─────────────┘                         │\n",
    "│   │ N=768   │                    │                                  │\n",
    "│   │ K=192   │              No match                                 │\n",
    "│   └─────────┘                    │                                  │\n",
    "│        │                         ▼                                  │\n",
    "│        │                  ┌─────────────┐                          │\n",
    "│        └─────────────────►│  Auto-Tune  │                          │\n",
    "│                           │  (GVSOC)    │                          │\n",
    "│                           └──────┬──────┘                          │\n",
    "│                                  │                                  │\n",
    "│                                  ▼                                  │\n",
    "│                           ┌─────────────┐                          │\n",
    "│                           │   Record    │                          │\n",
    "│                           │   to KB     │                          │\n",
    "│                           └─────────────┘                          │\n",
    "│                                                                     │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Using the Auto-Tuner\n",
    "\n",
    "```bash\n",
    "# Show what layers would be tuned (dry run)\n",
    "python tools/auto_tune.py --test test_1_simplecnn --dry-run\n",
    "\n",
    "# Tune all bottleneck layers (>100K cycles)\n",
    "python tools/auto_tune.py --test test_1_simplecnn --tune-all\n",
    "\n",
    "# Tune specific layers with more iterations\n",
    "python tools/auto_tune.py --test test_1_simplecnn --layers conv1 conv2 --max-iter 30\n",
    "```\n",
    "\n",
    "### Knowledge Base\n",
    "\n",
    "The knowledge base (`codegen/optimization/data/knowledge_base.json`) stores:\n",
    "\n",
    "- **Shape patterns**: Layer dimensions (M, N, K for linear; C_in, C_out, H, W for conv)\n",
    "- **Optimal configs**: Tiling, unrolling, pipeline settings\n",
    "- **Performance metrics**: Measured cycles, MACs/cycle\n",
    "- **Provenance**: Which test/run discovered this configuration\n",
    "\n",
    "When generating C code, ARES automatically queries the knowledge base:\n",
    "\n",
    "```\n",
    "  [KB] Auto-applying config for conv1: {'outch_unroll': 4}\n",
    "  [KB] Auto-applying config for fc1: {'tile_m': 1, 'tile_n': 128, 'tile_k': 256}\n",
    "```\n",
    "\n",
    "This means **you get optimized configurations automatically** for common layer shapes, without manual tuning.\n",
    "\n",
    "### Shape Matching\n",
    "\n",
    "The knowledge base uses **fuzzy shape matching** - if an exact match isn't found, it looks for similar shapes:\n",
    "\n",
    "- Linear layers: Match on (M, N, K) dimensions\n",
    "- Conv layers: Match on (C_in, C_out, kernel_size, spatial_dims)\n",
    "- Configurable tolerance for \"close enough\" matches\n",
    "\n",
    "This allows configurations learned from one model to benefit similar layers in other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Example 2 - MLP (Dense Network)\n",
    "\n",
    "Let's try a network with **no convolutions** - just linear (fully-connected) layers. This demonstrates that ARES handles different architectures and shows a different memory access pattern.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "INPUT (28x28x1)\n",
    "  → Flatten (784)\n",
    "  → Linear(784 → 256) → ReLU\n",
    "  → Linear(256 → 128) → ReLU  \n",
    "  → Linear(128 → 10)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer perceptron using Brevitas (Fake INT8).\n",
    "    \n",
    "    No convolutions - shows pure Linear layer quantization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        bit_width = 8\n",
    "        \n",
    "        # Input quantization (applied after flatten)\n",
    "        self.input_quant = QuantIdentity(bit_width=bit_width, return_quant_tensor=True)\n",
    "        \n",
    "        # Layer 1: 784 → 256\n",
    "        self.fc1 = QuantLinear(784, 256, bias=True, weight_bit_width=bit_width)\n",
    "        self.relu1 = QuantReLU(bit_width=bit_width, return_quant_tensor=True)\n",
    "        self.fc1_quant = QuantIdentity(bit_width=bit_width, return_quant_tensor=True)\n",
    "        \n",
    "        # Layer 2: 256 → 128\n",
    "        self.fc2 = QuantLinear(256, 128, bias=True, weight_bit_width=bit_width)\n",
    "        self.relu2 = QuantReLU(bit_width=bit_width, return_quant_tensor=True)\n",
    "        self.fc2_quant = QuantIdentity(bit_width=bit_width, return_quant_tensor=True)\n",
    "        \n",
    "        # Layer 3: 128 → 10 (output)\n",
    "        self.fc3 = QuantLinear(128, 10, bias=True, weight_bit_width=bit_width)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Flatten first, then quantize\n",
    "        x = x.view(x.size(0), -1)  # (batch, 784)\n",
    "        x = self.input_quant(x)\n",
    "        \n",
    "        # Layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc1_quant(x)\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc2_quant(x)\n",
    "        \n",
    "        # Layer 3 (output)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "mlp_model = SimpleMLP().to(DEVICE)\n",
    "print(\"Created SimpleMLP (Brevitas model - Fake INT8 quantization)\")\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in mlp_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the MLP\n",
    "print(\"=\"*60)\n",
    "print(\"STAGE 2: Training MLP with FAKE INT8 quantization\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n(This takes ~1-2 minutes)\\n\")\n",
    "mlp_model = train_model(mlp_model, train_loader, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "mlp_accuracy_fake = evaluate_model(mlp_model, test_loader)\n",
    "print(f\"\\nMLP Test Accuracy (Fake INT8): {mlp_accuracy_fake:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract MLP weights (Fake → True INT8)\n",
    "print(\"=\"*60)\n",
    "print(\"Extracting TRUE INT8 weights from MLP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "mlp_output_dir = OUTPUT_DIR / 'tutorial_mlp'\n",
    "mlp_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "mlp_model.eval()\n",
    "mlp_extractor = BrevitasExtractor(mlp_model)\n",
    "mlp_network_info = mlp_extractor.extract_all(sample_input.cpu())  # Note: extract_all()\n",
    "\n",
    "# Save\n",
    "mlp_golden_dir = mlp_output_dir / 'golden_outputs'\n",
    "mlp_golden_dir.mkdir(exist_ok=True)\n",
    "\n",
    "mlp_network_info_serializable = numpy_to_list(mlp_network_info)\n",
    "with open(mlp_golden_dir / 'network_info.json', 'w') as f:\n",
    "    json.dump(mlp_network_info_serializable, f, indent=2, default=str)\n",
    "\n",
    "mlp_weights_dir = mlp_golden_dir / 'weights'\n",
    "mlp_weights_dir.mkdir(exist_ok=True)\n",
    "mlp_extractor.save_weights(str(mlp_weights_dir))\n",
    "\n",
    "print(f\"\\nExtracted {len(mlp_network_info)} layers:\")\n",
    "for name, info in mlp_network_info.items():\n",
    "    if not name.startswith('__'):\n",
    "        print(f\"  {name}: {info.get('type', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN vs MLP: Performance Comparison\n",
    "\n",
    "| Metric | TutorialCNN | SimpleMLP |\n",
    "|--------|-------------|-----------|\n",
    "| Parameters | ~155K | ~235K |\n",
    "| MACs | ~7.5M | ~235K |\n",
    "| Weight size | 155 KB | 235 KB |\n",
    "| Compute MACs/cycle | 2.7 | **9.2** |\n",
    "| Total MACs/cycle | 2.7 | 0.7 |\n",
    "\n",
    "**Key insight:** The MLP achieves excellent compute efficiency (9.2 MACs/cycle using SIMD), but the large fc1 weight matrix (200KB) must stream from L3 HyperRAM, causing DMA overhead. The CNN has smaller weight matrices that fit in L2, so compute ≈ total.\n",
    "\n",
    "**When to use each:**\n",
    "- **CNN**: Image/signal data with spatial structure. Weights reused across spatial dimensions.\n",
    "- **MLP**: Tabular data, embeddings. Best when weights fit in L2 (< ~500KB total)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Key Concepts Deep Dive\n",
    "\n",
    "### The Three Stages Revisited\n",
    "\n",
    "```\n",
    "┌──────────────────┬────────────────────────┬────────────────────────┐\n",
    "│     STAGE 1      │       STAGE 2          │       STAGE 3          │\n",
    "│      FP32        │   Fake INT8 (Brevitas) │   True INT8 (ARES)     │\n",
    "├──────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ nn.Conv2d        │ QuantConv2d            │ int8 MAC loops in C    │\n",
    "│ nn.Linear        │ QuantLinear            │ int8 GEMM in C         │\n",
    "│ nn.ReLU          │ QuantReLU              │ max(0, x) on int8      │\n",
    "│                  │                        │                        │\n",
    "│ float32 weights  │ int8 vals as float32   │ int8 arrays (1 byte)   │\n",
    "│ float32 compute  │ float32 compute        │ int8xint8→int32        │\n",
    "│                  │                        │                        │\n",
    "│ No quantization  │ Simulated quantization │ Real quantization      │\n",
    "│ Can train        │ Can train (STE)        │ Inference only         │\n",
    "└──────────────────┴────────────────────────┴────────────────────────┘\n",
    "```\n",
    "\n",
    "### The Bias Trap\n",
    "\n",
    "A critical detail in INT8 inference is the order of bias addition:\n",
    "\n",
    "**WRONG:**\n",
    "```c\n",
    "output = (accumulator * scale) + bias;  // DON'T DO THIS\n",
    "```\n",
    "\n",
    "**CORRECT:**\n",
    "```c\n",
    "output = (accumulator + bias_int32) * scale;  // Bias BEFORE scale\n",
    "```\n",
    "\n",
    "**Why?** The bias must be added in the INT32 accumulator domain before scaling down to INT8. Adding bias after scaling destroys precision.\n",
    "\n",
    "**Symptom:** If you see ~60% error rate, check bias order!\n",
    "\n",
    "### QuantIdentity Placement\n",
    "\n",
    "`QuantIdentity` serves two critical purposes:\n",
    "\n",
    "1. **Input quantization**: Convert FP32 → INT8\n",
    "2. **Scale tracking**: Re-establish quantization scale after non-Brevitas ops\n",
    "\n",
    "**Rule:** Add `QuantIdentity` after any operation without a Brevitas equivalent:\n",
    "- After `nn.MaxPool2d` → `QuantIdentity`\n",
    "- After `nn.Flatten` / `.view()` → `QuantIdentity`\n",
    "- After `nn.Dropout` → `QuantIdentity`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Summary & Next Steps\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "1. **Three Quantization Stages:**\n",
    "   - FP32: Regular PyTorch\n",
    "   - Fake INT8: Brevitas (training)\n",
    "   - True INT8: ARES/GAP9 (deployment)\n",
    "\n",
    "2. **Brevitas Patterns:**\n",
    "   - Use `QuantConv2d`, `QuantLinear`, `QuantReLU`\n",
    "   - Add `QuantIdentity` after non-Brevitas ops\n",
    "   - Set `return_quant_tensor=True` for scale propagation\n",
    "\n",
    "3. **ARES Pipeline:**\n",
    "   - `BrevitasExtractor`: Fake → True INT8 weights\n",
    "   - `CCodeGenerator`: INT8 C code for GAP9\n",
    "   - Target: 0.0% error vs Python reference\n",
    "\n",
    "### Adding Your Own Network\n",
    "\n",
    "1. **Copy the pattern** from this tutorial\n",
    "2. **Replace layers** with your architecture\n",
    "3. **Add QuantIdentity** after every non-Brevitas operation\n",
    "4. **Train** to learn quantization scales\n",
    "5. **Extract** and generate C code\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- **System Architecture**: `docs/ARCHITECTURE.md`\n",
    "- **Quantization Details**: `docs/QUANTIZATION_GUIDE.md`\n",
    "- **Adding Operations**: `docs/ADDING_OPERATIONS.md`\n",
    "- **Debugging**: `docs/DEBUGGING.md`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TimeFM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
