#*----------------------------------------------------------------------------*
#* Copyright (C) 2025 ETH Zurich, Switzerland                                 *
#* SPDX-License-Identifier: Apache-2.0                                        *
#*                                                                            *
#* Licensed under the Apache License, Version 2.0 (the "License");            *
#* you may not use this file except in compliance with the License.           *
#* You may obtain a copy of the License at                                    *
#*                                                                            *
#* http://www.apache.org/licenses/LICENSE-2.0                                 *
#*                                                                            *
#* Unless required by applicable law or agreed to in writing, software        *
#* distributed under the License is distributed on an "AS IS" BASIS,          *
#* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.   *
#* See the License for the specific language governing permissions and        *
#* limitations under the License.                                             *
#*                                                                            *
#* Author:  Anna Tegon                                                        *
#* Author:  Thorir Mar Ingolfsson                                             *
#*----------------------------------------------------------------------------*
tag: eeg

gpus: 1
num_nodes: 1
num_workers: 2
batch_size: 2

seed: 42
resume: True
pretrained_checkpoint: null
load_state_dict: False
training: True
print_model: True
find_unused_parameters: False # This is set to True by default in PyTorch-Lightning


model_checkpoint:
  save_last: True
  save_top_k: 1
  monitor: "val_psnr"
  mode: "max"

callbacks:
  lr_monitor:
    _target_: 'pytorch_lightning.callbacks.LearningRateMonitor'
    logging_interval: step
  progress_bar:
    _target_: 'pytorch_lightning.callbacks.TQDMProgressBar'
    refresh_rate: 10

io:
  version: 0
  base_output_path: "#CHANGEME"

trainer:
  num_nodes: ${num_nodes}
  devices: ${gpus}
  strategy: auto
  max_epochs: null
  max_steps: 500000
  benchmark: True
  log_every_n_steps: 50
  check_val_every_n_epoch:  1
  val_check_interval: null # controls whether to use epoch-based or step-based training
  num_sanity_val_steps: 0

optimizer:
  optim: LAMB
  lr: 1e-4

masking:
  patch_size: [2, 16]
  masking_ratio: 0.6

input_normalization:
  normalize: True
  quartile_normalization_lower_val: -20
  quartile_normalization_upper_val:  20

scheduler:
  gamma: 0.1



defaults:
  - _self_
  - data_module: pretrain_data_module
  - task: pretrain_task
  - scheduler: multi_step_lr
  - model: FEMBA_pretrain
  - criterion: pretrain_criterion